{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MOT Read Latest Documentation - Browse GitHub Code Repository Project Welcome to MOT, the garbage detection on river banks github. It is part of a project led by Surfrider Europe , which aims at quantifying plastic pollution in rivers through space and time. MOT stands for Multi-Object Tracking, as we detect, then track the different plastic trash instances. The object detection part is based on tensorpack . The next subsections are useful to read if you want to train models or perform advanced tasks. However, if you just want to launch a serving container or perform inferences on one of those, directly jump to this file . Dataset You can download a training dataset on this link . Installation You may run directly the notebook in colab . For more details on training and inference of the object detection please see the following file which is based on the README of tensorpack. Classic To install locally, make sure you have Python 3.3+ and 1.6 <= tensorflow < 2.0 apt install libsm6 libxrender-dev libxext6 libcap-dev ffmpeg pip3 install --user . Docker The following command will build a docker for development and run interactively. PORT_JUPYTER = 22222 PORT_TENSORBOARD = 22223 make docker-training You don't have to specify the ports at the beginning of the command, but do so if you want to assign a specific port to access jupyter notebook and / or tensorboard. You can add arguments to the docker run command by specifying RUN_ARGS, for example: RUN_ARGS = \"-v /srv/data:/srv/data\" make docker-training Do the following command to exec an already running container: make docker-exec-training Internal tools You can launch a jupyter notebook or a tensorboard server by running the command. ./scripts/run_jupyter.sh or ./scripts/run_tensorboard.sh /path/to/the/model/folders/to/track Then, access those servers through the ports you used in the Make command. Train See the original tensorpack README for more details about the configurations and weights. python3 -m mot.object_detection.train --load /path/to/pretrained/weights --config DATA.BASEDIR = /path/to/the/dataset --config TODO = SEE_TENSORPACK_README The next files are pretrained weights on the dataset introduced previously: - https://files.heuritech.com/raw_files/surfrider/resnet50_fpn/model-6000.index - https://files.heuritech.com/raw_files/surfrider/resnet50_fpn/model-6000.data-00000-of-00001 The command used to train this model was: python3 -m mot.object_detection.train --load /path/to/pretrained_weights/COCO-MaskRCNN-R50FPN2x.npz --logdir /path/to/logdir --config DATA.BASEDIR = /path/to/dataset MODE_MASK = False TRAIN.LR_SCHEDULE = 250 ,500,750 Put those files in a folder, which will be /path/to/your/trained/model in the export section. Export First, you need to train an object detection model. Then, you can export this model in SavedModel format: python3 -m mot.object_detection.predict --load /path/to/your/trained/model --serving /path/to/serving --config DATA.BASEDIR = /path/to/the/dataset SAME_CONFIG = AS_TRAINING The dataset should be the one downloaded following the instructions above. You can also use a folder with only this file inside if you don't want to download the whole dataset. Also remember to use the same config as the one used for training (using FPN.CASCADE=True for instance). Serving Refer to this file . Developpers Please read the CONTRIBUTING.md Developper installation You need to install the repository in dev: pip install -e ./ The following libraries are needed to run the tests: pytest , pytest-cov Use with pyenv pyenv activate my_amazing_surfrider_project pip install . Run the tests Within your local environement: To run all the tests: make tests To run a specific test: pytest my_file.py::my_function Within a docker environement: To run all the tests: make docker-tests To run a specific test: make up-tests pytest my_file.py::my_function Status Model & training: [x] Object detection training [ ] Improving train, validation and test dataset [ ] Model improvements [ ] Connection with dataset to query dataset [ ] Tracking model (WIP) [ ] test dataset for tracking Inference and deployment: [x] Object detection inference notebook [ ] Inference on video (WIP) [ ] Connection with input data and inference [x] Small webserver and API (in local) [ ] Docker build and deployment","title":"Home"},{"location":"#mot","text":"Read Latest Documentation - Browse GitHub Code Repository","title":"MOT"},{"location":"#project","text":"Welcome to MOT, the garbage detection on river banks github. It is part of a project led by Surfrider Europe , which aims at quantifying plastic pollution in rivers through space and time. MOT stands for Multi-Object Tracking, as we detect, then track the different plastic trash instances. The object detection part is based on tensorpack . The next subsections are useful to read if you want to train models or perform advanced tasks. However, if you just want to launch a serving container or perform inferences on one of those, directly jump to this file .","title":"Project"},{"location":"#dataset","text":"You can download a training dataset on this link .","title":"Dataset"},{"location":"#installation","text":"You may run directly the notebook in colab . For more details on training and inference of the object detection please see the following file which is based on the README of tensorpack.","title":"Installation"},{"location":"#classic","text":"To install locally, make sure you have Python 3.3+ and 1.6 <= tensorflow < 2.0 apt install libsm6 libxrender-dev libxext6 libcap-dev ffmpeg pip3 install --user .","title":"Classic"},{"location":"#docker","text":"The following command will build a docker for development and run interactively. PORT_JUPYTER = 22222 PORT_TENSORBOARD = 22223 make docker-training You don't have to specify the ports at the beginning of the command, but do so if you want to assign a specific port to access jupyter notebook and / or tensorboard. You can add arguments to the docker run command by specifying RUN_ARGS, for example: RUN_ARGS = \"-v /srv/data:/srv/data\" make docker-training Do the following command to exec an already running container: make docker-exec-training","title":"Docker"},{"location":"#internal-tools","text":"You can launch a jupyter notebook or a tensorboard server by running the command. ./scripts/run_jupyter.sh or ./scripts/run_tensorboard.sh /path/to/the/model/folders/to/track Then, access those servers through the ports you used in the Make command.","title":"Internal tools"},{"location":"#train","text":"See the original tensorpack README for more details about the configurations and weights. python3 -m mot.object_detection.train --load /path/to/pretrained/weights --config DATA.BASEDIR = /path/to/the/dataset --config TODO = SEE_TENSORPACK_README The next files are pretrained weights on the dataset introduced previously: - https://files.heuritech.com/raw_files/surfrider/resnet50_fpn/model-6000.index - https://files.heuritech.com/raw_files/surfrider/resnet50_fpn/model-6000.data-00000-of-00001 The command used to train this model was: python3 -m mot.object_detection.train --load /path/to/pretrained_weights/COCO-MaskRCNN-R50FPN2x.npz --logdir /path/to/logdir --config DATA.BASEDIR = /path/to/dataset MODE_MASK = False TRAIN.LR_SCHEDULE = 250 ,500,750 Put those files in a folder, which will be /path/to/your/trained/model in the export section.","title":"Train"},{"location":"#export","text":"First, you need to train an object detection model. Then, you can export this model in SavedModel format: python3 -m mot.object_detection.predict --load /path/to/your/trained/model --serving /path/to/serving --config DATA.BASEDIR = /path/to/the/dataset SAME_CONFIG = AS_TRAINING The dataset should be the one downloaded following the instructions above. You can also use a folder with only this file inside if you don't want to download the whole dataset. Also remember to use the same config as the one used for training (using FPN.CASCADE=True for instance).","title":"Export"},{"location":"#serving","text":"Refer to this file .","title":"Serving"},{"location":"#developpers","text":"Please read the CONTRIBUTING.md","title":"Developpers"},{"location":"#developper-installation","text":"You need to install the repository in dev: pip install -e ./ The following libraries are needed to run the tests: pytest , pytest-cov","title":"Developper installation"},{"location":"#use-with-pyenv","text":"pyenv activate my_amazing_surfrider_project pip install .","title":"Use with pyenv"},{"location":"#run-the-tests","text":"Within your local environement: To run all the tests: make tests To run a specific test: pytest my_file.py::my_function Within a docker environement: To run all the tests: make docker-tests To run a specific test: make up-tests pytest my_file.py::my_function","title":"Run the tests"},{"location":"#status","text":"Model & training: [x] Object detection training [ ] Improving train, validation and test dataset [ ] Model improvements [ ] Connection with dataset to query dataset [ ] Tracking model (WIP) [ ] test dataset for tracking Inference and deployment: [x] Object detection inference notebook [ ] Inference on video (WIP) [ ] Connection with input data and inference [x] Small webserver and API (in local) [ ] Docker build and deployment","title":"Status"},{"location":"CONTRIBUTING/","text":"Contribution Tests make tests Developpers Code coverage Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% . However those repositories will increase their code coverage iteratively. Any decrease will lead to a rejected PR. Docstring should follow the sphinx formatting def func ( a , b ): \"\"\" Describe my function :param a: A param a description :param b: A param b description :returns: The sum of a + b :raises: (If exception are raised) Exception 1 \"\"\" return a + b The code formatting used is yapf The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"Contributing"},{"location":"CONTRIBUTING/#contribution","text":"","title":"Contribution"},{"location":"CONTRIBUTING/#tests","text":"make tests","title":"Tests"},{"location":"CONTRIBUTING/#developpers","text":"","title":"Developpers"},{"location":"CONTRIBUTING/#code-coverage","text":"Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% . However those repositories will increase their code coverage iteratively. Any decrease will lead to a rejected PR.","title":"Code coverage"},{"location":"CONTRIBUTING/#docstring-should-follow-the-sphinx-formatting","text":"def func ( a , b ): \"\"\" Describe my function :param a: A param a description :param b: A param b description :returns: The sum of a + b :raises: (If exception are raised) Exception 1 \"\"\" return a + b","title":"Docstring should follow the sphinx formatting"},{"location":"CONTRIBUTING/#the-code-formatting-used-is-yapf","text":"The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"The code formatting used is yapf"},{"location":"reference/mot/","text":"Module mot Sub-modules mot.object_detection mot.serving mot.tracker","title":"Index"},{"location":"reference/mot/#module-mot","text":"","title":"Module mot"},{"location":"reference/mot/#sub-modules","text":"mot.object_detection mot.serving mot.tracker","title":"Sub-modules"},{"location":"reference/mot/object_detection/","text":"Module mot.object_detection Sub-modules mot.object_detection.common mot.object_detection.config mot.object_detection.data mot.object_detection.dataset mot.object_detection.eval mot.object_detection.modeling mot.object_detection.predict mot.object_detection.preprocessing mot.object_detection.query_server mot.object_detection.train mot.object_detection.utils mot.object_detection.viz","title":"Index"},{"location":"reference/mot/object_detection/#module-motobject_detection","text":"","title":"Module mot.object_detection"},{"location":"reference/mot/object_detection/#sub-modules","text":"mot.object_detection.common mot.object_detection.config mot.object_detection.data mot.object_detection.dataset mot.object_detection.eval mot.object_detection.modeling mot.object_detection.predict mot.object_detection.preprocessing mot.object_detection.query_server mot.object_detection.train mot.object_detection.utils mot.object_detection.viz","title":"Sub-modules"},{"location":"reference/mot/object_detection/common/","text":"Module mot.object_detection.common View Source # -*- coding: utf-8 -*- # File: common.py import numpy as np import cv2 from tensorpack.dataflow import RNGDataFlow from tensorpack.dataflow.imgaug import ImageAugmentor , ResizeTransform class DataFromListOfDict ( RNGDataFlow ): def __init__ ( self , lst , keys , shuffle = False ): self . _lst = lst self . _keys = keys self . _shuffle = shuffle self . _size = len ( lst ) def __len__ ( self ): return self . _size def __iter__ ( self ): if self . _shuffle : self . rng . shuffle ( self . _lst ) for dic in self . _lst : dp = [ dic [ k ] for k in self . _keys ] yield dp class CustomResize ( ImageAugmentor ): \"\"\" Try resizing the shortest edge to a certain number while avoiding the longest edge to exceed max_size. \"\"\" def __init__ ( self , short_edge_length , max_size , interp = cv2 . INTER_LINEAR ): \"\"\" Args: short_edge_length ([int, int]): a [min, max] interval from which to sample the shortest edge length. max_size (int): maximum allowed longest edge length. \"\"\" super ( CustomResize , self ) . __init__ () if isinstance ( short_edge_length , int ): short_edge_length = ( short_edge_length , short_edge_length ) self . _init ( locals ()) def get_transform ( self , img ): h , w = img . shape [: 2 ] size = self . rng . randint ( self . short_edge_length [ 0 ], self . short_edge_length [ 1 ] + 1 ) scale = size * 1.0 / min ( h , w ) if h < w : newh , neww = size , scale * w else : newh , neww = scale * h , size if max ( newh , neww ) > self . max_size : scale = self . max_size * 1.0 / max ( newh , neww ) newh = newh * scale neww = neww * scale neww = int ( neww + 0.5 ) newh = int ( newh + 0.5 ) return ResizeTransform ( h , w , newh , neww , self . interp ) def box_to_point8 ( boxes ): \"\"\" Args: boxes: nx4 Returns: (nx4)x2 \"\"\" b = boxes [:, [ 0 , 1 , 2 , 3 , 0 , 3 , 2 , 1 ]] b = b . reshape (( - 1 , 2 )) return b def point8_to_box ( points ): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points . reshape (( - 1 , 4 , 2 )) minxy = p . min ( axis = 1 ) # nx2 maxxy = p . max ( axis = 1 ) # nx2 return np . concatenate (( minxy , maxxy ), axis = 1 ) def polygons_to_mask ( polys , height , width ): \"\"\" Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) \"\"\" polys = [ p . flatten () . tolist () for p in polys ] assert len ( polys ) > 0 , \"Polygons are empty!\" import pycocotools.mask as cocomask rles = cocomask . frPyObjects ( polys , height , width ) rle = cocomask . merge ( rles ) return cocomask . decode ( rle ) def clip_boxes ( boxes , shape ): \"\"\" Args: boxes: (...)x4, float shape: h, w \"\"\" orig_shape = boxes . shape boxes = boxes . reshape ([ - 1 , 4 ]) h , w = shape boxes [:, [ 0 , 1 ]] = np . maximum ( boxes [:, [ 0 , 1 ]], 0 ) boxes [:, 2 ] = np . minimum ( boxes [:, 2 ], w ) boxes [:, 3 ] = np . minimum ( boxes [:, 3 ], h ) return boxes . reshape ( orig_shape ) def filter_boxes_inside_shape ( boxes , shape ): \"\"\" Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) \"\"\" assert boxes . ndim == 2 , boxes . shape assert len ( shape ) == 2 , shape h , w = shape indices = np . where ( ( boxes [:, 0 ] >= 0 ) & ( boxes [:, 1 ] >= 0 ) & ( boxes [:, 2 ] <= w ) & ( boxes [:, 3 ] <= h ))[ 0 ] return indices , boxes [ indices , :] try : import pycocotools.mask as cocomask # Much faster than utils/np_box_ops def np_iou ( A , B ): def to_xywh ( box ): box = box . copy () box [:, 2 ] -= box [:, 0 ] box [:, 3 ] -= box [:, 1 ] return box ret = cocomask . iou ( to_xywh ( A ), to_xywh ( B ), np . zeros (( len ( B ),), dtype = np . bool )) # can accelerate even more, if using float32 return ret . astype ( 'float32' ) except ImportError : from mot.object_detection.utils.np_box_ops import iou as np_iou # noqa Functions box_to_point8 def box_to_point8 ( boxes ) Args: boxes: nx4 Returns: (nx4)x2 View Source def box_to_point8 ( boxes ): \"\"\" Args: boxes: nx4 Returns: (nx4)x2 \"\"\" b = boxes [:, [ 0 , 1 , 2 , 3 , 0 , 3 , 2 , 1 ]] b = b . reshape (( - 1 , 2 )) return b clip_boxes def clip_boxes ( boxes , shape ) Args: boxes: (...)x4, float shape: h, w View Source def clip_boxes ( boxes , shape ): \"\"\" Args: boxes: (...)x4, float shape: h, w \"\"\" orig_shape = boxes . shape boxes = boxes . reshape ([ - 1 , 4 ]) h , w = shape boxes [:, [ 0 , 1 ]] = np . maximum ( boxes [:, [ 0 , 1 ]], 0 ) boxes [:, 2 ] = np . minimum ( boxes [:, 2 ], w ) boxes [:, 3 ] = np . minimum ( boxes [:, 3 ], h ) return boxes . reshape ( orig_shape ) filter_boxes_inside_shape def filter_boxes_inside_shape ( boxes , shape ) Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) View Source def filter_boxes_inside_shape ( boxes , shape ): \"\"\" Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) \"\"\" assert boxes . ndim == 2 , boxes . shape assert len ( shape ) == 2 , shape h , w = shape indices = np . where ( ( boxes [:, 0 ] >= 0 ) & ( boxes [:, 1 ] >= 0 ) & ( boxes [:, 2 ] <= w ) & ( boxes [:, 3 ] <= h ))[ 0 ] return indices , boxes [ indices , :] point8_to_box def point8_to_box ( points ) Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) View Source def point8_to_box ( points ): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points . reshape (( - 1 , 4 , 2 )) minxy = p . min ( axis = 1 ) # nx2 maxxy = p . max ( axis = 1 ) # nx2 return np . concatenate (( minxy , maxxy ), axis = 1 ) polygons_to_mask def polygons_to_mask ( polys , height , width ) Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) View Source def polygons_to_mask ( polys , height , width ): \"\"\" Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) \"\"\" polys = [ p . flatten () . tolist () for p in polys ] assert len ( polys ) > 0 , \"Polygons are empty!\" import pycocotools.mask as cocomask rles = cocomask . frPyObjects ( polys , height , width ) rle = cocomask . merge ( rles ) return cocomask . decode ( rle ) Classes CustomResize class CustomResize ( short_edge_length , max_size , interp = 1 ) Try resizing the shortest edge to a certain number while avoiding the longest edge to exceed max_size. Ancestors (in MRO) tensorpack.dataflow.imgaug.base.ImageAugmentor Methods augment def augment ( self , img ) Create a transform, and apply it to augment the input image. This can save you one line of code, when you only care the augmentation of \"one image\". It will not return the :class: Transform object to you so you won't be able to apply the same transformation on other data associated with the image. Args: img (ndarray): see notes of this class on the requirements. Returns: img: augmented image. View Source def augment ( self , img ) : \" \"\" Create a transform, and apply it to augment the input image. This can save you one line of code, when you only care the augmentation of \" one image \". It will not return the :class:`Transform` object to you so you won't be able to apply the same transformation on other data associated with the image. Args: img (ndarray): see notes of this class on the requirements. Returns: img: augmented image. \"\" \" check_dtype ( img ) t = self . get_transform ( img ) return t . apply_image ( img ) augment_coords def augment_coords ( self , coords , param ) View Source @deprecated ( \"Please use `transform.apply_coords` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_coords ( self , coords , param ) : return param . apply_coords ( coords ) augment_return_params def augment_return_params ( self , d ) View Source @deprecated ( \"Please use `get_transform` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_return_params ( self , d ) : t = self . get_transform ( d ) return t . apply_image ( d ), t augment_with_params def augment_with_params ( self , d , param ) View Source @deprecated ( \"Please use `transform.apply_image` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_with_params ( self , d , param ) : return param . apply_image ( d ) get_transform def get_transform ( self , img ) View Source def get_transform ( self , img ): h , w = img . shape [: 2 ] size = self . rng . randint ( self . short_edge_length [ 0 ], self . short_edge_length [ 1 ] + 1 ) scale = size * 1 . 0 / min ( h , w ) if h < w : newh , neww = size , scale * w else : newh , neww = scale * h , size if max ( newh , neww ) > self . max_size : scale = self . max_size * 1 . 0 / max ( newh , neww ) newh = newh * scale neww = neww * scale neww = int ( neww + 0 . 5 ) newh = int ( newh + 0 . 5 ) return ResizeTransform ( h , w , newh , neww , self . interp ) reset_state def reset_state ( self ) Reset rng and other state of the augmentor. Similar to :meth: DataFlow.reset_state , the caller of Augmentor is responsible for calling this method (once or more times) in the process that uses the augmentor before using it. If you use a built-in augmentation dataflow (:class: AugmentImageComponent , etc), this method will be called in the dataflow's own reset_state method. If you use Python\u22653.7 on Unix, this method will be automatically called after fork, and you do not need to bother calling it. View Source def reset_state ( self ) : \" \"\" Reset rng and other state of the augmentor. Similar to :meth:`DataFlow.reset_state`, the caller of Augmentor is responsible for calling this method (once or more times) in the **process that uses the augmentor** before using it. If you use a built-in augmentation dataflow (:class:`AugmentImageComponent`, etc), this method will be called in the dataflow's own `reset_state` method. If you use Python\u22653.7 on Unix, this method will be automatically called after fork, and you do not need to bother calling it. \"\" \" self . rng = get_rng ( self ) DataFromListOfDict class DataFromListOfDict ( lst , keys , shuffle = False ) A DataFlow with RNG Ancestors (in MRO) tensorpack.dataflow.base.RNGDataFlow tensorpack.dataflow.base.DataFlow Class variables rng Methods get_data def get_data ( self ) View Source def get_data ( self ): return self . __iter__ () reset_state def reset_state ( self ) Reset the RNG View Source def reset_state ( self ): \"\"\" Reset the RNG \"\"\" self . rng = get_rng ( self ) size def size ( self ) View Source def size ( self ): return self . __len__ ()","title":"Common"},{"location":"reference/mot/object_detection/common/#module-motobject_detectioncommon","text":"View Source # -*- coding: utf-8 -*- # File: common.py import numpy as np import cv2 from tensorpack.dataflow import RNGDataFlow from tensorpack.dataflow.imgaug import ImageAugmentor , ResizeTransform class DataFromListOfDict ( RNGDataFlow ): def __init__ ( self , lst , keys , shuffle = False ): self . _lst = lst self . _keys = keys self . _shuffle = shuffle self . _size = len ( lst ) def __len__ ( self ): return self . _size def __iter__ ( self ): if self . _shuffle : self . rng . shuffle ( self . _lst ) for dic in self . _lst : dp = [ dic [ k ] for k in self . _keys ] yield dp class CustomResize ( ImageAugmentor ): \"\"\" Try resizing the shortest edge to a certain number while avoiding the longest edge to exceed max_size. \"\"\" def __init__ ( self , short_edge_length , max_size , interp = cv2 . INTER_LINEAR ): \"\"\" Args: short_edge_length ([int, int]): a [min, max] interval from which to sample the shortest edge length. max_size (int): maximum allowed longest edge length. \"\"\" super ( CustomResize , self ) . __init__ () if isinstance ( short_edge_length , int ): short_edge_length = ( short_edge_length , short_edge_length ) self . _init ( locals ()) def get_transform ( self , img ): h , w = img . shape [: 2 ] size = self . rng . randint ( self . short_edge_length [ 0 ], self . short_edge_length [ 1 ] + 1 ) scale = size * 1.0 / min ( h , w ) if h < w : newh , neww = size , scale * w else : newh , neww = scale * h , size if max ( newh , neww ) > self . max_size : scale = self . max_size * 1.0 / max ( newh , neww ) newh = newh * scale neww = neww * scale neww = int ( neww + 0.5 ) newh = int ( newh + 0.5 ) return ResizeTransform ( h , w , newh , neww , self . interp ) def box_to_point8 ( boxes ): \"\"\" Args: boxes: nx4 Returns: (nx4)x2 \"\"\" b = boxes [:, [ 0 , 1 , 2 , 3 , 0 , 3 , 2 , 1 ]] b = b . reshape (( - 1 , 2 )) return b def point8_to_box ( points ): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points . reshape (( - 1 , 4 , 2 )) minxy = p . min ( axis = 1 ) # nx2 maxxy = p . max ( axis = 1 ) # nx2 return np . concatenate (( minxy , maxxy ), axis = 1 ) def polygons_to_mask ( polys , height , width ): \"\"\" Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) \"\"\" polys = [ p . flatten () . tolist () for p in polys ] assert len ( polys ) > 0 , \"Polygons are empty!\" import pycocotools.mask as cocomask rles = cocomask . frPyObjects ( polys , height , width ) rle = cocomask . merge ( rles ) return cocomask . decode ( rle ) def clip_boxes ( boxes , shape ): \"\"\" Args: boxes: (...)x4, float shape: h, w \"\"\" orig_shape = boxes . shape boxes = boxes . reshape ([ - 1 , 4 ]) h , w = shape boxes [:, [ 0 , 1 ]] = np . maximum ( boxes [:, [ 0 , 1 ]], 0 ) boxes [:, 2 ] = np . minimum ( boxes [:, 2 ], w ) boxes [:, 3 ] = np . minimum ( boxes [:, 3 ], h ) return boxes . reshape ( orig_shape ) def filter_boxes_inside_shape ( boxes , shape ): \"\"\" Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) \"\"\" assert boxes . ndim == 2 , boxes . shape assert len ( shape ) == 2 , shape h , w = shape indices = np . where ( ( boxes [:, 0 ] >= 0 ) & ( boxes [:, 1 ] >= 0 ) & ( boxes [:, 2 ] <= w ) & ( boxes [:, 3 ] <= h ))[ 0 ] return indices , boxes [ indices , :] try : import pycocotools.mask as cocomask # Much faster than utils/np_box_ops def np_iou ( A , B ): def to_xywh ( box ): box = box . copy () box [:, 2 ] -= box [:, 0 ] box [:, 3 ] -= box [:, 1 ] return box ret = cocomask . iou ( to_xywh ( A ), to_xywh ( B ), np . zeros (( len ( B ),), dtype = np . bool )) # can accelerate even more, if using float32 return ret . astype ( 'float32' ) except ImportError : from mot.object_detection.utils.np_box_ops import iou as np_iou # noqa","title":"Module mot.object_detection.common"},{"location":"reference/mot/object_detection/common/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/common/#box_to_point8","text":"def box_to_point8 ( boxes ) Args: boxes: nx4 Returns: (nx4)x2 View Source def box_to_point8 ( boxes ): \"\"\" Args: boxes: nx4 Returns: (nx4)x2 \"\"\" b = boxes [:, [ 0 , 1 , 2 , 3 , 0 , 3 , 2 , 1 ]] b = b . reshape (( - 1 , 2 )) return b","title":"box_to_point8"},{"location":"reference/mot/object_detection/common/#clip_boxes","text":"def clip_boxes ( boxes , shape ) Args: boxes: (...)x4, float shape: h, w View Source def clip_boxes ( boxes , shape ): \"\"\" Args: boxes: (...)x4, float shape: h, w \"\"\" orig_shape = boxes . shape boxes = boxes . reshape ([ - 1 , 4 ]) h , w = shape boxes [:, [ 0 , 1 ]] = np . maximum ( boxes [:, [ 0 , 1 ]], 0 ) boxes [:, 2 ] = np . minimum ( boxes [:, 2 ], w ) boxes [:, 3 ] = np . minimum ( boxes [:, 3 ], h ) return boxes . reshape ( orig_shape )","title":"clip_boxes"},{"location":"reference/mot/object_detection/common/#filter_boxes_inside_shape","text":"def filter_boxes_inside_shape ( boxes , shape ) Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) View Source def filter_boxes_inside_shape ( boxes , shape ): \"\"\" Args: boxes: (nx4), float shape: (h, w) Returns: indices: (k, ) selection: (kx4) \"\"\" assert boxes . ndim == 2 , boxes . shape assert len ( shape ) == 2 , shape h , w = shape indices = np . where ( ( boxes [:, 0 ] >= 0 ) & ( boxes [:, 1 ] >= 0 ) & ( boxes [:, 2 ] <= w ) & ( boxes [:, 3 ] <= h ))[ 0 ] return indices , boxes [ indices , :]","title":"filter_boxes_inside_shape"},{"location":"reference/mot/object_detection/common/#point8_to_box","text":"def point8_to_box ( points ) Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) View Source def point8_to_box ( points ): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points . reshape (( - 1 , 4 , 2 )) minxy = p . min ( axis = 1 ) # nx2 maxxy = p . max ( axis = 1 ) # nx2 return np . concatenate (( minxy , maxxy ), axis = 1 )","title":"point8_to_box"},{"location":"reference/mot/object_detection/common/#polygons_to_mask","text":"def polygons_to_mask ( polys , height , width ) Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) View Source def polygons_to_mask ( polys , height , width ): \"\"\" Convert polygons to binary masks. Args: polys: a list of nx2 float array. Each array contains many (x, y) coordinates. Returns: a binary matrix of (height, width) \"\"\" polys = [ p . flatten () . tolist () for p in polys ] assert len ( polys ) > 0 , \"Polygons are empty!\" import pycocotools.mask as cocomask rles = cocomask . frPyObjects ( polys , height , width ) rle = cocomask . merge ( rles ) return cocomask . decode ( rle )","title":"polygons_to_mask"},{"location":"reference/mot/object_detection/common/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/common/#customresize","text":"class CustomResize ( short_edge_length , max_size , interp = 1 ) Try resizing the shortest edge to a certain number while avoiding the longest edge to exceed max_size.","title":"CustomResize"},{"location":"reference/mot/object_detection/common/#ancestors-in-mro","text":"tensorpack.dataflow.imgaug.base.ImageAugmentor","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/common/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/common/#augment","text":"def augment ( self , img ) Create a transform, and apply it to augment the input image. This can save you one line of code, when you only care the augmentation of \"one image\". It will not return the :class: Transform object to you so you won't be able to apply the same transformation on other data associated with the image. Args: img (ndarray): see notes of this class on the requirements. Returns: img: augmented image. View Source def augment ( self , img ) : \" \"\" Create a transform, and apply it to augment the input image. This can save you one line of code, when you only care the augmentation of \" one image \". It will not return the :class:`Transform` object to you so you won't be able to apply the same transformation on other data associated with the image. Args: img (ndarray): see notes of this class on the requirements. Returns: img: augmented image. \"\" \" check_dtype ( img ) t = self . get_transform ( img ) return t . apply_image ( img )","title":"augment"},{"location":"reference/mot/object_detection/common/#augment_coords","text":"def augment_coords ( self , coords , param ) View Source @deprecated ( \"Please use `transform.apply_coords` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_coords ( self , coords , param ) : return param . apply_coords ( coords )","title":"augment_coords"},{"location":"reference/mot/object_detection/common/#augment_return_params","text":"def augment_return_params ( self , d ) View Source @deprecated ( \"Please use `get_transform` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_return_params ( self , d ) : t = self . get_transform ( d ) return t . apply_image ( d ), t","title":"augment_return_params"},{"location":"reference/mot/object_detection/common/#augment_with_params","text":"def augment_with_params ( self , d , param ) View Source @deprecated ( \"Please use `transform.apply_image` instead!\" , \"2020-06-06\" , max_num_warnings = 3 ) def augment_with_params ( self , d , param ) : return param . apply_image ( d )","title":"augment_with_params"},{"location":"reference/mot/object_detection/common/#get_transform","text":"def get_transform ( self , img ) View Source def get_transform ( self , img ): h , w = img . shape [: 2 ] size = self . rng . randint ( self . short_edge_length [ 0 ], self . short_edge_length [ 1 ] + 1 ) scale = size * 1 . 0 / min ( h , w ) if h < w : newh , neww = size , scale * w else : newh , neww = scale * h , size if max ( newh , neww ) > self . max_size : scale = self . max_size * 1 . 0 / max ( newh , neww ) newh = newh * scale neww = neww * scale neww = int ( neww + 0 . 5 ) newh = int ( newh + 0 . 5 ) return ResizeTransform ( h , w , newh , neww , self . interp )","title":"get_transform"},{"location":"reference/mot/object_detection/common/#reset_state","text":"def reset_state ( self ) Reset rng and other state of the augmentor. Similar to :meth: DataFlow.reset_state , the caller of Augmentor is responsible for calling this method (once or more times) in the process that uses the augmentor before using it. If you use a built-in augmentation dataflow (:class: AugmentImageComponent , etc), this method will be called in the dataflow's own reset_state method. If you use Python\u22653.7 on Unix, this method will be automatically called after fork, and you do not need to bother calling it. View Source def reset_state ( self ) : \" \"\" Reset rng and other state of the augmentor. Similar to :meth:`DataFlow.reset_state`, the caller of Augmentor is responsible for calling this method (once or more times) in the **process that uses the augmentor** before using it. If you use a built-in augmentation dataflow (:class:`AugmentImageComponent`, etc), this method will be called in the dataflow's own `reset_state` method. If you use Python\u22653.7 on Unix, this method will be automatically called after fork, and you do not need to bother calling it. \"\" \" self . rng = get_rng ( self )","title":"reset_state"},{"location":"reference/mot/object_detection/common/#datafromlistofdict","text":"class DataFromListOfDict ( lst , keys , shuffle = False ) A DataFlow with RNG","title":"DataFromListOfDict"},{"location":"reference/mot/object_detection/common/#ancestors-in-mro_1","text":"tensorpack.dataflow.base.RNGDataFlow tensorpack.dataflow.base.DataFlow","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/common/#class-variables","text":"rng","title":"Class variables"},{"location":"reference/mot/object_detection/common/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/object_detection/common/#get_data","text":"def get_data ( self ) View Source def get_data ( self ): return self . __iter__ ()","title":"get_data"},{"location":"reference/mot/object_detection/common/#reset_state_1","text":"def reset_state ( self ) Reset the RNG View Source def reset_state ( self ): \"\"\" Reset the RNG \"\"\" self . rng = get_rng ( self )","title":"reset_state"},{"location":"reference/mot/object_detection/common/#size","text":"def size ( self ) View Source def size ( self ): return self . __len__ ()","title":"size"},{"location":"reference/mot/object_detection/config/","text":"Module mot.object_detection.config View Source # -*- coding: utf-8 -*- # File: config.py import numpy as np import os import pprint import six from tensorpack.utils import logger from tensorpack.utils.gpu import get_num_gpu __all__ = [ 'config' , 'finalize_configs' ] class AttrDict (): _freezed = False \"\"\" Avoid accidental creation of new hierarchies. \"\"\" def __getattr__ ( self , name ): if self . _freezed : raise AttributeError ( name ) if name . startswith ( '_' ): # Do not mess with internals. Otherwise copy/pickle will fail raise AttributeError ( name ) ret = AttrDict () setattr ( self , name , ret ) return ret def __setattr__ ( self , name , value ): if self . _freezed and name not in self . __dict__ : raise AttributeError ( \"Config was freezed! Unknown config: {} \" . format ( name )) super () . __setattr__ ( name , value ) def __str__ ( self ): return pprint . pformat ( self . to_dict (), indent = 1 , width = 100 , compact = True ) __repr__ = __str__ def to_dict ( self ): \"\"\"Convert to a nested dict. \"\"\" return { k : v . to_dict () if isinstance ( v , AttrDict ) else v for k , v in self . __dict__ . items () if not k . startswith ( '_' )} def update_args ( self , args ): \"\"\"Update from command line args. \"\"\" for cfg in args : keys , v = cfg . split ( '=' , maxsplit = 1 ) keylist = keys . split ( '.' ) dic = self for i , k in enumerate ( keylist [: - 1 ]): assert k in dir ( dic ), \"Unknown config key: {} \" . format ( keys ) dic = getattr ( dic , k ) key = keylist [ - 1 ] oldv = getattr ( dic , key ) if not isinstance ( oldv , str ): v = eval ( v ) setattr ( dic , key , v ) def freeze ( self , freezed = True ): self . _freezed = freezed for v in self . __dict__ . values (): if isinstance ( v , AttrDict ): v . freeze ( freezed ) # avoid silent bugs def __eq__ ( self , _ ): raise NotImplementedError () def __ne__ ( self , _ ): raise NotImplementedError () config = AttrDict () _C = config # short alias to avoid coding # mode flags --------------------- _C . TRAINER = 'replicated' # options: 'horovod', 'replicated' _C . MODE_MASK = False # Faster R-CNN or Mask R-CNN _C . MODE_FPN = True # dataset ----------------------- _C . DATA . BASEDIR = '/workspace/mot/dataset_surfrider_cleaned' # All TRAIN dataset will be concatenated for training. _C . DATA . TRAIN = ( 'mot_train' ,) # Each VAL dataset will be evaluated separately (instead of concatenated) _C . DATA . VAL = ( 'mot_val' ,) # These two configs will be populated later inside `finalize_configs`. _C . DATA . NUM_CATEGORY = - 1 # without the background class (e.g., 80 for COCO) _C . DATA . CLASS_NAMES = [] # NUM_CLASS (NUM_CATEGORY+1) strings, the first is \"BG\". # whether the coordinates in your registered dataset are # absolute pixel values in range [0, W or H] or relative values in [0, 1] _C . DATA . ABSOLUTE_COORD = True # Number of data loading workers. # In case of horovod training, this is the number of workers per-GPU (so you may want to use a smaller number). # Set to 0 to disable parallel data loading _C . DATA . NUM_WORKERS = 10 # backbone ---------------------- _C . BACKBONE . WEIGHTS = '' # To train from scratch, set it to empty, and set FREEZE_AT to 0 # To train from ImageNet pre-trained models, use the one that matches your # architecture from http://models.tensorpack.com under the 'FasterRCNN' section. # To train from an existing COCO model, use the path to that file, and change # the other configurations according to that model. _C . BACKBONE . RESNET_NUM_BLOCKS = [ 3 , 4 , 6 , 3 ] # for resnet50 # RESNET_NUM_BLOCKS = [3, 4, 23, 3] # for resnet101 _C . BACKBONE . FREEZE_AFFINE = False # do not train affine parameters inside norm layers _C . BACKBONE . NORM = 'FreezeBN' # options: FreezeBN, SyncBN, GN, None _C . BACKBONE . FREEZE_AT = 2 # options: 0, 1, 2. How many stages in backbone to freeze (not training) # Use a base model with TF-preferred padding mode, # which may pad more pixels on right/bottom than top/left. # See https://github.com/tensorflow/tensorflow/issues/18213 # In tensorpack model zoo, ResNet models with TF_PAD_MODE=False are marked with \"-AlignPadding\". # All other models under `ResNet/` in the model zoo are using TF_PAD_MODE=True. # Using either one should probably give the same performance. # We use the \"AlignPadding\" one just to be consistent with caffe2. _C . BACKBONE . TF_PAD_MODE = False _C . BACKBONE . STRIDE_1X1 = False # True for MSRA models # schedule ----------------------- _C . TRAIN . NUM_GPUS = None # by default, will be set from code _C . TRAIN . WEIGHT_DECAY = 1e-4 _C . TRAIN . BASE_LR = 1e-2 # defined for total batch size=8. Otherwise it will be adjusted automatically _C . TRAIN . WARMUP = 1000 # in terms of iterations. This is not affected by #GPUs _C . TRAIN . WARMUP_INIT_LR = 1e-2 * 0.33 # defined for total batch size=8. Otherwise it will be adjusted automatically _C . TRAIN . STEPS_PER_EPOCH = 500 _C . TRAIN . STARTING_EPOCH = 1 # the first epoch to start with, useful to continue a training # LR_SCHEDULE means equivalent steps when the total batch size is 8. # When the total bs!=8, the actual iterations to decrease learning rate, and # the base learning rate are computed from BASE_LR and LR_SCHEDULE. # Therefore, there is *no need* to modify the config if you only change the number of GPUs. _C . TRAIN . LR_SCHEDULE = \"1x\" # \"1x\" schedule in detectron _C . TRAIN . EVAL_PERIOD = 50 # period (epochs) to run evaluation _C . TRAIN . CHECKPOINT_PERIOD = 20 # period (epochs) to save model # preprocessing -------------------- # Alternative old (worse & faster) setting: 600 _C . PREPROC . TRAIN_SHORT_EDGE_SIZE = [ 800 , 800 ] # [min, max] to sample from _C . PREPROC . TEST_SHORT_EDGE_SIZE = 800 _C . PREPROC . MAX_SIZE = 1333 # mean and std in RGB order. # Un-scaled version: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] _C . PREPROC . PIXEL_MEAN = [ 123.675 , 116.28 , 103.53 ] _C . PREPROC . PIXEL_STD = [ 58.395 , 57.12 , 57.375 ] # anchors ------------------------- _C . RPN . ANCHOR_STRIDE = 16 _C . RPN . ANCHOR_SIZES = ( 32 , 64 , 128 , 256 , 512 ) # sqrtarea of the anchor box _C . RPN . ANCHOR_RATIOS = ( 0.5 , 1. , 2. ) _C . RPN . POSITIVE_ANCHOR_THRESH = 0.7 _C . RPN . NEGATIVE_ANCHOR_THRESH = 0.3 # rpn training ------------------------- _C . RPN . FG_RATIO = 0.5 # fg ratio among selected RPN anchors _C . RPN . BATCH_PER_IM = 256 # total (across FPN levels) number of anchors that are marked valid _C . RPN . MIN_SIZE = 0 _C . RPN . PROPOSAL_NMS_THRESH = 0.7 # Anchors which overlap with a crowd box (IOA larger than threshold) will be ignored. # Setting this to a value larger than 1.0 will disable the feature. # It is disabled by default because Detectron does not do this. _C . RPN . CROWD_OVERLAP_THRESH = 9.99 _C . RPN . HEAD_DIM = 1024 # used in C4 only # RPN proposal selection ------------------------------- # for C4 _C . RPN . TRAIN_PRE_NMS_TOPK = 12000 _C . RPN . TRAIN_POST_NMS_TOPK = 2000 _C . RPN . TEST_PRE_NMS_TOPK = 6000 _C . RPN . TEST_POST_NMS_TOPK = 1000 # if you encounter OOM in inference, set this to a smaller number # for FPN, #proposals per-level and #proposals after merging are (for now) the same # if FPN.PROPOSAL_MODE = 'Joint', these options have no effect _C . RPN . TRAIN_PER_LEVEL_NMS_TOPK = 2000 _C . RPN . TEST_PER_LEVEL_NMS_TOPK = 1000 # fastrcnn training --------------------- _C . FRCNN . BATCH_PER_IM = 512 _C . FRCNN . BBOX_REG_WEIGHTS = [ 10. , 10. , 5. , 5. ] # Slightly better setting: 20, 20, 10, 10 _C . FRCNN . FG_THRESH = 0.5 _C . FRCNN . FG_RATIO = 0.25 # fg ratio in a ROI batch # FPN ------------------------- _C . FPN . ANCHOR_STRIDES = ( 4 , 8 , 16 , 32 , 64 ) # strides for each FPN level. Must be the same length as ANCHOR_SIZES _C . FPN . PROPOSAL_MODE = 'Level' # 'Level', 'Joint' _C . FPN . NUM_CHANNEL = 256 _C . FPN . NORM = 'None' # 'None', 'GN' # The head option is only used in FPN. For C4 models, the head is C5 _C . FPN . FRCNN_HEAD_FUNC = 'fastrcnn_2fc_head' # choices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head _C . FPN . FRCNN_CONV_HEAD_DIM = 256 _C . FPN . FRCNN_FC_HEAD_DIM = 1024 _C . FPN . MRCNN_HEAD_FUNC = 'maskrcnn_up4conv_head' # choices: maskrcnn_up4conv_{,gn_}head # Mask R-CNN _C . MRCNN . HEAD_DIM = 256 _C . MRCNN . ACCURATE_PASTE = True # slightly more aligned results, but very slow on numpy # Cascade R-CNN, only available in FPN mode _C . FPN . CASCADE = False _C . CASCADE . IOUS = [ 0.5 , 0.6 , 0.7 ] _C . CASCADE . BBOX_REG_WEIGHTS = [[ 10. , 10. , 5. , 5. ], [ 20. , 20. , 10. , 10. ], [ 30. , 30. , 15. , 15. ]] # testing ----------------------- _C . TEST . FRCNN_NMS_THRESH = 0.5 # Smaller threshold value gives significantly better mAP. But we use 0.05 for consistency with Detectron. # mAP with 1e-4 threshold can be found at https://github.com/tensorpack/tensorpack/commit/26321ae58120af2568bdbf2269f32aa708d425a8#diff-61085c48abee915b584027e1085e1043 # noqa _C . TEST . RESULT_SCORE_THRESH = 0.05 _C . TEST . RESULT_SCORE_THRESH_VIS = 0.5 # only visualize confident results _C . TEST . RESULTS_PER_IM = 100 _C . freeze () # avoid typo / wrong config keys def finalize_configs ( is_training ): \"\"\" Run some sanity checks, and populate some configs from others \"\"\" _C . freeze ( False ) # populate new keys now if isinstance ( _C . DATA . VAL , six . string_types ): # support single string (the typical case) as well _C . DATA . VAL = ( _C . DATA . VAL , ) if isinstance ( _C . DATA . TRAIN , six . string_types ): # support single string _C . DATA . TRAIN = ( _C . DATA . TRAIN , ) # finalize dataset definitions ... from mot.object_detection.dataset import DatasetRegistry datasets = list ( _C . DATA . TRAIN ) + list ( _C . DATA . VAL ) _C . DATA . CLASS_NAMES = DatasetRegistry . get_metadata ( datasets [ 0 ], \"class_names\" ) _C . DATA . NUM_CATEGORY = len ( _C . DATA . CLASS_NAMES ) - 1 assert _C . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' , 'GN' , 'None' ], _C . BACKBONE . NORM if _C . BACKBONE . NORM != 'FreezeBN' : assert not _C . BACKBONE . FREEZE_AFFINE assert _C . BACKBONE . FREEZE_AT in [ 0 , 1 , 2 ] _C . RPN . NUM_ANCHOR = len ( _C . RPN . ANCHOR_SIZES ) * len ( _C . RPN . ANCHOR_RATIOS ) assert len ( _C . FPN . ANCHOR_STRIDES ) == len ( _C . RPN . ANCHOR_SIZES ) # image size into the backbone has to be multiple of this number _C . FPN . RESOLUTION_REQUIREMENT = _C . FPN . ANCHOR_STRIDES [ 3 ] # [3] because we build FPN with features r2,r3,r4,r5 if _C . MODE_FPN : size_mult = _C . FPN . RESOLUTION_REQUIREMENT * 1. _C . PREPROC . MAX_SIZE = np . ceil ( _C . PREPROC . MAX_SIZE / size_mult ) * size_mult assert _C . FPN . PROPOSAL_MODE in [ 'Level' , 'Joint' ] assert _C . FPN . FRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . MRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . NORM in [ 'None' , 'GN' ] if _C . FPN . CASCADE : # the first threshold is the proposal sampling threshold assert _C . CASCADE . IOUS [ 0 ] == _C . FRCNN . FG_THRESH assert len ( _C . CASCADE . BBOX_REG_WEIGHTS ) == len ( _C . CASCADE . IOUS ) if is_training : train_scales = _C . PREPROC . TRAIN_SHORT_EDGE_SIZE if isinstance ( train_scales , ( list , tuple )) and train_scales [ 1 ] - train_scales [ 0 ] > 100 : # don't autotune if augmentation is on os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' os . environ [ 'TF_AUTOTUNE_THRESHOLD' ] = '1' assert _C . TRAINER in [ 'horovod' , 'replicated' ], _C . TRAINER lr = _C . TRAIN . LR_SCHEDULE if isinstance ( lr , six . string_types ): if lr . endswith ( \"x\" ): LR_SCHEDULE_KITER = { \" {} x\" . format ( k ): [ 180 * k - 120 , 180 * k - 40 , 180 * k ] for k in range ( 2 , 10 )} LR_SCHEDULE_KITER [ \"1x\" ] = [ 120 , 160 , 180 ] _C . TRAIN . LR_SCHEDULE = [ x * 1000 for x in LR_SCHEDULE_KITER [ lr ]] else : _C . TRAIN . LR_SCHEDULE = eval ( lr ) # setup NUM_GPUS if _C . TRAINER == 'horovod' : import horovod.tensorflow as hvd ngpu = hvd . size () logger . info ( \"Horovod Rank= {} , Size= {} , LocalRank= {} \" . format ( hvd . rank (), hvd . size (), hvd . local_rank ())) else : assert 'OMPI_COMM_WORLD_SIZE' not in os . environ ngpu = get_num_gpu () assert ngpu > 0 , \"Has to train with GPU!\" assert ngpu % 8 == 0 or 8 % ngpu == 0 , \"Can only train with 1,2,4 or >=8 GPUs, but found {} GPUs\" . format ( ngpu ) else : # autotune is too slow for inference os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' ngpu = get_num_gpu () if _C . TRAIN . NUM_GPUS is None : _C . TRAIN . NUM_GPUS = ngpu else : if _C . TRAINER == 'horovod' : assert _C . TRAIN . NUM_GPUS == ngpu else : assert _C . TRAIN . NUM_GPUS <= ngpu _C . freeze () logger . info ( \"Config: ------------------------------------------ \\n \" + str ( _C )) Variables config Functions finalize_configs def finalize_configs ( is_training ) Run some sanity checks, and populate some configs from others View Source def finalize_configs ( is_training ): \"\"\" Run some sanity checks, and populate some configs from others \"\"\" _C . freeze ( False ) # populate new keys now if isinstance ( _C . DATA . VAL , six . string_types ): # support single string (the typical case) as well _C . DATA . VAL = ( _C . DATA . VAL , ) if isinstance ( _C . DATA . TRAIN , six . string_types ): # support single string _C . DATA . TRAIN = ( _C . DATA . TRAIN , ) # finalize dataset definitions ... from mot.object_detection.dataset import DatasetRegistry datasets = list ( _C . DATA . TRAIN ) + list ( _C . DATA . VAL ) _C . DATA . CLASS_NAMES = DatasetRegistry . get_metadata ( datasets [ 0 ], \"class_names\" ) _C . DATA . NUM_CATEGORY = len ( _C . DATA . CLASS_NAMES ) - 1 assert _C . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' , 'GN' , 'None' ], _C . BACKBONE . NORM if _C . BACKBONE . NORM != 'FreezeBN' : assert not _C . BACKBONE . FREEZE_AFFINE assert _C . BACKBONE . FREEZE_AT in [ 0 , 1 , 2 ] _C . RPN . NUM_ANCHOR = len ( _C . RPN . ANCHOR_SIZES ) * len ( _C . RPN . ANCHOR_RATIOS ) assert len ( _C . FPN . ANCHOR_STRIDES ) == len ( _C . RPN . ANCHOR_SIZES ) # image size into the backbone has to be multiple of this number _C . FPN . RESOLUTION_REQUIREMENT = _C . FPN . ANCHOR_STRIDES [ 3 ] # [3] because we build FPN with features r2,r3,r4,r5 if _C . MODE_FPN : size_mult = _C . FPN . RESOLUTION_REQUIREMENT * 1. _C . PREPROC . MAX_SIZE = np . ceil ( _C . PREPROC . MAX_SIZE / size_mult ) * size_mult assert _C . FPN . PROPOSAL_MODE in [ 'Level' , 'Joint' ] assert _C . FPN . FRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . MRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . NORM in [ 'None' , 'GN' ] if _C . FPN . CASCADE : # the first threshold is the proposal sampling threshold assert _C . CASCADE . IOUS [ 0 ] == _C . FRCNN . FG_THRESH assert len ( _C . CASCADE . BBOX_REG_WEIGHTS ) == len ( _C . CASCADE . IOUS ) if is_training : train_scales = _C . PREPROC . TRAIN_SHORT_EDGE_SIZE if isinstance ( train_scales , ( list , tuple )) and train_scales [ 1 ] - train_scales [ 0 ] > 100 : # don't autotune if augmentation is on os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' os . environ [ 'TF_AUTOTUNE_THRESHOLD' ] = '1' assert _C . TRAINER in [ 'horovod' , 'replicated' ], _C . TRAINER lr = _C . TRAIN . LR_SCHEDULE if isinstance ( lr , six . string_types ): if lr . endswith ( \"x\" ): LR_SCHEDULE_KITER = { \"{}x\" . format ( k ): [ 180 * k - 120 , 180 * k - 40 , 180 * k ] for k in range ( 2 , 10 )} LR_SCHEDULE_KITER [ \"1x\" ] = [ 120 , 160 , 180 ] _C . TRAIN . LR_SCHEDULE = [ x * 1000 for x in LR_SCHEDULE_KITER [ lr ]] else : _C . TRAIN . LR_SCHEDULE = eval ( lr ) # setup NUM_GPUS if _C . TRAINER == 'horovod' : import horovod.tensorflow as hvd ngpu = hvd . size () logger . info ( \"Horovod Rank={}, Size={}, LocalRank={}\" . format ( hvd . rank (), hvd . size (), hvd . local_rank ())) else : assert 'OMPI_COMM_WORLD_SIZE' not in os . environ ngpu = get_num_gpu () assert ngpu > 0 , \"Has to train with GPU!\" assert ngpu % 8 == 0 or 8 % ngpu == 0 , \"Can only train with 1,2,4 or >=8 GPUs, but found {} GPUs\" . format ( ngpu ) else : # autotune is too slow for inference os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' ngpu = get_num_gpu () if _C . TRAIN . NUM_GPUS is None : _C . TRAIN . NUM_GPUS = ngpu else : if _C . TRAINER == 'horovod' : assert _C . TRAIN . NUM_GPUS == ngpu else : assert _C . TRAIN . NUM_GPUS <= ngpu _C . freeze () logger . info ( \"Config: ------------------------------------------ \\n \" + str ( _C ))","title":"Config"},{"location":"reference/mot/object_detection/config/#module-motobject_detectionconfig","text":"View Source # -*- coding: utf-8 -*- # File: config.py import numpy as np import os import pprint import six from tensorpack.utils import logger from tensorpack.utils.gpu import get_num_gpu __all__ = [ 'config' , 'finalize_configs' ] class AttrDict (): _freezed = False \"\"\" Avoid accidental creation of new hierarchies. \"\"\" def __getattr__ ( self , name ): if self . _freezed : raise AttributeError ( name ) if name . startswith ( '_' ): # Do not mess with internals. Otherwise copy/pickle will fail raise AttributeError ( name ) ret = AttrDict () setattr ( self , name , ret ) return ret def __setattr__ ( self , name , value ): if self . _freezed and name not in self . __dict__ : raise AttributeError ( \"Config was freezed! Unknown config: {} \" . format ( name )) super () . __setattr__ ( name , value ) def __str__ ( self ): return pprint . pformat ( self . to_dict (), indent = 1 , width = 100 , compact = True ) __repr__ = __str__ def to_dict ( self ): \"\"\"Convert to a nested dict. \"\"\" return { k : v . to_dict () if isinstance ( v , AttrDict ) else v for k , v in self . __dict__ . items () if not k . startswith ( '_' )} def update_args ( self , args ): \"\"\"Update from command line args. \"\"\" for cfg in args : keys , v = cfg . split ( '=' , maxsplit = 1 ) keylist = keys . split ( '.' ) dic = self for i , k in enumerate ( keylist [: - 1 ]): assert k in dir ( dic ), \"Unknown config key: {} \" . format ( keys ) dic = getattr ( dic , k ) key = keylist [ - 1 ] oldv = getattr ( dic , key ) if not isinstance ( oldv , str ): v = eval ( v ) setattr ( dic , key , v ) def freeze ( self , freezed = True ): self . _freezed = freezed for v in self . __dict__ . values (): if isinstance ( v , AttrDict ): v . freeze ( freezed ) # avoid silent bugs def __eq__ ( self , _ ): raise NotImplementedError () def __ne__ ( self , _ ): raise NotImplementedError () config = AttrDict () _C = config # short alias to avoid coding # mode flags --------------------- _C . TRAINER = 'replicated' # options: 'horovod', 'replicated' _C . MODE_MASK = False # Faster R-CNN or Mask R-CNN _C . MODE_FPN = True # dataset ----------------------- _C . DATA . BASEDIR = '/workspace/mot/dataset_surfrider_cleaned' # All TRAIN dataset will be concatenated for training. _C . DATA . TRAIN = ( 'mot_train' ,) # Each VAL dataset will be evaluated separately (instead of concatenated) _C . DATA . VAL = ( 'mot_val' ,) # These two configs will be populated later inside `finalize_configs`. _C . DATA . NUM_CATEGORY = - 1 # without the background class (e.g., 80 for COCO) _C . DATA . CLASS_NAMES = [] # NUM_CLASS (NUM_CATEGORY+1) strings, the first is \"BG\". # whether the coordinates in your registered dataset are # absolute pixel values in range [0, W or H] or relative values in [0, 1] _C . DATA . ABSOLUTE_COORD = True # Number of data loading workers. # In case of horovod training, this is the number of workers per-GPU (so you may want to use a smaller number). # Set to 0 to disable parallel data loading _C . DATA . NUM_WORKERS = 10 # backbone ---------------------- _C . BACKBONE . WEIGHTS = '' # To train from scratch, set it to empty, and set FREEZE_AT to 0 # To train from ImageNet pre-trained models, use the one that matches your # architecture from http://models.tensorpack.com under the 'FasterRCNN' section. # To train from an existing COCO model, use the path to that file, and change # the other configurations according to that model. _C . BACKBONE . RESNET_NUM_BLOCKS = [ 3 , 4 , 6 , 3 ] # for resnet50 # RESNET_NUM_BLOCKS = [3, 4, 23, 3] # for resnet101 _C . BACKBONE . FREEZE_AFFINE = False # do not train affine parameters inside norm layers _C . BACKBONE . NORM = 'FreezeBN' # options: FreezeBN, SyncBN, GN, None _C . BACKBONE . FREEZE_AT = 2 # options: 0, 1, 2. How many stages in backbone to freeze (not training) # Use a base model with TF-preferred padding mode, # which may pad more pixels on right/bottom than top/left. # See https://github.com/tensorflow/tensorflow/issues/18213 # In tensorpack model zoo, ResNet models with TF_PAD_MODE=False are marked with \"-AlignPadding\". # All other models under `ResNet/` in the model zoo are using TF_PAD_MODE=True. # Using either one should probably give the same performance. # We use the \"AlignPadding\" one just to be consistent with caffe2. _C . BACKBONE . TF_PAD_MODE = False _C . BACKBONE . STRIDE_1X1 = False # True for MSRA models # schedule ----------------------- _C . TRAIN . NUM_GPUS = None # by default, will be set from code _C . TRAIN . WEIGHT_DECAY = 1e-4 _C . TRAIN . BASE_LR = 1e-2 # defined for total batch size=8. Otherwise it will be adjusted automatically _C . TRAIN . WARMUP = 1000 # in terms of iterations. This is not affected by #GPUs _C . TRAIN . WARMUP_INIT_LR = 1e-2 * 0.33 # defined for total batch size=8. Otherwise it will be adjusted automatically _C . TRAIN . STEPS_PER_EPOCH = 500 _C . TRAIN . STARTING_EPOCH = 1 # the first epoch to start with, useful to continue a training # LR_SCHEDULE means equivalent steps when the total batch size is 8. # When the total bs!=8, the actual iterations to decrease learning rate, and # the base learning rate are computed from BASE_LR and LR_SCHEDULE. # Therefore, there is *no need* to modify the config if you only change the number of GPUs. _C . TRAIN . LR_SCHEDULE = \"1x\" # \"1x\" schedule in detectron _C . TRAIN . EVAL_PERIOD = 50 # period (epochs) to run evaluation _C . TRAIN . CHECKPOINT_PERIOD = 20 # period (epochs) to save model # preprocessing -------------------- # Alternative old (worse & faster) setting: 600 _C . PREPROC . TRAIN_SHORT_EDGE_SIZE = [ 800 , 800 ] # [min, max] to sample from _C . PREPROC . TEST_SHORT_EDGE_SIZE = 800 _C . PREPROC . MAX_SIZE = 1333 # mean and std in RGB order. # Un-scaled version: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] _C . PREPROC . PIXEL_MEAN = [ 123.675 , 116.28 , 103.53 ] _C . PREPROC . PIXEL_STD = [ 58.395 , 57.12 , 57.375 ] # anchors ------------------------- _C . RPN . ANCHOR_STRIDE = 16 _C . RPN . ANCHOR_SIZES = ( 32 , 64 , 128 , 256 , 512 ) # sqrtarea of the anchor box _C . RPN . ANCHOR_RATIOS = ( 0.5 , 1. , 2. ) _C . RPN . POSITIVE_ANCHOR_THRESH = 0.7 _C . RPN . NEGATIVE_ANCHOR_THRESH = 0.3 # rpn training ------------------------- _C . RPN . FG_RATIO = 0.5 # fg ratio among selected RPN anchors _C . RPN . BATCH_PER_IM = 256 # total (across FPN levels) number of anchors that are marked valid _C . RPN . MIN_SIZE = 0 _C . RPN . PROPOSAL_NMS_THRESH = 0.7 # Anchors which overlap with a crowd box (IOA larger than threshold) will be ignored. # Setting this to a value larger than 1.0 will disable the feature. # It is disabled by default because Detectron does not do this. _C . RPN . CROWD_OVERLAP_THRESH = 9.99 _C . RPN . HEAD_DIM = 1024 # used in C4 only # RPN proposal selection ------------------------------- # for C4 _C . RPN . TRAIN_PRE_NMS_TOPK = 12000 _C . RPN . TRAIN_POST_NMS_TOPK = 2000 _C . RPN . TEST_PRE_NMS_TOPK = 6000 _C . RPN . TEST_POST_NMS_TOPK = 1000 # if you encounter OOM in inference, set this to a smaller number # for FPN, #proposals per-level and #proposals after merging are (for now) the same # if FPN.PROPOSAL_MODE = 'Joint', these options have no effect _C . RPN . TRAIN_PER_LEVEL_NMS_TOPK = 2000 _C . RPN . TEST_PER_LEVEL_NMS_TOPK = 1000 # fastrcnn training --------------------- _C . FRCNN . BATCH_PER_IM = 512 _C . FRCNN . BBOX_REG_WEIGHTS = [ 10. , 10. , 5. , 5. ] # Slightly better setting: 20, 20, 10, 10 _C . FRCNN . FG_THRESH = 0.5 _C . FRCNN . FG_RATIO = 0.25 # fg ratio in a ROI batch # FPN ------------------------- _C . FPN . ANCHOR_STRIDES = ( 4 , 8 , 16 , 32 , 64 ) # strides for each FPN level. Must be the same length as ANCHOR_SIZES _C . FPN . PROPOSAL_MODE = 'Level' # 'Level', 'Joint' _C . FPN . NUM_CHANNEL = 256 _C . FPN . NORM = 'None' # 'None', 'GN' # The head option is only used in FPN. For C4 models, the head is C5 _C . FPN . FRCNN_HEAD_FUNC = 'fastrcnn_2fc_head' # choices: fastrcnn_2fc_head, fastrcnn_4conv1fc_{,gn_}head _C . FPN . FRCNN_CONV_HEAD_DIM = 256 _C . FPN . FRCNN_FC_HEAD_DIM = 1024 _C . FPN . MRCNN_HEAD_FUNC = 'maskrcnn_up4conv_head' # choices: maskrcnn_up4conv_{,gn_}head # Mask R-CNN _C . MRCNN . HEAD_DIM = 256 _C . MRCNN . ACCURATE_PASTE = True # slightly more aligned results, but very slow on numpy # Cascade R-CNN, only available in FPN mode _C . FPN . CASCADE = False _C . CASCADE . IOUS = [ 0.5 , 0.6 , 0.7 ] _C . CASCADE . BBOX_REG_WEIGHTS = [[ 10. , 10. , 5. , 5. ], [ 20. , 20. , 10. , 10. ], [ 30. , 30. , 15. , 15. ]] # testing ----------------------- _C . TEST . FRCNN_NMS_THRESH = 0.5 # Smaller threshold value gives significantly better mAP. But we use 0.05 for consistency with Detectron. # mAP with 1e-4 threshold can be found at https://github.com/tensorpack/tensorpack/commit/26321ae58120af2568bdbf2269f32aa708d425a8#diff-61085c48abee915b584027e1085e1043 # noqa _C . TEST . RESULT_SCORE_THRESH = 0.05 _C . TEST . RESULT_SCORE_THRESH_VIS = 0.5 # only visualize confident results _C . TEST . RESULTS_PER_IM = 100 _C . freeze () # avoid typo / wrong config keys def finalize_configs ( is_training ): \"\"\" Run some sanity checks, and populate some configs from others \"\"\" _C . freeze ( False ) # populate new keys now if isinstance ( _C . DATA . VAL , six . string_types ): # support single string (the typical case) as well _C . DATA . VAL = ( _C . DATA . VAL , ) if isinstance ( _C . DATA . TRAIN , six . string_types ): # support single string _C . DATA . TRAIN = ( _C . DATA . TRAIN , ) # finalize dataset definitions ... from mot.object_detection.dataset import DatasetRegistry datasets = list ( _C . DATA . TRAIN ) + list ( _C . DATA . VAL ) _C . DATA . CLASS_NAMES = DatasetRegistry . get_metadata ( datasets [ 0 ], \"class_names\" ) _C . DATA . NUM_CATEGORY = len ( _C . DATA . CLASS_NAMES ) - 1 assert _C . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' , 'GN' , 'None' ], _C . BACKBONE . NORM if _C . BACKBONE . NORM != 'FreezeBN' : assert not _C . BACKBONE . FREEZE_AFFINE assert _C . BACKBONE . FREEZE_AT in [ 0 , 1 , 2 ] _C . RPN . NUM_ANCHOR = len ( _C . RPN . ANCHOR_SIZES ) * len ( _C . RPN . ANCHOR_RATIOS ) assert len ( _C . FPN . ANCHOR_STRIDES ) == len ( _C . RPN . ANCHOR_SIZES ) # image size into the backbone has to be multiple of this number _C . FPN . RESOLUTION_REQUIREMENT = _C . FPN . ANCHOR_STRIDES [ 3 ] # [3] because we build FPN with features r2,r3,r4,r5 if _C . MODE_FPN : size_mult = _C . FPN . RESOLUTION_REQUIREMENT * 1. _C . PREPROC . MAX_SIZE = np . ceil ( _C . PREPROC . MAX_SIZE / size_mult ) * size_mult assert _C . FPN . PROPOSAL_MODE in [ 'Level' , 'Joint' ] assert _C . FPN . FRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . MRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . NORM in [ 'None' , 'GN' ] if _C . FPN . CASCADE : # the first threshold is the proposal sampling threshold assert _C . CASCADE . IOUS [ 0 ] == _C . FRCNN . FG_THRESH assert len ( _C . CASCADE . BBOX_REG_WEIGHTS ) == len ( _C . CASCADE . IOUS ) if is_training : train_scales = _C . PREPROC . TRAIN_SHORT_EDGE_SIZE if isinstance ( train_scales , ( list , tuple )) and train_scales [ 1 ] - train_scales [ 0 ] > 100 : # don't autotune if augmentation is on os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' os . environ [ 'TF_AUTOTUNE_THRESHOLD' ] = '1' assert _C . TRAINER in [ 'horovod' , 'replicated' ], _C . TRAINER lr = _C . TRAIN . LR_SCHEDULE if isinstance ( lr , six . string_types ): if lr . endswith ( \"x\" ): LR_SCHEDULE_KITER = { \" {} x\" . format ( k ): [ 180 * k - 120 , 180 * k - 40 , 180 * k ] for k in range ( 2 , 10 )} LR_SCHEDULE_KITER [ \"1x\" ] = [ 120 , 160 , 180 ] _C . TRAIN . LR_SCHEDULE = [ x * 1000 for x in LR_SCHEDULE_KITER [ lr ]] else : _C . TRAIN . LR_SCHEDULE = eval ( lr ) # setup NUM_GPUS if _C . TRAINER == 'horovod' : import horovod.tensorflow as hvd ngpu = hvd . size () logger . info ( \"Horovod Rank= {} , Size= {} , LocalRank= {} \" . format ( hvd . rank (), hvd . size (), hvd . local_rank ())) else : assert 'OMPI_COMM_WORLD_SIZE' not in os . environ ngpu = get_num_gpu () assert ngpu > 0 , \"Has to train with GPU!\" assert ngpu % 8 == 0 or 8 % ngpu == 0 , \"Can only train with 1,2,4 or >=8 GPUs, but found {} GPUs\" . format ( ngpu ) else : # autotune is too slow for inference os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' ngpu = get_num_gpu () if _C . TRAIN . NUM_GPUS is None : _C . TRAIN . NUM_GPUS = ngpu else : if _C . TRAINER == 'horovod' : assert _C . TRAIN . NUM_GPUS == ngpu else : assert _C . TRAIN . NUM_GPUS <= ngpu _C . freeze () logger . info ( \"Config: ------------------------------------------ \\n \" + str ( _C ))","title":"Module mot.object_detection.config"},{"location":"reference/mot/object_detection/config/#variables","text":"config","title":"Variables"},{"location":"reference/mot/object_detection/config/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/config/#finalize_configs","text":"def finalize_configs ( is_training ) Run some sanity checks, and populate some configs from others View Source def finalize_configs ( is_training ): \"\"\" Run some sanity checks, and populate some configs from others \"\"\" _C . freeze ( False ) # populate new keys now if isinstance ( _C . DATA . VAL , six . string_types ): # support single string (the typical case) as well _C . DATA . VAL = ( _C . DATA . VAL , ) if isinstance ( _C . DATA . TRAIN , six . string_types ): # support single string _C . DATA . TRAIN = ( _C . DATA . TRAIN , ) # finalize dataset definitions ... from mot.object_detection.dataset import DatasetRegistry datasets = list ( _C . DATA . TRAIN ) + list ( _C . DATA . VAL ) _C . DATA . CLASS_NAMES = DatasetRegistry . get_metadata ( datasets [ 0 ], \"class_names\" ) _C . DATA . NUM_CATEGORY = len ( _C . DATA . CLASS_NAMES ) - 1 assert _C . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' , 'GN' , 'None' ], _C . BACKBONE . NORM if _C . BACKBONE . NORM != 'FreezeBN' : assert not _C . BACKBONE . FREEZE_AFFINE assert _C . BACKBONE . FREEZE_AT in [ 0 , 1 , 2 ] _C . RPN . NUM_ANCHOR = len ( _C . RPN . ANCHOR_SIZES ) * len ( _C . RPN . ANCHOR_RATIOS ) assert len ( _C . FPN . ANCHOR_STRIDES ) == len ( _C . RPN . ANCHOR_SIZES ) # image size into the backbone has to be multiple of this number _C . FPN . RESOLUTION_REQUIREMENT = _C . FPN . ANCHOR_STRIDES [ 3 ] # [3] because we build FPN with features r2,r3,r4,r5 if _C . MODE_FPN : size_mult = _C . FPN . RESOLUTION_REQUIREMENT * 1. _C . PREPROC . MAX_SIZE = np . ceil ( _C . PREPROC . MAX_SIZE / size_mult ) * size_mult assert _C . FPN . PROPOSAL_MODE in [ 'Level' , 'Joint' ] assert _C . FPN . FRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . MRCNN_HEAD_FUNC . endswith ( '_head' ) assert _C . FPN . NORM in [ 'None' , 'GN' ] if _C . FPN . CASCADE : # the first threshold is the proposal sampling threshold assert _C . CASCADE . IOUS [ 0 ] == _C . FRCNN . FG_THRESH assert len ( _C . CASCADE . BBOX_REG_WEIGHTS ) == len ( _C . CASCADE . IOUS ) if is_training : train_scales = _C . PREPROC . TRAIN_SHORT_EDGE_SIZE if isinstance ( train_scales , ( list , tuple )) and train_scales [ 1 ] - train_scales [ 0 ] > 100 : # don't autotune if augmentation is on os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' os . environ [ 'TF_AUTOTUNE_THRESHOLD' ] = '1' assert _C . TRAINER in [ 'horovod' , 'replicated' ], _C . TRAINER lr = _C . TRAIN . LR_SCHEDULE if isinstance ( lr , six . string_types ): if lr . endswith ( \"x\" ): LR_SCHEDULE_KITER = { \"{}x\" . format ( k ): [ 180 * k - 120 , 180 * k - 40 , 180 * k ] for k in range ( 2 , 10 )} LR_SCHEDULE_KITER [ \"1x\" ] = [ 120 , 160 , 180 ] _C . TRAIN . LR_SCHEDULE = [ x * 1000 for x in LR_SCHEDULE_KITER [ lr ]] else : _C . TRAIN . LR_SCHEDULE = eval ( lr ) # setup NUM_GPUS if _C . TRAINER == 'horovod' : import horovod.tensorflow as hvd ngpu = hvd . size () logger . info ( \"Horovod Rank={}, Size={}, LocalRank={}\" . format ( hvd . rank (), hvd . size (), hvd . local_rank ())) else : assert 'OMPI_COMM_WORLD_SIZE' not in os . environ ngpu = get_num_gpu () assert ngpu > 0 , \"Has to train with GPU!\" assert ngpu % 8 == 0 or 8 % ngpu == 0 , \"Can only train with 1,2,4 or >=8 GPUs, but found {} GPUs\" . format ( ngpu ) else : # autotune is too slow for inference os . environ [ 'TF_CUDNN_USE_AUTOTUNE' ] = '0' ngpu = get_num_gpu () if _C . TRAIN . NUM_GPUS is None : _C . TRAIN . NUM_GPUS = ngpu else : if _C . TRAINER == 'horovod' : assert _C . TRAIN . NUM_GPUS == ngpu else : assert _C . TRAIN . NUM_GPUS <= ngpu _C . freeze () logger . info ( \"Config: ------------------------------------------ \\n \" + str ( _C ))","title":"finalize_configs"},{"location":"reference/mot/object_detection/data/","text":"Module mot.object_detection.data View Source # -*- coding: utf-8 -*- # File: data.py import copy import itertools import numpy as np import cv2 from tabulate import tabulate from termcolor import colored from tensorpack.dataflow import ( DataFromList , MapData , MapDataComponent , MultiProcessMapData , MultiThreadMapData , TestDataSpeed , imgaug , ) from tensorpack.utils import logger from tensorpack.utils.argtools import log_once from mot.object_detection.modeling.model_rpn import get_all_anchors from mot.object_detection.modeling.model_fpn import get_all_anchors_fpn from mot.object_detection.common import ( CustomResize , DataFromListOfDict , box_to_point8 , filter_boxes_inside_shape , np_iou , point8_to_box , polygons_to_mask , ) from mot.object_detection.config import config as cfg from mot.object_detection.dataset import DatasetRegistry , register_mot from mot.object_detection.utils.np_box_ops import area as np_area from mot.object_detection.utils.np_box_ops import ioa as np_ioa # import tensorpack.utils.viz as tpviz class MalformedData ( BaseException ): pass def print_class_histogram ( roidbs ): \"\"\" Args: roidbs (list[dict]): the same format as the output of `training_roidbs`. \"\"\" class_names = DatasetRegistry . get_metadata ( cfg . DATA . TRAIN [ 0 ], 'class_names' ) # labels are in [1, NUM_CATEGORY], hence +2 for bins hist_bins = np . arange ( cfg . DATA . NUM_CATEGORY + 2 ) # Histogram of ground-truth objects gt_hist = np . zeros (( cfg . DATA . NUM_CATEGORY + 1 ,), dtype = np . int ) for entry in roidbs : # filter crowd? gt_inds = np . where (( entry [ \"class\" ] > 0 ) & ( entry [ \"is_crowd\" ] == 0 ))[ 0 ] gt_classes = entry [ \"class\" ][ gt_inds ] gt_hist += np . histogram ( gt_classes , bins = hist_bins )[ 0 ] data = list ( itertools . chain ( * [[ class_names [ i + 1 ], v ] for i , v in enumerate ( gt_hist [ 1 :])])) COL = min ( 6 , len ( data )) total_instances = sum ( data [ 1 :: 2 ]) data . extend ([ None ] * (( COL - len ( data ) % COL ) % COL )) data . extend ([ \"total\" , total_instances ]) data = itertools . zip_longest ( * [ data [ i :: COL ] for i in range ( COL )]) # the first line is BG table = tabulate ( data , headers = [ \"class\" , \"#box\" ] * ( COL // 2 ), tablefmt = \"pipe\" , stralign = \"center\" , numalign = \"left\" ) logger . info ( \"Ground-Truth category distribution: \\n \" + colored ( table , \"cyan\" )) class TrainingDataPreprocessor : \"\"\" The mapper to preprocess the input data for training. Since the mapping may run in other processes, we write a new class and explicitly pass cfg to it, in the spirit of \"explicitly pass resources to subprocess\". \"\"\" def __init__ ( self , cfg ): self . cfg = cfg self . aug = imgaug . AugmentorList ([ CustomResize ( cfg . PREPROC . TRAIN_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ), imgaug . Flip ( horiz = True ) ]) def __call__ ( self , roidb ): fname , boxes , klass , is_crowd = roidb [ \"file_name\" ], roidb [ \"boxes\" ], roidb [ \"class\" ], roidb [ \"is_crowd\" ] assert boxes . ndim == 2 and boxes . shape [ 1 ] == 4 , boxes . shape boxes = np . copy ( boxes ) im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname im = im . astype ( \"float32\" ) height , width = im . shape [: 2 ] # assume floatbox as input assert boxes . dtype == np . float32 , \"Loader has to return float32 boxes!\" if not self . cfg . DATA . ABSOLUTE_COORD : boxes [:, 0 :: 2 ] *= width boxes [:, 1 :: 2 ] *= height # augmentation: tfms = self . aug . get_transform ( im ) im = tfms . apply_image ( im ) points = box_to_point8 ( boxes ) points = tfms . apply_coords ( points ) boxes = point8_to_box ( points ) if len ( boxes ): assert np . min ( np_area ( boxes )) > 0 , \"Some boxes have zero area!\" ret = { \"image\" : im } # Add rpn data to dataflow: try : if self . cfg . MODE_FPN : multilevel_anchor_inputs = self . get_multilevel_rpn_anchor_input ( im , boxes , is_crowd ) for i , ( anchor_labels , anchor_boxes ) in enumerate ( multilevel_anchor_inputs ): ret [ \"anchor_labels_lvl {} \" . format ( i + 2 )] = anchor_labels ret [ \"anchor_boxes_lvl {} \" . format ( i + 2 )] = anchor_boxes else : ret [ \"anchor_labels\" ], ret [ \"anchor_boxes\" ] = self . get_rpn_anchor_input ( im , boxes , is_crowd ) boxes = boxes [ is_crowd == 0 ] # skip crowd boxes in training target klass = klass [ is_crowd == 0 ] ret [ \"gt_boxes\" ] = boxes ret [ \"gt_labels\" ] = klass except MalformedData as e : log_once ( \"Input {} is filtered for training: {} \" . format ( fname , str ( e )), \"warn\" ) return None if self . cfg . MODE_MASK : # augmentation will modify the polys in-place segmentation = copy . deepcopy ( roidb [ \"segmentation\" ]) segmentation = [ segmentation [ k ] for k in range ( len ( segmentation )) if not is_crowd [ k ]] assert len ( segmentation ) == len ( boxes ) # Apply augmentation on polygon coordinates. # And produce one image-sized binary mask per box. masks = [] width_height = np . asarray ([ width , height ], dtype = np . float32 ) gt_mask_width = int ( np . ceil ( im . shape [ 1 ] / 8.0 ) * 8 ) # pad to 8 in order to pack mask into bits for polys in segmentation : if not self . cfg . DATA . ABSOLUTE_COORD : polys = [ p * width_height for p in polys ] polys = [ tfms . apply_coords ( p ) for p in polys ] masks . append ( polygons_to_mask ( polys , im . shape [ 0 ], gt_mask_width )) if len ( masks ): masks = np . asarray ( masks , dtype = 'uint8' ) # values in {0, 1} masks = np . packbits ( masks , axis =- 1 ) else : # no gt on the image masks = np . zeros (( 0 , im . shape [ 0 ], gt_mask_width // 8 ), dtype = 'uint8' ) ret [ 'gt_masks_packed' ] = masks # from viz import draw_annotation, draw_mask # viz = draw_annotation(im, boxes, klass) # for mask in masks: # viz = draw_mask(viz, mask) # tpviz.interactive_imshow(viz) return ret def get_rpn_anchor_input ( self , im , boxes , is_crowd ): \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS \"\"\" boxes = boxes . copy () all_anchors = np . copy ( get_all_anchors ( stride = self . cfg . RPN . ANCHOR_STRIDE , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) ) # fHxfWxAx4 -> (-1, 4) featuremap_anchors_flatten = all_anchors . reshape (( - 1 , 4 )) # only use anchors inside the image inside_ind , inside_anchors = filter_boxes_inside_shape ( featuremap_anchors_flatten , im . shape [: 2 ]) # obtain anchor labels and their corresponding gt boxes anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ], boxes [ is_crowd == 1 ] ) # Fill them back to original size: fHxfWx1, fHxfWx4 num_anchor = self . cfg . RPN . NUM_ANCHOR anchorH , anchorW = all_anchors . shape [: 2 ] featuremap_labels = - np . ones (( anchorH * anchorW * num_anchor ,), dtype = \"int32\" ) featuremap_labels [ inside_ind ] = anchor_labels featuremap_labels = featuremap_labels . reshape (( anchorH , anchorW , num_anchor )) featuremap_boxes = np . zeros (( anchorH * anchorW * num_anchor , 4 ), dtype = \"float32\" ) featuremap_boxes [ inside_ind , :] = anchor_gt_boxes featuremap_boxes = featuremap_boxes . reshape (( anchorH , anchorW , num_anchor , 4 )) return featuremap_labels , featuremap_boxes # TODO: can probably merge single-level logic with FPN logic to simplify code def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ): \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWx NUM_ANCHOR_RATIOS fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4 \"\"\" boxes = boxes . copy () anchors_per_level = get_all_anchors_fpn ( strides = self . cfg . FPN . ANCHOR_STRIDES , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) flatten_anchors_per_level = [ k . reshape (( - 1 , 4 )) for k in anchors_per_level ] all_anchors_flatten = np . concatenate ( flatten_anchors_per_level , axis = 0 ) inside_ind , inside_anchors = filter_boxes_inside_shape ( all_anchors_flatten , im . shape [: 2 ]) anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ], boxes [ is_crowd == 1 ] ) # map back to all_anchors, then split to each level num_all_anchors = all_anchors_flatten . shape [ 0 ] all_labels = - np . ones (( num_all_anchors ,), dtype = \"int32\" ) all_labels [ inside_ind ] = anchor_labels all_boxes = np . zeros (( num_all_anchors , 4 ), dtype = \"float32\" ) all_boxes [ inside_ind ] = anchor_gt_boxes start = 0 multilevel_inputs = [] for level_anchor in anchors_per_level : assert level_anchor . shape [ 2 ] == len ( self . cfg . RPN . ANCHOR_RATIOS ) anchor_shape = level_anchor . shape [: 3 ] # fHxfWxNUM_ANCHOR_RATIOS num_anchor_this_level = np . prod ( anchor_shape ) end = start + num_anchor_this_level multilevel_inputs . append ( ( all_labels [ start : end ] . reshape ( anchor_shape ), all_boxes [ start : end , :] . reshape ( anchor_shape + ( 4 ,))) ) start = end assert end == num_all_anchors , \" {} != {} \" . format ( end , num_all_anchors ) return multilevel_inputs def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ): \"\"\" Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. \"\"\" # This function will modify labels and return the filtered inds def filter_box_label ( labels , value , max_num ): curr_inds = np . where ( labels == value )[ 0 ] if len ( curr_inds ) > max_num : disable_inds = np . random . choice ( curr_inds , size = ( len ( curr_inds ) - max_num ), replace = False ) labels [ disable_inds ] = - 1 # ignore them curr_inds = np . where ( labels == value )[ 0 ] return curr_inds NA , NB = len ( anchors ), len ( gt_boxes ) if NB == 0 : # No groundtruth. All anchors are either background or ignored. anchor_labels = np . zeros (( NA ,), dtype = \"int32\" ) filter_box_label ( anchor_labels , 0 , self . cfg . RPN . BATCH_PER_IM ) return anchor_labels , np . zeros (( NA , 4 ), dtype = \"float32\" ) box_ious = np_iou ( anchors , gt_boxes ) # NA x NB ious_argmax_per_anchor = box_ious . argmax ( axis = 1 ) # NA, ious_max_per_anchor = box_ious . max ( axis = 1 ) ious_max_per_gt = np . amax ( box_ious , axis = 0 , keepdims = True ) # 1xNB # for each gt, find all those anchors (including ties) that has the max ious with it anchors_with_max_iou_per_gt = np . where ( box_ious == ious_max_per_gt )[ 0 ] # Setting NA labels: 1--fg 0--bg -1--ignore anchor_labels = - np . ones (( NA ,), dtype = \"int32\" ) # NA, # the order of setting neg/pos labels matter anchor_labels [ anchors_with_max_iou_per_gt ] = 1 anchor_labels [ ious_max_per_anchor >= self . cfg . RPN . POSITIVE_ANCHOR_THRESH ] = 1 anchor_labels [ ious_max_per_anchor < self . cfg . RPN . NEGATIVE_ANCHOR_THRESH ] = 0 # label all non-ignore candidate boxes which overlap crowd as ignore if crowd_boxes . size > 0 : cand_inds = np . where ( anchor_labels >= 0 )[ 0 ] cand_anchors = anchors [ cand_inds ] ioas = np_ioa ( crowd_boxes , cand_anchors ) overlap_with_crowd = cand_inds [ ioas . max ( axis = 0 ) > self . cfg . RPN . CROWD_OVERLAP_THRESH ] anchor_labels [ overlap_with_crowd ] = - 1 # Subsample fg labels: ignore some fg if fg is too many target_num_fg = int ( self . cfg . RPN . BATCH_PER_IM * self . cfg . RPN . FG_RATIO ) fg_inds = filter_box_label ( anchor_labels , 1 , target_num_fg ) # Keep an image even if there is no foreground anchors # if len(fg_inds) == 0: # raise MalformedData(\"No valid foreground for RPN!\") # Subsample bg labels. num_bg is not allowed to be too many old_num_bg = np . sum ( anchor_labels == 0 ) if old_num_bg == 0 : # No valid bg in this image, skip. raise MalformedData ( \"No valid background for RPN!\" ) target_num_bg = self . cfg . RPN . BATCH_PER_IM - len ( fg_inds ) filter_box_label ( anchor_labels , 0 , target_num_bg ) # ignore return values # Set anchor boxes: the best gt_box for each fg anchor anchor_boxes = np . zeros (( NA , 4 ), dtype = \"float32\" ) fg_boxes = gt_boxes [ ious_argmax_per_anchor [ fg_inds ], :] anchor_boxes [ fg_inds , :] = fg_boxes # assert len(fg_inds) + np.sum(anchor_labels == 0) == self.cfg.RPN.BATCH_PER_IM return anchor_labels , anchor_boxes def get_train_dataflow (): \"\"\" Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) \"\"\" roidbs = list ( itertools . chain . from_iterable ( DatasetRegistry . get ( x ) . training_roidbs () for x in cfg . DATA . TRAIN )) print_class_histogram ( roidbs ) # Filter out images that have no gt boxes, but this filter shall not be applied for testing. # The model does support training with empty images, but it is not useful for COCO. num = len ( roidbs ) roidbs = list ( filter ( lambda img : len ( img [ \"boxes\" ][ img [ \"is_crowd\" ] == 0 ]) > 0 , roidbs )) logger . info ( \"Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {} \" . format ( num - len ( roidbs ), len ( roidbs ) ) ) ds = DataFromList ( roidbs , shuffle = True ) preprocess = TrainingDataPreprocessor ( cfg ) if cfg . DATA . NUM_WORKERS > 0 : if cfg . TRAINER == \"horovod\" : buffer_size = cfg . DATA . NUM_WORKERS * 10 # one dataflow for each process, therefore don't need large buffer ds = MultiThreadMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) # MPI does not like fork() else : buffer_size = cfg . DATA . NUM_WORKERS * 20 ds = MultiProcessMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) else : ds = MapData ( ds , preprocess ) return ds def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ): \"\"\" Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data \"\"\" roidbs = DatasetRegistry . get ( name ) . inference_roidbs () logger . info ( \"Found {} images for inference.\" . format ( len ( roidbs ))) num_imgs = len ( roidbs ) img_per_shard = num_imgs // num_shards img_range = ( shard * img_per_shard , ( shard + 1 ) * img_per_shard if shard + 1 < num_shards else num_imgs ) # no filter for training ds = DataFromListOfDict ( roidbs [ img_range [ 0 ]: img_range [ 1 ]], [ \"file_name\" , \"image_id\" ]) def f ( fname ): im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname return im ds = MapDataComponent ( ds , f , 0 ) # Evaluation itself may be multi-threaded, therefore don't add prefetch here. return ds if __name__ == \"__main__\" : import os from tensorpack.dataflow import PrintData from mot.object_detection.config import finalize_configs register_mot ( cfg . DATA . BASEDIR ) finalize_configs ( is_training = True ) ds = get_train_dataflow () ds = PrintData ( ds , 10 ) TestDataSpeed ( ds , 50000 ) . start () for k in ds : pass Functions get_eval_dataflow def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ) Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data View Source def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ): \"\"\" Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data \"\"\" roidbs = DatasetRegistry . get ( name ). inference_roidbs () logger . info ( \"Found {} images for inference.\" . format ( len ( roidbs ))) num_imgs = len ( roidbs ) img_per_shard = num_imgs // num_shards img_range = ( shard * img_per_shard , ( shard + 1 ) * img_per_shard if shard + 1 < num_shards else num_imgs ) # no filter for training ds = DataFromListOfDict ( roidbs [ img_range [ 0 ]: img_range [ 1 ]], [ \"file_name\" , \"image_id\" ]) def f ( fname ): im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname return im ds = MapDataComponent ( ds , f , 0 ) # Evaluation itself may be multi - threaded , therefore don ' t add prefetch here . return ds get_train_dataflow def get_train_dataflow ( ) Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) View Source def get_train_dataflow (): \"\"\" Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) \"\"\" roidbs = list ( itertools . chain . from_iterable ( DatasetRegistry . get ( x ) . training_roidbs () for x in cfg . DATA . TRAIN )) print_class_histogram ( roidbs ) # Filter out images that have no gt boxes, but this filter shall not be applied for testing. # The model does support training with empty images, but it is not useful for COCO. num = len ( roidbs ) roidbs = list ( filter ( lambda img : len ( img [ \"boxes\" ][ img [ \"is_crowd\" ] == 0 ]) > 0 , roidbs )) logger . info ( \"Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {}\" . format ( num - len ( roidbs ), len ( roidbs ) ) ) ds = DataFromList ( roidbs , shuffle = True ) preprocess = TrainingDataPreprocessor ( cfg ) if cfg . DATA . NUM_WORKERS > 0 : if cfg . TRAINER == \"horovod\" : buffer_size = cfg . DATA . NUM_WORKERS * 10 # one dataflow for each process, therefore don't need large buffer ds = MultiThreadMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) # MPI does not like fork() else : buffer_size = cfg . DATA . NUM_WORKERS * 20 ds = MultiProcessMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) else : ds = MapData ( ds , preprocess ) return ds print_class_histogram def print_class_histogram ( roidbs ) Args: roidbs (list[dict]): the same format as the output of training_roidbs . View Source def print_class_histogram ( roidbs ) : \"\"\" Args: roidbs (list[dict]): the same format as the output of `training_roidbs`. \"\"\" class_names = DatasetRegistry . get_metadata ( cfg . DATA . TRAIN [ 0 ] , 'class_names' ) # labels are in [ 1, NUM_CATEGORY ] , hence + 2 for bins hist_bins = np . arange ( cfg . DATA . NUM_CATEGORY + 2 ) # Histogram of ground - truth objects gt_hist = np . zeros (( cfg . DATA . NUM_CATEGORY + 1 ,), dtype = np . int ) for entry in roidbs : # filter crowd ? gt_inds = np . where (( entry [ \"class\" ] > 0 ) & ( entry [ \"is_crowd\" ] == 0 )) [ 0 ] gt_classes = entry [ \"class\" ][ gt_inds ] gt_hist += np . histogram ( gt_classes , bins = hist_bins ) [ 0 ] data = list ( itertools . chain ( *[ [class_names[i + 1 ] , v ] for i , v in enumerate ( gt_hist [ 1: ] ) ] )) COL = min ( 6 , len ( data )) total_instances = sum ( data [ 1::2 ] ) data . extend ( [ None ] * (( COL - len ( data ) % COL ) % COL )) data . extend ( [ \"total\", total_instances ] ) data = itertools . zip_longest ( *[ data[i::COL ] for i in range ( COL ) ] ) # the first line is BG table = tabulate ( data , headers =[ \"class\", \"#box\" ] * ( COL // 2 ), tablefmt = \"pipe\" , stralign = \"center\" , numalign = \"left\" ) logger . info ( \"Ground-Truth category distribution:\\n\" + colored ( table , \"cyan\" )) Classes MalformedData class MalformedData ( / , * args , ** kwargs ) Common base class for all exceptions Ancestors (in MRO) builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. TrainingDataPreprocessor class TrainingDataPreprocessor ( cfg ) The mapper to preprocess the input data for training. Since the mapping may run in other processes, we write a new class and explicitly pass cfg to it, in the spirit of \"explicitly pass resources to subprocess\". Methods get_anchor_labels def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ) Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. View Source def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ) : \"\"\" Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. \"\"\" # This function will modify labels and return the filtered inds def filter_box_label ( labels , value , max_num ) : curr_inds = np . where ( labels == value ) [ 0 ] if len ( curr_inds ) > max_num : disable_inds = np . random . choice ( curr_inds , size = ( len ( curr_inds ) - max_num ), replace = False ) labels [ disable_inds ] = - 1 # ignore them curr_inds = np . where ( labels == value ) [ 0 ] return curr_inds NA , NB = len ( anchors ), len ( gt_boxes ) if NB == 0 : # No groundtruth . All anchors are either background or ignored . anchor_labels = np . zeros (( NA ,), dtype = \"int32\" ) filter_box_label ( anchor_labels , 0 , self . cfg . RPN . BATCH_PER_IM ) return anchor_labels , np . zeros (( NA , 4 ), dtype = \"float32\" ) box_ious = np_iou ( anchors , gt_boxes ) # NA x NB ious_argmax_per_anchor = box_ious . argmax ( axis = 1 ) # NA , ious_max_per_anchor = box_ious . max ( axis = 1 ) ious_max_per_gt = np . amax ( box_ious , axis = 0 , keepdims = True ) # 1 xNB # for each gt , find all those anchors ( including ties ) that has the max ious with it anchors_with_max_iou_per_gt = np . where ( box_ious == ious_max_per_gt ) [ 0 ] # Setting NA labels : 1 -- fg 0 -- bg - 1 -- ignore anchor_labels = - np . ones (( NA ,), dtype = \"int32\" ) # NA , # the order of setting neg / pos labels matter anchor_labels [ anchors_with_max_iou_per_gt ] = 1 anchor_labels [ ious_max_per_anchor >= self.cfg.RPN.POSITIVE_ANCHOR_THRESH ] = 1 anchor_labels [ ious_max_per_anchor < self.cfg.RPN.NEGATIVE_ANCHOR_THRESH ] = 0 # label all non - ignore candidate boxes which overlap crowd as ignore if crowd_boxes . size > 0 : cand_inds = np . where ( anchor_labels >= 0 ) [ 0 ] cand_anchors = anchors [ cand_inds ] ioas = np_ioa ( crowd_boxes , cand_anchors ) overlap_with_crowd = cand_inds [ ioas.max(axis=0) > self.cfg.RPN.CROWD_OVERLAP_THRESH ] anchor_labels [ overlap_with_crowd ] = - 1 # Subsample fg labels : ignore some fg if fg is too many target_num_fg = int ( self . cfg . RPN . BATCH_PER_IM * self . cfg . RPN . FG_RATIO ) fg_inds = filter_box_label ( anchor_labels , 1 , target_num_fg ) # Keep an image even if there is no foreground anchors # if len ( fg_inds ) == 0 : # raise MalformedData ( \"No valid foreground for RPN!\" ) # Subsample bg labels . num_bg is not allowed to be too many old_num_bg = np . sum ( anchor_labels == 0 ) if old_num_bg == 0 : # No valid bg in this image , skip . raise MalformedData ( \"No valid background for RPN!\" ) target_num_bg = self . cfg . RPN . BATCH_PER_IM - len ( fg_inds ) filter_box_label ( anchor_labels , 0 , target_num_bg ) # ignore return values # Set anchor boxes : the best gt_box for each fg anchor anchor_boxes = np . zeros (( NA , 4 ), dtype = \"float32\" ) fg_boxes = gt_boxes [ ious_argmax_per_anchor[fg_inds ] , :] anchor_boxes [ fg_inds, : ] = fg_boxes # assert len ( fg_inds ) + np . sum ( anchor_labels == 0 ) == self . cfg . RPN . BATCH_PER_IM return anchor_labels , anchor_boxes get_multilevel_rpn_anchor_input def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ) Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels : fHxfWx NUM_ANCHOR_RATIOS fm_boxes : fHxfWx NUM_ANCHOR_RATIOS x4 View Source def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ) : \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWx NUM_ANCHOR_RATIOS fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4 \"\"\" boxes = boxes . copy () anchors_per_level = get_all_anchors_fpn ( strides = self . cfg . FPN . ANCHOR_STRIDES , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) flatten_anchors_per_level = [ k.reshape((-1, 4)) for k in anchors_per_level ] all_anchors_flatten = np . concatenate ( flatten_anchors_per_level , axis = 0 ) inside_ind , inside_anchors = filter_boxes_inside_shape ( all_anchors_flatten , im . shape [ :2 ] ) anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ] , boxes [ is_crowd == 1 ] ) # map back to all_anchors , then split to each level num_all_anchors = all_anchors_flatten . shape [ 0 ] all_labels = - np . ones (( num_all_anchors ,), dtype = \"int32\" ) all_labels [ inside_ind ] = anchor_labels all_boxes = np . zeros (( num_all_anchors , 4 ), dtype = \"float32\" ) all_boxes [ inside_ind ] = anchor_gt_boxes start = 0 multilevel_inputs = [] for level_anchor in anchors_per_level : assert level_anchor . shape [ 2 ] == len ( self . cfg . RPN . ANCHOR_RATIOS ) anchor_shape = level_anchor . shape [ :3 ] # fHxfWxNUM_ANCHOR_RATIOS num_anchor_this_level = np . prod ( anchor_shape ) end = start + num_anchor_this_level multilevel_inputs . append ( ( all_labels [ start:end ] . reshape ( anchor_shape ), all_boxes [ start:end, : ] . reshape ( anchor_shape + ( 4 ,))) ) start = end assert end == num_all_anchors , \"{} != {}\" . format ( end , num_all_anchors ) return multilevel_inputs get_rpn_anchor_input def get_rpn_anchor_input ( self , im , boxes , is_crowd ) Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS View Source def get_rpn_anchor_input ( self , im , boxes , is_crowd ) : \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS \"\"\" boxes = boxes . copy () all_anchors = np . copy ( get_all_anchors ( stride = self . cfg . RPN . ANCHOR_STRIDE , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) ) # fHxfWxAx4 -> ( - 1 , 4 ) featuremap_anchors_flatten = all_anchors . reshape (( - 1 , 4 )) # only use anchors inside the image inside_ind , inside_anchors = filter_boxes_inside_shape ( featuremap_anchors_flatten , im . shape [ :2 ] ) # obtain anchor labels and their corresponding gt boxes anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ] , boxes [ is_crowd == 1 ] ) # Fill them back to original size : fHxfWx1 , fHxfWx4 num_anchor = self . cfg . RPN . NUM_ANCHOR anchorH , anchorW = all_anchors . shape [ :2 ] featuremap_labels = - np . ones (( anchorH * anchorW * num_anchor ,), dtype = \"int32\" ) featuremap_labels [ inside_ind ] = anchor_labels featuremap_labels = featuremap_labels . reshape (( anchorH , anchorW , num_anchor )) featuremap_boxes = np . zeros (( anchorH * anchorW * num_anchor , 4 ), dtype = \"float32\" ) featuremap_boxes [ inside_ind, : ] = anchor_gt_boxes featuremap_boxes = featuremap_boxes . reshape (( anchorH , anchorW , num_anchor , 4 )) return featuremap_labels , featuremap_boxes","title":"Data"},{"location":"reference/mot/object_detection/data/#module-motobject_detectiondata","text":"View Source # -*- coding: utf-8 -*- # File: data.py import copy import itertools import numpy as np import cv2 from tabulate import tabulate from termcolor import colored from tensorpack.dataflow import ( DataFromList , MapData , MapDataComponent , MultiProcessMapData , MultiThreadMapData , TestDataSpeed , imgaug , ) from tensorpack.utils import logger from tensorpack.utils.argtools import log_once from mot.object_detection.modeling.model_rpn import get_all_anchors from mot.object_detection.modeling.model_fpn import get_all_anchors_fpn from mot.object_detection.common import ( CustomResize , DataFromListOfDict , box_to_point8 , filter_boxes_inside_shape , np_iou , point8_to_box , polygons_to_mask , ) from mot.object_detection.config import config as cfg from mot.object_detection.dataset import DatasetRegistry , register_mot from mot.object_detection.utils.np_box_ops import area as np_area from mot.object_detection.utils.np_box_ops import ioa as np_ioa # import tensorpack.utils.viz as tpviz class MalformedData ( BaseException ): pass def print_class_histogram ( roidbs ): \"\"\" Args: roidbs (list[dict]): the same format as the output of `training_roidbs`. \"\"\" class_names = DatasetRegistry . get_metadata ( cfg . DATA . TRAIN [ 0 ], 'class_names' ) # labels are in [1, NUM_CATEGORY], hence +2 for bins hist_bins = np . arange ( cfg . DATA . NUM_CATEGORY + 2 ) # Histogram of ground-truth objects gt_hist = np . zeros (( cfg . DATA . NUM_CATEGORY + 1 ,), dtype = np . int ) for entry in roidbs : # filter crowd? gt_inds = np . where (( entry [ \"class\" ] > 0 ) & ( entry [ \"is_crowd\" ] == 0 ))[ 0 ] gt_classes = entry [ \"class\" ][ gt_inds ] gt_hist += np . histogram ( gt_classes , bins = hist_bins )[ 0 ] data = list ( itertools . chain ( * [[ class_names [ i + 1 ], v ] for i , v in enumerate ( gt_hist [ 1 :])])) COL = min ( 6 , len ( data )) total_instances = sum ( data [ 1 :: 2 ]) data . extend ([ None ] * (( COL - len ( data ) % COL ) % COL )) data . extend ([ \"total\" , total_instances ]) data = itertools . zip_longest ( * [ data [ i :: COL ] for i in range ( COL )]) # the first line is BG table = tabulate ( data , headers = [ \"class\" , \"#box\" ] * ( COL // 2 ), tablefmt = \"pipe\" , stralign = \"center\" , numalign = \"left\" ) logger . info ( \"Ground-Truth category distribution: \\n \" + colored ( table , \"cyan\" )) class TrainingDataPreprocessor : \"\"\" The mapper to preprocess the input data for training. Since the mapping may run in other processes, we write a new class and explicitly pass cfg to it, in the spirit of \"explicitly pass resources to subprocess\". \"\"\" def __init__ ( self , cfg ): self . cfg = cfg self . aug = imgaug . AugmentorList ([ CustomResize ( cfg . PREPROC . TRAIN_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ), imgaug . Flip ( horiz = True ) ]) def __call__ ( self , roidb ): fname , boxes , klass , is_crowd = roidb [ \"file_name\" ], roidb [ \"boxes\" ], roidb [ \"class\" ], roidb [ \"is_crowd\" ] assert boxes . ndim == 2 and boxes . shape [ 1 ] == 4 , boxes . shape boxes = np . copy ( boxes ) im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname im = im . astype ( \"float32\" ) height , width = im . shape [: 2 ] # assume floatbox as input assert boxes . dtype == np . float32 , \"Loader has to return float32 boxes!\" if not self . cfg . DATA . ABSOLUTE_COORD : boxes [:, 0 :: 2 ] *= width boxes [:, 1 :: 2 ] *= height # augmentation: tfms = self . aug . get_transform ( im ) im = tfms . apply_image ( im ) points = box_to_point8 ( boxes ) points = tfms . apply_coords ( points ) boxes = point8_to_box ( points ) if len ( boxes ): assert np . min ( np_area ( boxes )) > 0 , \"Some boxes have zero area!\" ret = { \"image\" : im } # Add rpn data to dataflow: try : if self . cfg . MODE_FPN : multilevel_anchor_inputs = self . get_multilevel_rpn_anchor_input ( im , boxes , is_crowd ) for i , ( anchor_labels , anchor_boxes ) in enumerate ( multilevel_anchor_inputs ): ret [ \"anchor_labels_lvl {} \" . format ( i + 2 )] = anchor_labels ret [ \"anchor_boxes_lvl {} \" . format ( i + 2 )] = anchor_boxes else : ret [ \"anchor_labels\" ], ret [ \"anchor_boxes\" ] = self . get_rpn_anchor_input ( im , boxes , is_crowd ) boxes = boxes [ is_crowd == 0 ] # skip crowd boxes in training target klass = klass [ is_crowd == 0 ] ret [ \"gt_boxes\" ] = boxes ret [ \"gt_labels\" ] = klass except MalformedData as e : log_once ( \"Input {} is filtered for training: {} \" . format ( fname , str ( e )), \"warn\" ) return None if self . cfg . MODE_MASK : # augmentation will modify the polys in-place segmentation = copy . deepcopy ( roidb [ \"segmentation\" ]) segmentation = [ segmentation [ k ] for k in range ( len ( segmentation )) if not is_crowd [ k ]] assert len ( segmentation ) == len ( boxes ) # Apply augmentation on polygon coordinates. # And produce one image-sized binary mask per box. masks = [] width_height = np . asarray ([ width , height ], dtype = np . float32 ) gt_mask_width = int ( np . ceil ( im . shape [ 1 ] / 8.0 ) * 8 ) # pad to 8 in order to pack mask into bits for polys in segmentation : if not self . cfg . DATA . ABSOLUTE_COORD : polys = [ p * width_height for p in polys ] polys = [ tfms . apply_coords ( p ) for p in polys ] masks . append ( polygons_to_mask ( polys , im . shape [ 0 ], gt_mask_width )) if len ( masks ): masks = np . asarray ( masks , dtype = 'uint8' ) # values in {0, 1} masks = np . packbits ( masks , axis =- 1 ) else : # no gt on the image masks = np . zeros (( 0 , im . shape [ 0 ], gt_mask_width // 8 ), dtype = 'uint8' ) ret [ 'gt_masks_packed' ] = masks # from viz import draw_annotation, draw_mask # viz = draw_annotation(im, boxes, klass) # for mask in masks: # viz = draw_mask(viz, mask) # tpviz.interactive_imshow(viz) return ret def get_rpn_anchor_input ( self , im , boxes , is_crowd ): \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS \"\"\" boxes = boxes . copy () all_anchors = np . copy ( get_all_anchors ( stride = self . cfg . RPN . ANCHOR_STRIDE , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) ) # fHxfWxAx4 -> (-1, 4) featuremap_anchors_flatten = all_anchors . reshape (( - 1 , 4 )) # only use anchors inside the image inside_ind , inside_anchors = filter_boxes_inside_shape ( featuremap_anchors_flatten , im . shape [: 2 ]) # obtain anchor labels and their corresponding gt boxes anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ], boxes [ is_crowd == 1 ] ) # Fill them back to original size: fHxfWx1, fHxfWx4 num_anchor = self . cfg . RPN . NUM_ANCHOR anchorH , anchorW = all_anchors . shape [: 2 ] featuremap_labels = - np . ones (( anchorH * anchorW * num_anchor ,), dtype = \"int32\" ) featuremap_labels [ inside_ind ] = anchor_labels featuremap_labels = featuremap_labels . reshape (( anchorH , anchorW , num_anchor )) featuremap_boxes = np . zeros (( anchorH * anchorW * num_anchor , 4 ), dtype = \"float32\" ) featuremap_boxes [ inside_ind , :] = anchor_gt_boxes featuremap_boxes = featuremap_boxes . reshape (( anchorH , anchorW , num_anchor , 4 )) return featuremap_labels , featuremap_boxes # TODO: can probably merge single-level logic with FPN logic to simplify code def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ): \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWx NUM_ANCHOR_RATIOS fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4 \"\"\" boxes = boxes . copy () anchors_per_level = get_all_anchors_fpn ( strides = self . cfg . FPN . ANCHOR_STRIDES , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) flatten_anchors_per_level = [ k . reshape (( - 1 , 4 )) for k in anchors_per_level ] all_anchors_flatten = np . concatenate ( flatten_anchors_per_level , axis = 0 ) inside_ind , inside_anchors = filter_boxes_inside_shape ( all_anchors_flatten , im . shape [: 2 ]) anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ], boxes [ is_crowd == 1 ] ) # map back to all_anchors, then split to each level num_all_anchors = all_anchors_flatten . shape [ 0 ] all_labels = - np . ones (( num_all_anchors ,), dtype = \"int32\" ) all_labels [ inside_ind ] = anchor_labels all_boxes = np . zeros (( num_all_anchors , 4 ), dtype = \"float32\" ) all_boxes [ inside_ind ] = anchor_gt_boxes start = 0 multilevel_inputs = [] for level_anchor in anchors_per_level : assert level_anchor . shape [ 2 ] == len ( self . cfg . RPN . ANCHOR_RATIOS ) anchor_shape = level_anchor . shape [: 3 ] # fHxfWxNUM_ANCHOR_RATIOS num_anchor_this_level = np . prod ( anchor_shape ) end = start + num_anchor_this_level multilevel_inputs . append ( ( all_labels [ start : end ] . reshape ( anchor_shape ), all_boxes [ start : end , :] . reshape ( anchor_shape + ( 4 ,))) ) start = end assert end == num_all_anchors , \" {} != {} \" . format ( end , num_all_anchors ) return multilevel_inputs def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ): \"\"\" Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. \"\"\" # This function will modify labels and return the filtered inds def filter_box_label ( labels , value , max_num ): curr_inds = np . where ( labels == value )[ 0 ] if len ( curr_inds ) > max_num : disable_inds = np . random . choice ( curr_inds , size = ( len ( curr_inds ) - max_num ), replace = False ) labels [ disable_inds ] = - 1 # ignore them curr_inds = np . where ( labels == value )[ 0 ] return curr_inds NA , NB = len ( anchors ), len ( gt_boxes ) if NB == 0 : # No groundtruth. All anchors are either background or ignored. anchor_labels = np . zeros (( NA ,), dtype = \"int32\" ) filter_box_label ( anchor_labels , 0 , self . cfg . RPN . BATCH_PER_IM ) return anchor_labels , np . zeros (( NA , 4 ), dtype = \"float32\" ) box_ious = np_iou ( anchors , gt_boxes ) # NA x NB ious_argmax_per_anchor = box_ious . argmax ( axis = 1 ) # NA, ious_max_per_anchor = box_ious . max ( axis = 1 ) ious_max_per_gt = np . amax ( box_ious , axis = 0 , keepdims = True ) # 1xNB # for each gt, find all those anchors (including ties) that has the max ious with it anchors_with_max_iou_per_gt = np . where ( box_ious == ious_max_per_gt )[ 0 ] # Setting NA labels: 1--fg 0--bg -1--ignore anchor_labels = - np . ones (( NA ,), dtype = \"int32\" ) # NA, # the order of setting neg/pos labels matter anchor_labels [ anchors_with_max_iou_per_gt ] = 1 anchor_labels [ ious_max_per_anchor >= self . cfg . RPN . POSITIVE_ANCHOR_THRESH ] = 1 anchor_labels [ ious_max_per_anchor < self . cfg . RPN . NEGATIVE_ANCHOR_THRESH ] = 0 # label all non-ignore candidate boxes which overlap crowd as ignore if crowd_boxes . size > 0 : cand_inds = np . where ( anchor_labels >= 0 )[ 0 ] cand_anchors = anchors [ cand_inds ] ioas = np_ioa ( crowd_boxes , cand_anchors ) overlap_with_crowd = cand_inds [ ioas . max ( axis = 0 ) > self . cfg . RPN . CROWD_OVERLAP_THRESH ] anchor_labels [ overlap_with_crowd ] = - 1 # Subsample fg labels: ignore some fg if fg is too many target_num_fg = int ( self . cfg . RPN . BATCH_PER_IM * self . cfg . RPN . FG_RATIO ) fg_inds = filter_box_label ( anchor_labels , 1 , target_num_fg ) # Keep an image even if there is no foreground anchors # if len(fg_inds) == 0: # raise MalformedData(\"No valid foreground for RPN!\") # Subsample bg labels. num_bg is not allowed to be too many old_num_bg = np . sum ( anchor_labels == 0 ) if old_num_bg == 0 : # No valid bg in this image, skip. raise MalformedData ( \"No valid background for RPN!\" ) target_num_bg = self . cfg . RPN . BATCH_PER_IM - len ( fg_inds ) filter_box_label ( anchor_labels , 0 , target_num_bg ) # ignore return values # Set anchor boxes: the best gt_box for each fg anchor anchor_boxes = np . zeros (( NA , 4 ), dtype = \"float32\" ) fg_boxes = gt_boxes [ ious_argmax_per_anchor [ fg_inds ], :] anchor_boxes [ fg_inds , :] = fg_boxes # assert len(fg_inds) + np.sum(anchor_labels == 0) == self.cfg.RPN.BATCH_PER_IM return anchor_labels , anchor_boxes def get_train_dataflow (): \"\"\" Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) \"\"\" roidbs = list ( itertools . chain . from_iterable ( DatasetRegistry . get ( x ) . training_roidbs () for x in cfg . DATA . TRAIN )) print_class_histogram ( roidbs ) # Filter out images that have no gt boxes, but this filter shall not be applied for testing. # The model does support training with empty images, but it is not useful for COCO. num = len ( roidbs ) roidbs = list ( filter ( lambda img : len ( img [ \"boxes\" ][ img [ \"is_crowd\" ] == 0 ]) > 0 , roidbs )) logger . info ( \"Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {} \" . format ( num - len ( roidbs ), len ( roidbs ) ) ) ds = DataFromList ( roidbs , shuffle = True ) preprocess = TrainingDataPreprocessor ( cfg ) if cfg . DATA . NUM_WORKERS > 0 : if cfg . TRAINER == \"horovod\" : buffer_size = cfg . DATA . NUM_WORKERS * 10 # one dataflow for each process, therefore don't need large buffer ds = MultiThreadMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) # MPI does not like fork() else : buffer_size = cfg . DATA . NUM_WORKERS * 20 ds = MultiProcessMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) else : ds = MapData ( ds , preprocess ) return ds def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ): \"\"\" Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data \"\"\" roidbs = DatasetRegistry . get ( name ) . inference_roidbs () logger . info ( \"Found {} images for inference.\" . format ( len ( roidbs ))) num_imgs = len ( roidbs ) img_per_shard = num_imgs // num_shards img_range = ( shard * img_per_shard , ( shard + 1 ) * img_per_shard if shard + 1 < num_shards else num_imgs ) # no filter for training ds = DataFromListOfDict ( roidbs [ img_range [ 0 ]: img_range [ 1 ]], [ \"file_name\" , \"image_id\" ]) def f ( fname ): im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname return im ds = MapDataComponent ( ds , f , 0 ) # Evaluation itself may be multi-threaded, therefore don't add prefetch here. return ds if __name__ == \"__main__\" : import os from tensorpack.dataflow import PrintData from mot.object_detection.config import finalize_configs register_mot ( cfg . DATA . BASEDIR ) finalize_configs ( is_training = True ) ds = get_train_dataflow () ds = PrintData ( ds , 10 ) TestDataSpeed ( ds , 50000 ) . start () for k in ds : pass","title":"Module mot.object_detection.data"},{"location":"reference/mot/object_detection/data/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/data/#get_eval_dataflow","text":"def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ) Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data View Source def get_eval_dataflow ( name , shard = 0 , num_shards = 1 ): \"\"\" Args: name (str): name of the dataset to evaluate shard, num_shards: to get subset of evaluation data \"\"\" roidbs = DatasetRegistry . get ( name ). inference_roidbs () logger . info ( \"Found {} images for inference.\" . format ( len ( roidbs ))) num_imgs = len ( roidbs ) img_per_shard = num_imgs // num_shards img_range = ( shard * img_per_shard , ( shard + 1 ) * img_per_shard if shard + 1 < num_shards else num_imgs ) # no filter for training ds = DataFromListOfDict ( roidbs [ img_range [ 0 ]: img_range [ 1 ]], [ \"file_name\" , \"image_id\" ]) def f ( fname ): im = cv2 . imread ( fname , cv2 . IMREAD_COLOR ) assert im is not None , fname return im ds = MapDataComponent ( ds , f , 0 ) # Evaluation itself may be multi - threaded , therefore don ' t add prefetch here . return ds","title":"get_eval_dataflow"},{"location":"reference/mot/object_detection/data/#get_train_dataflow","text":"def get_train_dataflow ( ) Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) View Source def get_train_dataflow (): \"\"\" Return a training dataflow. Each datapoint consists of the following: An image: (h, w, 3), 1 or more pairs of (anchor_labels, anchor_boxes): anchor_labels: (h', w', NA) anchor_boxes: (h', w', NA, 4) gt_boxes: (N, 4) gt_labels: (N,) If MODE_MASK, gt_masks: (N, h, w) \"\"\" roidbs = list ( itertools . chain . from_iterable ( DatasetRegistry . get ( x ) . training_roidbs () for x in cfg . DATA . TRAIN )) print_class_histogram ( roidbs ) # Filter out images that have no gt boxes, but this filter shall not be applied for testing. # The model does support training with empty images, but it is not useful for COCO. num = len ( roidbs ) roidbs = list ( filter ( lambda img : len ( img [ \"boxes\" ][ img [ \"is_crowd\" ] == 0 ]) > 0 , roidbs )) logger . info ( \"Filtered {} images which contain no non-crowd groudtruth boxes. Total #images for training: {}\" . format ( num - len ( roidbs ), len ( roidbs ) ) ) ds = DataFromList ( roidbs , shuffle = True ) preprocess = TrainingDataPreprocessor ( cfg ) if cfg . DATA . NUM_WORKERS > 0 : if cfg . TRAINER == \"horovod\" : buffer_size = cfg . DATA . NUM_WORKERS * 10 # one dataflow for each process, therefore don't need large buffer ds = MultiThreadMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) # MPI does not like fork() else : buffer_size = cfg . DATA . NUM_WORKERS * 20 ds = MultiProcessMapData ( ds , cfg . DATA . NUM_WORKERS , preprocess , buffer_size = buffer_size ) else : ds = MapData ( ds , preprocess ) return ds","title":"get_train_dataflow"},{"location":"reference/mot/object_detection/data/#print_class_histogram","text":"def print_class_histogram ( roidbs ) Args: roidbs (list[dict]): the same format as the output of training_roidbs . View Source def print_class_histogram ( roidbs ) : \"\"\" Args: roidbs (list[dict]): the same format as the output of `training_roidbs`. \"\"\" class_names = DatasetRegistry . get_metadata ( cfg . DATA . TRAIN [ 0 ] , 'class_names' ) # labels are in [ 1, NUM_CATEGORY ] , hence + 2 for bins hist_bins = np . arange ( cfg . DATA . NUM_CATEGORY + 2 ) # Histogram of ground - truth objects gt_hist = np . zeros (( cfg . DATA . NUM_CATEGORY + 1 ,), dtype = np . int ) for entry in roidbs : # filter crowd ? gt_inds = np . where (( entry [ \"class\" ] > 0 ) & ( entry [ \"is_crowd\" ] == 0 )) [ 0 ] gt_classes = entry [ \"class\" ][ gt_inds ] gt_hist += np . histogram ( gt_classes , bins = hist_bins ) [ 0 ] data = list ( itertools . chain ( *[ [class_names[i + 1 ] , v ] for i , v in enumerate ( gt_hist [ 1: ] ) ] )) COL = min ( 6 , len ( data )) total_instances = sum ( data [ 1::2 ] ) data . extend ( [ None ] * (( COL - len ( data ) % COL ) % COL )) data . extend ( [ \"total\", total_instances ] ) data = itertools . zip_longest ( *[ data[i::COL ] for i in range ( COL ) ] ) # the first line is BG table = tabulate ( data , headers =[ \"class\", \"#box\" ] * ( COL // 2 ), tablefmt = \"pipe\" , stralign = \"center\" , numalign = \"left\" ) logger . info ( \"Ground-Truth category distribution:\\n\" + colored ( table , \"cyan\" ))","title":"print_class_histogram"},{"location":"reference/mot/object_detection/data/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/data/#malformeddata","text":"class MalformedData ( / , * args , ** kwargs ) Common base class for all exceptions","title":"MalformedData"},{"location":"reference/mot/object_detection/data/#ancestors-in-mro","text":"builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/data/#class-variables","text":"args","title":"Class variables"},{"location":"reference/mot/object_detection/data/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/data/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/mot/object_detection/data/#trainingdatapreprocessor","text":"class TrainingDataPreprocessor ( cfg ) The mapper to preprocess the input data for training. Since the mapping may run in other processes, we write a new class and explicitly pass cfg to it, in the spirit of \"explicitly pass resources to subprocess\".","title":"TrainingDataPreprocessor"},{"location":"reference/mot/object_detection/data/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/object_detection/data/#get_anchor_labels","text":"def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ) Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. View Source def get_anchor_labels ( self , anchors , gt_boxes , crowd_boxes ) : \"\"\" Label each anchor as fg/bg/ignore. Args: anchors: Ax4 float gt_boxes: Bx4 float, non-crowd crowd_boxes: Cx4 float Returns: anchor_labels: (A,) int. Each element is {-1, 0, 1} anchor_boxes: Ax4. Contains the target gt_box for each anchor when the anchor is fg. \"\"\" # This function will modify labels and return the filtered inds def filter_box_label ( labels , value , max_num ) : curr_inds = np . where ( labels == value ) [ 0 ] if len ( curr_inds ) > max_num : disable_inds = np . random . choice ( curr_inds , size = ( len ( curr_inds ) - max_num ), replace = False ) labels [ disable_inds ] = - 1 # ignore them curr_inds = np . where ( labels == value ) [ 0 ] return curr_inds NA , NB = len ( anchors ), len ( gt_boxes ) if NB == 0 : # No groundtruth . All anchors are either background or ignored . anchor_labels = np . zeros (( NA ,), dtype = \"int32\" ) filter_box_label ( anchor_labels , 0 , self . cfg . RPN . BATCH_PER_IM ) return anchor_labels , np . zeros (( NA , 4 ), dtype = \"float32\" ) box_ious = np_iou ( anchors , gt_boxes ) # NA x NB ious_argmax_per_anchor = box_ious . argmax ( axis = 1 ) # NA , ious_max_per_anchor = box_ious . max ( axis = 1 ) ious_max_per_gt = np . amax ( box_ious , axis = 0 , keepdims = True ) # 1 xNB # for each gt , find all those anchors ( including ties ) that has the max ious with it anchors_with_max_iou_per_gt = np . where ( box_ious == ious_max_per_gt ) [ 0 ] # Setting NA labels : 1 -- fg 0 -- bg - 1 -- ignore anchor_labels = - np . ones (( NA ,), dtype = \"int32\" ) # NA , # the order of setting neg / pos labels matter anchor_labels [ anchors_with_max_iou_per_gt ] = 1 anchor_labels [ ious_max_per_anchor >= self.cfg.RPN.POSITIVE_ANCHOR_THRESH ] = 1 anchor_labels [ ious_max_per_anchor < self.cfg.RPN.NEGATIVE_ANCHOR_THRESH ] = 0 # label all non - ignore candidate boxes which overlap crowd as ignore if crowd_boxes . size > 0 : cand_inds = np . where ( anchor_labels >= 0 ) [ 0 ] cand_anchors = anchors [ cand_inds ] ioas = np_ioa ( crowd_boxes , cand_anchors ) overlap_with_crowd = cand_inds [ ioas.max(axis=0) > self.cfg.RPN.CROWD_OVERLAP_THRESH ] anchor_labels [ overlap_with_crowd ] = - 1 # Subsample fg labels : ignore some fg if fg is too many target_num_fg = int ( self . cfg . RPN . BATCH_PER_IM * self . cfg . RPN . FG_RATIO ) fg_inds = filter_box_label ( anchor_labels , 1 , target_num_fg ) # Keep an image even if there is no foreground anchors # if len ( fg_inds ) == 0 : # raise MalformedData ( \"No valid foreground for RPN!\" ) # Subsample bg labels . num_bg is not allowed to be too many old_num_bg = np . sum ( anchor_labels == 0 ) if old_num_bg == 0 : # No valid bg in this image , skip . raise MalformedData ( \"No valid background for RPN!\" ) target_num_bg = self . cfg . RPN . BATCH_PER_IM - len ( fg_inds ) filter_box_label ( anchor_labels , 0 , target_num_bg ) # ignore return values # Set anchor boxes : the best gt_box for each fg anchor anchor_boxes = np . zeros (( NA , 4 ), dtype = \"float32\" ) fg_boxes = gt_boxes [ ious_argmax_per_anchor[fg_inds ] , :] anchor_boxes [ fg_inds, : ] = fg_boxes # assert len ( fg_inds ) + np . sum ( anchor_labels == 0 ) == self . cfg . RPN . BATCH_PER_IM return anchor_labels , anchor_boxes","title":"get_anchor_labels"},{"location":"reference/mot/object_detection/data/#get_multilevel_rpn_anchor_input","text":"def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ) Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels : fHxfWx NUM_ANCHOR_RATIOS fm_boxes : fHxfWx NUM_ANCHOR_RATIOS x4 View Source def get_multilevel_rpn_anchor_input ( self , im , boxes , is_crowd ) : \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level. Each tuple contains the anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWx NUM_ANCHOR_RATIOS fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4 \"\"\" boxes = boxes . copy () anchors_per_level = get_all_anchors_fpn ( strides = self . cfg . FPN . ANCHOR_STRIDES , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) flatten_anchors_per_level = [ k.reshape((-1, 4)) for k in anchors_per_level ] all_anchors_flatten = np . concatenate ( flatten_anchors_per_level , axis = 0 ) inside_ind , inside_anchors = filter_boxes_inside_shape ( all_anchors_flatten , im . shape [ :2 ] ) anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ] , boxes [ is_crowd == 1 ] ) # map back to all_anchors , then split to each level num_all_anchors = all_anchors_flatten . shape [ 0 ] all_labels = - np . ones (( num_all_anchors ,), dtype = \"int32\" ) all_labels [ inside_ind ] = anchor_labels all_boxes = np . zeros (( num_all_anchors , 4 ), dtype = \"float32\" ) all_boxes [ inside_ind ] = anchor_gt_boxes start = 0 multilevel_inputs = [] for level_anchor in anchors_per_level : assert level_anchor . shape [ 2 ] == len ( self . cfg . RPN . ANCHOR_RATIOS ) anchor_shape = level_anchor . shape [ :3 ] # fHxfWxNUM_ANCHOR_RATIOS num_anchor_this_level = np . prod ( anchor_shape ) end = start + num_anchor_this_level multilevel_inputs . append ( ( all_labels [ start:end ] . reshape ( anchor_shape ), all_boxes [ start:end, : ] . reshape ( anchor_shape + ( 4 ,))) ) start = end assert end == num_all_anchors , \"{} != {}\" . format ( end , num_all_anchors ) return multilevel_inputs","title":"get_multilevel_rpn_anchor_input"},{"location":"reference/mot/object_detection/data/#get_rpn_anchor_input","text":"def get_rpn_anchor_input ( self , im , boxes , is_crowd ) Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS View Source def get_rpn_anchor_input ( self , im , boxes , is_crowd ) : \"\"\" Args: im: an image boxes: nx4, floatbox, gt. shoudn't be changed is_crowd: n, Returns: The anchor labels and target boxes for each pixel in the featuremap. fm_labels: fHxfWxNA fm_boxes: fHxfWxNAx4 NA will be NUM_ANCHOR_SIZES x NUM_ANCHOR_RATIOS \"\"\" boxes = boxes . copy () all_anchors = np . copy ( get_all_anchors ( stride = self . cfg . RPN . ANCHOR_STRIDE , sizes = self . cfg . RPN . ANCHOR_SIZES , ratios = self . cfg . RPN . ANCHOR_RATIOS , max_size = self . cfg . PREPROC . MAX_SIZE , ) ) # fHxfWxAx4 -> ( - 1 , 4 ) featuremap_anchors_flatten = all_anchors . reshape (( - 1 , 4 )) # only use anchors inside the image inside_ind , inside_anchors = filter_boxes_inside_shape ( featuremap_anchors_flatten , im . shape [ :2 ] ) # obtain anchor labels and their corresponding gt boxes anchor_labels , anchor_gt_boxes = self . get_anchor_labels ( inside_anchors , boxes [ is_crowd == 0 ] , boxes [ is_crowd == 1 ] ) # Fill them back to original size : fHxfWx1 , fHxfWx4 num_anchor = self . cfg . RPN . NUM_ANCHOR anchorH , anchorW = all_anchors . shape [ :2 ] featuremap_labels = - np . ones (( anchorH * anchorW * num_anchor ,), dtype = \"int32\" ) featuremap_labels [ inside_ind ] = anchor_labels featuremap_labels = featuremap_labels . reshape (( anchorH , anchorW , num_anchor )) featuremap_boxes = np . zeros (( anchorH * anchorW * num_anchor , 4 ), dtype = \"float32\" ) featuremap_boxes [ inside_ind, : ] = anchor_gt_boxes featuremap_boxes = featuremap_boxes . reshape (( anchorH , anchorW , num_anchor , 4 )) return featuremap_labels , featuremap_boxes","title":"get_rpn_anchor_input"},{"location":"reference/mot/object_detection/eval/","text":"Module mot.object_detection.eval View Source # -*- coding: utf-8 -*- # File: eval.py import itertools import json import numpy as np import os import sys import tensorflow as tf from collections import namedtuple from concurrent.futures import ThreadPoolExecutor from contextlib import ExitStack import cv2 import tqdm from scipy import interpolate from collections.abc import Iterable from tensorpack.callbacks import Callback from tensorpack.tfutils.common import get_tf_version_tuple from tensorpack.utils import logger , get_tqdm from mot.object_detection.common import CustomResize , clip_boxes from mot.object_detection.config import config as cfg from mot.object_detection.data import get_eval_dataflow from mot.object_detection.dataset import DatasetRegistry try : import horovod.tensorflow as hvd except ImportError : pass DetectionResult = namedtuple ( 'DetectionResult' , [ 'box' , 'score' , 'class_id' , 'mask' ]) \"\"\" box: 4 float score: float class_id: int, 1~NUM_CLASS mask: None, or a binary image of the original image shape \"\"\" def _scale_box ( box , scale ): w_half = ( box [ 2 ] - box [ 0 ]) * 0.5 h_half = ( box [ 3 ] - box [ 1 ]) * 0.5 x_c = ( box [ 2 ] + box [ 0 ]) * 0.5 y_c = ( box [ 3 ] + box [ 1 ]) * 0.5 w_half *= scale h_half *= scale scaled_box = np . zeros_like ( box ) scaled_box [ 0 ] = x_c - w_half scaled_box [ 2 ] = x_c + w_half scaled_box [ 1 ] = y_c - h_half scaled_box [ 3 ] = y_c + h_half return scaled_box def _paste_mask ( box , mask , shape ): \"\"\" Args: box: 4 float mask: MxM floats shape: h,w Returns: A uint8 binary image of hxw. \"\"\" assert mask . shape [ 0 ] == mask . shape [ 1 ], mask . shape if cfg . MRCNN . ACCURATE_PASTE : # This method is accurate but much slower. mask = np . pad ( mask , [( 1 , 1 ), ( 1 , 1 )], mode = 'constant' ) box = _scale_box ( box , float ( mask . shape [ 0 ]) / ( mask . shape [ 0 ] - 2 )) mask_pixels = np . arange ( 0.0 , mask . shape [ 0 ]) + 0.5 mask_continuous = interpolate . interp2d ( mask_pixels , mask_pixels , mask , fill_value = 0.0 ) h , w = shape ys = np . arange ( 0.0 , h ) + 0.5 xs = np . arange ( 0.0 , w ) + 0.5 ys = ( ys - box [ 1 ]) / ( box [ 3 ] - box [ 1 ]) * mask . shape [ 0 ] xs = ( xs - box [ 0 ]) / ( box [ 2 ] - box [ 0 ]) * mask . shape [ 1 ] # Waste a lot of compute since most indices are out-of-border res = mask_continuous ( xs , ys ) return ( res >= 0.5 ) . astype ( 'uint8' ) else : # This method (inspired by Detectron) is less accurate but fast. # int() is floor # box fpcoor=0.0 -> intcoor=0.0 x0 , y0 = list ( map ( int , box [: 2 ] + 0.5 )) # box fpcoor=h -> intcoor=h-1, inclusive x1 , y1 = list ( map ( int , box [ 2 :] - 0.5 )) # inclusive x1 = max ( x0 , x1 ) # require at least 1x1 y1 = max ( y0 , y1 ) w = x1 + 1 - x0 h = y1 + 1 - y0 # rounding errors could happen here, because masks were not originally computed for this shape. # but it's hard to do better, because the network does not know the \"original\" scale mask = ( cv2 . resize ( mask , ( w , h )) > 0.5 ) . astype ( 'uint8' ) ret = np . zeros ( shape , dtype = 'uint8' ) ret [ y0 : y1 + 1 , x0 : x1 + 1 ] = mask return ret def predict_image ( img , model_func , as_named_tuple = False ): \"\"\" Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] \"\"\" orig_shape = img . shape [: 2 ] resizer = CustomResize ( cfg . PREPROC . TEST_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ) resized_img = resizer . augment ( img ) scale = np . sqrt ( resized_img . shape [ 0 ] * 1.0 / img . shape [ 0 ] * resized_img . shape [ 1 ] / img . shape [ 1 ]) boxes , probs , labels , * masks = model_func ( resized_img ) # Some slow numpy postprocessing: boxes = boxes / scale # boxes are already clipped inside the graph, but after the floating point scaling, this may not be true any more. boxes = clip_boxes ( boxes , orig_shape ) if masks : full_masks = [ _paste_mask ( box , mask , orig_shape ) for box , mask in zip ( boxes , masks [ 0 ])] masks = full_masks else : # fill with none masks = [ None ] * len ( boxes ) if as_named_tuple : if len ( probs . shape ) == 2 : # if the scores of all entities are returned for each prediction probs = np . max ( probs , axis = 1 ) # we keep here only the biggest score return [ DetectionResult ( * args ) for args in zip ( boxes , probs , labels . tolist (), masks )] return ( boxes , probs , labels ) def predict_dataflow ( df , model_func , tqdm_bar = None ): \"\"\" Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" df . reset_state () all_results = [] with ExitStack () as stack : # tqdm is not quite thread-safe: https://github.com/tqdm/tqdm/issues/323 if tqdm_bar is None : tqdm_bar = stack . enter_context ( get_tqdm ( total = df . size ())) for img , img_id in df : results = predict_image ( img , model_func , as_named_tuple = True ) for r in results : # int()/float() to make it json-serializable # our model can return only the highest score, or the scores of all entities # here, we are just interested in the highest score if isinstance ( r . score , Iterable ): score = max ( r . score ) else : score = r . score res = { 'image_id' : img_id , 'category_id' : int ( r . class_id ), 'bbox' : [ round ( float ( x ), 4 ) for x in r . box ], 'score' : round ( float ( score ), 4 ), } # also append segmentation to results if r . mask is not None : import pycocotools.mask as cocomask rle = cocomask . encode ( np . array ( r . mask [:, :, None ], order = 'F' ))[ 0 ] rle [ 'counts' ] = rle [ 'counts' ] . decode ( 'ascii' ) res [ 'segmentation' ] = rle all_results . append ( res ) tqdm_bar . update ( 1 ) return all_results def multithread_predict_dataflow ( dataflows , model_funcs ): \"\"\" Running multiple `predict_dataflow` in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func:`predict_dataflow` model_funcs: a list of callable to be used in :func:`predict_dataflow` Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" num_worker = len ( model_funcs ) assert len ( dataflows ) == num_worker if num_worker == 1 : return predict_dataflow ( dataflows [ 0 ], model_funcs [ 0 ]) kwargs = { 'thread_name_prefix' : 'EvalWorker' } if sys . version_info . minor >= 6 else {} with ThreadPoolExecutor ( max_workers = num_worker , ** kwargs ) as executor , \\ tqdm . tqdm ( total = sum ([ df . size () for df in dataflows ])) as pbar : futures = [] for dataflow , pred in zip ( dataflows , model_funcs ): futures . append ( executor . submit ( predict_dataflow , dataflow , pred , pbar )) all_results = list ( itertools . chain ( * [ fut . result () for fut in futures ])) return all_results class EvalCallback ( Callback ): \"\"\" A callback that runs evaluation once a while. It supports multi-gpu evaluation. \"\"\" _chief_only = False def __init__ ( self , eval_dataset , in_names , out_names , output_dir ): self . _eval_dataset = eval_dataset self . _in_names , self . _out_names = in_names , out_names self . _output_dir = output_dir def _setup_graph ( self ): num_gpu = cfg . TRAIN . NUM_GPUS if cfg . TRAINER == 'replicated' : # TF bug in version 1.11, 1.12: https://github.com/tensorflow/tensorflow/issues/22750 buggy_tf = get_tf_version_tuple () in [( 1 , 11 ), ( 1 , 12 )] # Use two predictor threads per GPU to get better throughput self . num_predictor = num_gpu if buggy_tf else num_gpu * 2 self . predictors = [ self . _build_predictor ( k % num_gpu ) for k in range ( self . num_predictor )] self . dataflows = [ get_eval_dataflow ( self . _eval_dataset , shard = k , num_shards = self . num_predictor ) for k in range ( self . num_predictor )] else : # Only eval on the first machine, # Because evaluation assumes that all horovod workers share the filesystem. # Alternatively, can eval on all ranks and use allgather, but allgather sometimes hangs self . _horovod_run_eval = hvd . rank () == hvd . local_rank () if self . _horovod_run_eval : self . predictor = self . _build_predictor ( 0 ) self . dataflow = get_eval_dataflow ( self . _eval_dataset , shard = hvd . local_rank (), num_shards = hvd . local_size ()) self . barrier = hvd . allreduce ( tf . random_normal ( shape = [ 1 ])) def _build_predictor ( self , idx ): return self . trainer . get_predictor ( self . _in_names , self . _out_names , device = idx ) def _before_train ( self ): eval_period = cfg . TRAIN . EVAL_PERIOD self . epochs_to_eval = set () for k in itertools . count ( 1 ): if k * eval_period > self . trainer . max_epoch : break self . epochs_to_eval . add ( k * eval_period ) self . epochs_to_eval . add ( self . trainer . max_epoch ) logger . info ( \"[EvalCallback] Will evaluate every {} epochs\" . format ( eval_period )) def _eval ( self ): logdir = self . _output_dir if cfg . TRAINER == 'replicated' : all_results = multithread_predict_dataflow ( self . dataflows , self . predictors ) else : filenames = [ os . path . join ( logdir , 'outputs {} -part {} .json' . format ( self . global_step , rank ) ) for rank in range ( hvd . local_size ())] if self . _horovod_run_eval : local_results = predict_dataflow ( self . dataflow , self . predictor ) fname = filenames [ hvd . local_rank ()] with open ( fname , 'w' ) as f : json . dump ( local_results , f ) self . barrier . eval () if hvd . rank () > 0 : return all_results = [] for fname in filenames : with open ( fname , 'r' ) as f : obj = json . load ( f ) all_results . extend ( obj ) os . unlink ( fname ) scores = DatasetRegistry . get ( self . _eval_dataset ) . eval_inference_results ( all_results ) for k , v in scores . items (): self . trainer . monitors . put_scalar ( self . _eval_dataset + '-' + k , v ) def _trigger_epoch ( self ): if self . epoch_num in self . epochs_to_eval : logger . info ( \"Running evaluation ...\" ) self . _eval () Functions multithread_predict_dataflow def multithread_predict_dataflow ( dataflows , model_funcs ) Running multiple predict_dataflow in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func: predict_dataflow model_funcs: a list of callable to be used in :func: predict_dataflow Returns: list of dict, in the format used by DatasetSplit.eval_inference_results View Source def multithread_predict_dataflow ( dataflows , model_funcs ) : \" \"\" Running multiple `predict_dataflow` in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func:`predict_dataflow` model_funcs: a list of callable to be used in :func:`predict_dataflow` Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\" \" num_worker = len ( model_funcs ) assert len ( dataflows ) == num_worker if num_worker == 1 : return predict_dataflow ( dataflows [ 0 ] , model_funcs [ 0 ] ) kwargs = { 'thread_name_prefix' : 'EvalWorker' } if sys . version_info . minor >= 6 else {} with ThreadPoolExecutor ( max_workers = num_worker , ** kwargs ) as executor , \\ tqdm . tqdm ( total = sum ( [ df . size () for df in dataflows ] )) as pbar : futures = [] for dataflow , pred in zip ( dataflows , model_funcs ) : futures . append ( executor . submit ( predict_dataflow , dataflow , pred , pbar )) all_results = list ( itertools . chain ( * [ fut . result () for fut in futures ] )) return all_results predict_dataflow def predict_dataflow ( df , model_func , tqdm_bar = None ) Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by DatasetSplit.eval_inference_results View Source def predict_dataflow ( df , model_func , tqdm_bar = None ) : \"\"\" Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" df . reset_state () all_results = [] with ExitStack () as stack : # tqdm is not quite thread - safe : https : // github . com / tqdm / tqdm / issues / 323 if tqdm_bar is None : tqdm_bar = stack . enter_context ( get_tqdm ( total = df . size ())) for img , img_id in df : results = predict_image ( img , model_func , as_named_tuple = True ) for r in results : # int () / float () to make it json - serializable # our model can return only the highest score , or the scores of all entities # here , we are just interested in the highest score if isinstance ( r . score , Iterable ) : score = max ( r . score ) else : score = r . score res = { 'image_id' : img_id , 'category_id' : int ( r . class_id ), 'bbox' : [ round(float(x), 4) for x in r.box ] , 'score' : round ( float ( score ), 4 ), } # also append segmentation to results if r . mask is not None : import pycocotools . mask as cocomask rle = cocomask . encode ( np . array ( r . mask [ :, :, None ] , order = 'F' )) [ 0 ] rle [ 'counts' ] = rle [ 'counts' ] . decode ( 'ascii' ) res [ 'segmentation' ] = rle all_results . append ( res ) tqdm_bar . update ( 1 ) return all_results predict_image def predict_image ( img , model_func , as_named_tuple = False ) Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] View Source def predict_image ( img , model_func , as_named_tuple = False ) : \"\"\" Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] \"\"\" orig_shape = img . shape [ :2 ] resizer = CustomResize ( cfg . PREPROC . TEST_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ) resized_img = resizer . augment ( img ) scale = np . sqrt ( resized_img . shape [ 0 ] * 1.0 / img . shape [ 0 ] * resized_img . shape [ 1 ] / img . shape [ 1 ] ) boxes , probs , labels , * masks = model_func ( resized_img ) # Some slow numpy postprocessing : boxes = boxes / scale # boxes are already clipped inside the graph , but after the floating point scaling , this may not be true any more . boxes = clip_boxes ( boxes , orig_shape ) if masks : full_masks = [ _paste_mask(box, mask, orig_shape) for box, mask in zip(boxes, masks[0 ] ) ] masks = full_masks else : # fill with none masks = [ None ] * len ( boxes ) if as_named_tuple : if len ( probs . shape ) == 2 : # if the scores of all entities are returned for each prediction probs = np . max ( probs , axis = 1 ) # we keep here only the biggest score return [ DetectionResult(*args) for args in zip(boxes, probs, labels.tolist(), masks) ] return ( boxes , probs , labels ) Classes DetectionResult class DetectionResult ( / , * args , ** kwargs ) DetectionResult(box, score, class_id, mask) Ancestors (in MRO) builtins.tuple Instance variables box Alias for field number 0 class_id Alias for field number 2 mask Alias for field number 3 score Alias for field number 1 Methods count def count ( ... ) T.count(value) -> integer -- return number of occurrences of value index def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. EvalCallback class EvalCallback ( eval_dataset , in_names , out_names , output_dir ) A callback that runs evaluation once a while. It supports multi-gpu evaluation. Ancestors (in MRO) tensorpack.callbacks.base.Callback Class variables name_scope Instance variables chief_only Only run this callback on chief training process. Returns: bool epoch_num global_step local_step Methods after_epoch def after_epoch ( self ) View Source def after_epoch ( self ): self . _after_epoch () after_run def after_run ( self , run_context , run_values ) View Source def after_run ( self , run_context , run_values ): self . _after_run ( run_context , run_values ) after_train def after_train ( self ) View Source def after_train ( self ): self . _after_train () before_epoch def before_epoch ( self ) View Source def before_epoch ( self ): self . _before_epoch () before_run def before_run ( self , ctx ) View Source def before_run ( self , ctx ): fetches = self . _before_run ( ctx ) if fetches is None : return None if isinstance ( fetches , tf . train . SessionRunArgs ): return fetches # also support list of names assert isinstance ( fetches , list ), fetches ret = [] for f in fetches : if isinstance ( f , ( tf . Tensor , tf . Operation )): ret . append ( f ) else : # warn about speed ret . append ( get_op_or_tensor_by_name ( f )) return tf . train . SessionRunArgs ( fetches = ret ) before_train def before_train ( self ) View Source def before_train ( self ): self . _before_train () get_tensors_maybe_in_tower def get_tensors_maybe_in_tower ( self , names ) Get tensors in the graph with the given names. Will automatically check for the first training tower if no existing tensor is found with the name. Returns: [tf.Tensor] View Source def get_tensors_maybe_in_tower ( self , names ): \"\"\" Get tensors in the graph with the given names. Will automatically check for the *first training tower* if no existing tensor is found with the name. Returns: [tf.Tensor] \"\"\" from ..train.tower import TowerTrainer # noqa def get_tensor ( name ): msg = \"Tensor {} not found in the graph!\" . format ( name ) try : return get_op_or_tensor_by_name ( name ) except KeyError : pass if not isinstance ( self . trainer , TowerTrainer ): raise KeyError ( msg ) towers = self . trainer . towers try : return towers . training ()[ 0 ][ name ] except KeyError : raise KeyError ( msg ) return [ get_tensor ( name ) for name in names ] set_chief_only def set_chief_only ( self , v = True ) Set chief_only property, and returns the callback itself. View Source def set_chief_only ( self , v = True ): \"\"\" Set chief_only property, and returns the callback itself. \"\"\" self . _chief_only = v return self setup_graph def setup_graph ( self , trainer ) View Source def setup_graph ( self , trainer ): self . trainer = trainer self . graph = tf . get_default_graph () scope_name = self . name_scope or type ( self ). __name__ scope_name = scope_name . replace ( '_' , '' ) with tf . name_scope ( scope_name ): self . _setup_graph () trigger def trigger ( self ) View Source def trigger ( self ): self . _trigger () trigger_epoch def trigger_epoch ( self ) View Source def trigger_epoch ( self ): self . _trigger_epoch () trigger_step def trigger_step ( self ) View Source def trigger_step ( self ): self . _trigger_step ()","title":"Eval"},{"location":"reference/mot/object_detection/eval/#module-motobject_detectioneval","text":"View Source # -*- coding: utf-8 -*- # File: eval.py import itertools import json import numpy as np import os import sys import tensorflow as tf from collections import namedtuple from concurrent.futures import ThreadPoolExecutor from contextlib import ExitStack import cv2 import tqdm from scipy import interpolate from collections.abc import Iterable from tensorpack.callbacks import Callback from tensorpack.tfutils.common import get_tf_version_tuple from tensorpack.utils import logger , get_tqdm from mot.object_detection.common import CustomResize , clip_boxes from mot.object_detection.config import config as cfg from mot.object_detection.data import get_eval_dataflow from mot.object_detection.dataset import DatasetRegistry try : import horovod.tensorflow as hvd except ImportError : pass DetectionResult = namedtuple ( 'DetectionResult' , [ 'box' , 'score' , 'class_id' , 'mask' ]) \"\"\" box: 4 float score: float class_id: int, 1~NUM_CLASS mask: None, or a binary image of the original image shape \"\"\" def _scale_box ( box , scale ): w_half = ( box [ 2 ] - box [ 0 ]) * 0.5 h_half = ( box [ 3 ] - box [ 1 ]) * 0.5 x_c = ( box [ 2 ] + box [ 0 ]) * 0.5 y_c = ( box [ 3 ] + box [ 1 ]) * 0.5 w_half *= scale h_half *= scale scaled_box = np . zeros_like ( box ) scaled_box [ 0 ] = x_c - w_half scaled_box [ 2 ] = x_c + w_half scaled_box [ 1 ] = y_c - h_half scaled_box [ 3 ] = y_c + h_half return scaled_box def _paste_mask ( box , mask , shape ): \"\"\" Args: box: 4 float mask: MxM floats shape: h,w Returns: A uint8 binary image of hxw. \"\"\" assert mask . shape [ 0 ] == mask . shape [ 1 ], mask . shape if cfg . MRCNN . ACCURATE_PASTE : # This method is accurate but much slower. mask = np . pad ( mask , [( 1 , 1 ), ( 1 , 1 )], mode = 'constant' ) box = _scale_box ( box , float ( mask . shape [ 0 ]) / ( mask . shape [ 0 ] - 2 )) mask_pixels = np . arange ( 0.0 , mask . shape [ 0 ]) + 0.5 mask_continuous = interpolate . interp2d ( mask_pixels , mask_pixels , mask , fill_value = 0.0 ) h , w = shape ys = np . arange ( 0.0 , h ) + 0.5 xs = np . arange ( 0.0 , w ) + 0.5 ys = ( ys - box [ 1 ]) / ( box [ 3 ] - box [ 1 ]) * mask . shape [ 0 ] xs = ( xs - box [ 0 ]) / ( box [ 2 ] - box [ 0 ]) * mask . shape [ 1 ] # Waste a lot of compute since most indices are out-of-border res = mask_continuous ( xs , ys ) return ( res >= 0.5 ) . astype ( 'uint8' ) else : # This method (inspired by Detectron) is less accurate but fast. # int() is floor # box fpcoor=0.0 -> intcoor=0.0 x0 , y0 = list ( map ( int , box [: 2 ] + 0.5 )) # box fpcoor=h -> intcoor=h-1, inclusive x1 , y1 = list ( map ( int , box [ 2 :] - 0.5 )) # inclusive x1 = max ( x0 , x1 ) # require at least 1x1 y1 = max ( y0 , y1 ) w = x1 + 1 - x0 h = y1 + 1 - y0 # rounding errors could happen here, because masks were not originally computed for this shape. # but it's hard to do better, because the network does not know the \"original\" scale mask = ( cv2 . resize ( mask , ( w , h )) > 0.5 ) . astype ( 'uint8' ) ret = np . zeros ( shape , dtype = 'uint8' ) ret [ y0 : y1 + 1 , x0 : x1 + 1 ] = mask return ret def predict_image ( img , model_func , as_named_tuple = False ): \"\"\" Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] \"\"\" orig_shape = img . shape [: 2 ] resizer = CustomResize ( cfg . PREPROC . TEST_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ) resized_img = resizer . augment ( img ) scale = np . sqrt ( resized_img . shape [ 0 ] * 1.0 / img . shape [ 0 ] * resized_img . shape [ 1 ] / img . shape [ 1 ]) boxes , probs , labels , * masks = model_func ( resized_img ) # Some slow numpy postprocessing: boxes = boxes / scale # boxes are already clipped inside the graph, but after the floating point scaling, this may not be true any more. boxes = clip_boxes ( boxes , orig_shape ) if masks : full_masks = [ _paste_mask ( box , mask , orig_shape ) for box , mask in zip ( boxes , masks [ 0 ])] masks = full_masks else : # fill with none masks = [ None ] * len ( boxes ) if as_named_tuple : if len ( probs . shape ) == 2 : # if the scores of all entities are returned for each prediction probs = np . max ( probs , axis = 1 ) # we keep here only the biggest score return [ DetectionResult ( * args ) for args in zip ( boxes , probs , labels . tolist (), masks )] return ( boxes , probs , labels ) def predict_dataflow ( df , model_func , tqdm_bar = None ): \"\"\" Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" df . reset_state () all_results = [] with ExitStack () as stack : # tqdm is not quite thread-safe: https://github.com/tqdm/tqdm/issues/323 if tqdm_bar is None : tqdm_bar = stack . enter_context ( get_tqdm ( total = df . size ())) for img , img_id in df : results = predict_image ( img , model_func , as_named_tuple = True ) for r in results : # int()/float() to make it json-serializable # our model can return only the highest score, or the scores of all entities # here, we are just interested in the highest score if isinstance ( r . score , Iterable ): score = max ( r . score ) else : score = r . score res = { 'image_id' : img_id , 'category_id' : int ( r . class_id ), 'bbox' : [ round ( float ( x ), 4 ) for x in r . box ], 'score' : round ( float ( score ), 4 ), } # also append segmentation to results if r . mask is not None : import pycocotools.mask as cocomask rle = cocomask . encode ( np . array ( r . mask [:, :, None ], order = 'F' ))[ 0 ] rle [ 'counts' ] = rle [ 'counts' ] . decode ( 'ascii' ) res [ 'segmentation' ] = rle all_results . append ( res ) tqdm_bar . update ( 1 ) return all_results def multithread_predict_dataflow ( dataflows , model_funcs ): \"\"\" Running multiple `predict_dataflow` in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func:`predict_dataflow` model_funcs: a list of callable to be used in :func:`predict_dataflow` Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" num_worker = len ( model_funcs ) assert len ( dataflows ) == num_worker if num_worker == 1 : return predict_dataflow ( dataflows [ 0 ], model_funcs [ 0 ]) kwargs = { 'thread_name_prefix' : 'EvalWorker' } if sys . version_info . minor >= 6 else {} with ThreadPoolExecutor ( max_workers = num_worker , ** kwargs ) as executor , \\ tqdm . tqdm ( total = sum ([ df . size () for df in dataflows ])) as pbar : futures = [] for dataflow , pred in zip ( dataflows , model_funcs ): futures . append ( executor . submit ( predict_dataflow , dataflow , pred , pbar )) all_results = list ( itertools . chain ( * [ fut . result () for fut in futures ])) return all_results class EvalCallback ( Callback ): \"\"\" A callback that runs evaluation once a while. It supports multi-gpu evaluation. \"\"\" _chief_only = False def __init__ ( self , eval_dataset , in_names , out_names , output_dir ): self . _eval_dataset = eval_dataset self . _in_names , self . _out_names = in_names , out_names self . _output_dir = output_dir def _setup_graph ( self ): num_gpu = cfg . TRAIN . NUM_GPUS if cfg . TRAINER == 'replicated' : # TF bug in version 1.11, 1.12: https://github.com/tensorflow/tensorflow/issues/22750 buggy_tf = get_tf_version_tuple () in [( 1 , 11 ), ( 1 , 12 )] # Use two predictor threads per GPU to get better throughput self . num_predictor = num_gpu if buggy_tf else num_gpu * 2 self . predictors = [ self . _build_predictor ( k % num_gpu ) for k in range ( self . num_predictor )] self . dataflows = [ get_eval_dataflow ( self . _eval_dataset , shard = k , num_shards = self . num_predictor ) for k in range ( self . num_predictor )] else : # Only eval on the first machine, # Because evaluation assumes that all horovod workers share the filesystem. # Alternatively, can eval on all ranks and use allgather, but allgather sometimes hangs self . _horovod_run_eval = hvd . rank () == hvd . local_rank () if self . _horovod_run_eval : self . predictor = self . _build_predictor ( 0 ) self . dataflow = get_eval_dataflow ( self . _eval_dataset , shard = hvd . local_rank (), num_shards = hvd . local_size ()) self . barrier = hvd . allreduce ( tf . random_normal ( shape = [ 1 ])) def _build_predictor ( self , idx ): return self . trainer . get_predictor ( self . _in_names , self . _out_names , device = idx ) def _before_train ( self ): eval_period = cfg . TRAIN . EVAL_PERIOD self . epochs_to_eval = set () for k in itertools . count ( 1 ): if k * eval_period > self . trainer . max_epoch : break self . epochs_to_eval . add ( k * eval_period ) self . epochs_to_eval . add ( self . trainer . max_epoch ) logger . info ( \"[EvalCallback] Will evaluate every {} epochs\" . format ( eval_period )) def _eval ( self ): logdir = self . _output_dir if cfg . TRAINER == 'replicated' : all_results = multithread_predict_dataflow ( self . dataflows , self . predictors ) else : filenames = [ os . path . join ( logdir , 'outputs {} -part {} .json' . format ( self . global_step , rank ) ) for rank in range ( hvd . local_size ())] if self . _horovod_run_eval : local_results = predict_dataflow ( self . dataflow , self . predictor ) fname = filenames [ hvd . local_rank ()] with open ( fname , 'w' ) as f : json . dump ( local_results , f ) self . barrier . eval () if hvd . rank () > 0 : return all_results = [] for fname in filenames : with open ( fname , 'r' ) as f : obj = json . load ( f ) all_results . extend ( obj ) os . unlink ( fname ) scores = DatasetRegistry . get ( self . _eval_dataset ) . eval_inference_results ( all_results ) for k , v in scores . items (): self . trainer . monitors . put_scalar ( self . _eval_dataset + '-' + k , v ) def _trigger_epoch ( self ): if self . epoch_num in self . epochs_to_eval : logger . info ( \"Running evaluation ...\" ) self . _eval ()","title":"Module mot.object_detection.eval"},{"location":"reference/mot/object_detection/eval/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/eval/#multithread_predict_dataflow","text":"def multithread_predict_dataflow ( dataflows , model_funcs ) Running multiple predict_dataflow in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func: predict_dataflow model_funcs: a list of callable to be used in :func: predict_dataflow Returns: list of dict, in the format used by DatasetSplit.eval_inference_results View Source def multithread_predict_dataflow ( dataflows , model_funcs ) : \" \"\" Running multiple `predict_dataflow` in multiple threads, and aggregate the results. Args: dataflows: a list of DataFlow to be used in :func:`predict_dataflow` model_funcs: a list of callable to be used in :func:`predict_dataflow` Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\" \" num_worker = len ( model_funcs ) assert len ( dataflows ) == num_worker if num_worker == 1 : return predict_dataflow ( dataflows [ 0 ] , model_funcs [ 0 ] ) kwargs = { 'thread_name_prefix' : 'EvalWorker' } if sys . version_info . minor >= 6 else {} with ThreadPoolExecutor ( max_workers = num_worker , ** kwargs ) as executor , \\ tqdm . tqdm ( total = sum ( [ df . size () for df in dataflows ] )) as pbar : futures = [] for dataflow , pred in zip ( dataflows , model_funcs ) : futures . append ( executor . submit ( predict_dataflow , dataflow , pred , pbar )) all_results = list ( itertools . chain ( * [ fut . result () for fut in futures ] )) return all_results","title":"multithread_predict_dataflow"},{"location":"reference/mot/object_detection/eval/#predict_dataflow","text":"def predict_dataflow ( df , model_func , tqdm_bar = None ) Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by DatasetSplit.eval_inference_results View Source def predict_dataflow ( df , model_func , tqdm_bar = None ) : \"\"\" Args: df: a DataFlow which produces (image, image_id) model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) tqdm_bar: a tqdm object to be shared among multiple evaluation instances. If None, will create a new one. Returns: list of dict, in the format used by `DatasetSplit.eval_inference_results` \"\"\" df . reset_state () all_results = [] with ExitStack () as stack : # tqdm is not quite thread - safe : https : // github . com / tqdm / tqdm / issues / 323 if tqdm_bar is None : tqdm_bar = stack . enter_context ( get_tqdm ( total = df . size ())) for img , img_id in df : results = predict_image ( img , model_func , as_named_tuple = True ) for r in results : # int () / float () to make it json - serializable # our model can return only the highest score , or the scores of all entities # here , we are just interested in the highest score if isinstance ( r . score , Iterable ) : score = max ( r . score ) else : score = r . score res = { 'image_id' : img_id , 'category_id' : int ( r . class_id ), 'bbox' : [ round(float(x), 4) for x in r.box ] , 'score' : round ( float ( score ), 4 ), } # also append segmentation to results if r . mask is not None : import pycocotools . mask as cocomask rle = cocomask . encode ( np . array ( r . mask [ :, :, None ] , order = 'F' )) [ 0 ] rle [ 'counts' ] = rle [ 'counts' ] . decode ( 'ascii' ) res [ 'segmentation' ] = rle all_results . append ( res ) tqdm_bar . update ( 1 ) return all_results","title":"predict_dataflow"},{"location":"reference/mot/object_detection/eval/#predict_image","text":"def predict_image ( img , model_func , as_named_tuple = False ) Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] View Source def predict_image ( img , model_func , as_named_tuple = False ) : \"\"\" Run detection on one image, using the TF callable. This function should handle the preprocessing internally. Args: img: an image model_func: a callable from the TF model. It takes image and returns (boxes, probs, labels, [masks]) Returns: [DetectionResult] \"\"\" orig_shape = img . shape [ :2 ] resizer = CustomResize ( cfg . PREPROC . TEST_SHORT_EDGE_SIZE , cfg . PREPROC . MAX_SIZE ) resized_img = resizer . augment ( img ) scale = np . sqrt ( resized_img . shape [ 0 ] * 1.0 / img . shape [ 0 ] * resized_img . shape [ 1 ] / img . shape [ 1 ] ) boxes , probs , labels , * masks = model_func ( resized_img ) # Some slow numpy postprocessing : boxes = boxes / scale # boxes are already clipped inside the graph , but after the floating point scaling , this may not be true any more . boxes = clip_boxes ( boxes , orig_shape ) if masks : full_masks = [ _paste_mask(box, mask, orig_shape) for box, mask in zip(boxes, masks[0 ] ) ] masks = full_masks else : # fill with none masks = [ None ] * len ( boxes ) if as_named_tuple : if len ( probs . shape ) == 2 : # if the scores of all entities are returned for each prediction probs = np . max ( probs , axis = 1 ) # we keep here only the biggest score return [ DetectionResult(*args) for args in zip(boxes, probs, labels.tolist(), masks) ] return ( boxes , probs , labels )","title":"predict_image"},{"location":"reference/mot/object_detection/eval/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/eval/#detectionresult","text":"class DetectionResult ( / , * args , ** kwargs ) DetectionResult(box, score, class_id, mask)","title":"DetectionResult"},{"location":"reference/mot/object_detection/eval/#ancestors-in-mro","text":"builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/eval/#instance-variables","text":"box Alias for field number 0 class_id Alias for field number 2 mask Alias for field number 3 score Alias for field number 1","title":"Instance variables"},{"location":"reference/mot/object_detection/eval/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/eval/#count","text":"def count ( ... ) T.count(value) -> integer -- return number of occurrences of value","title":"count"},{"location":"reference/mot/object_detection/eval/#index","text":"def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/mot/object_detection/eval/#evalcallback","text":"class EvalCallback ( eval_dataset , in_names , out_names , output_dir ) A callback that runs evaluation once a while. It supports multi-gpu evaluation.","title":"EvalCallback"},{"location":"reference/mot/object_detection/eval/#ancestors-in-mro_1","text":"tensorpack.callbacks.base.Callback","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/eval/#class-variables","text":"name_scope","title":"Class variables"},{"location":"reference/mot/object_detection/eval/#instance-variables_1","text":"chief_only Only run this callback on chief training process. Returns: bool epoch_num global_step local_step","title":"Instance variables"},{"location":"reference/mot/object_detection/eval/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/object_detection/eval/#after_epoch","text":"def after_epoch ( self ) View Source def after_epoch ( self ): self . _after_epoch ()","title":"after_epoch"},{"location":"reference/mot/object_detection/eval/#after_run","text":"def after_run ( self , run_context , run_values ) View Source def after_run ( self , run_context , run_values ): self . _after_run ( run_context , run_values )","title":"after_run"},{"location":"reference/mot/object_detection/eval/#after_train","text":"def after_train ( self ) View Source def after_train ( self ): self . _after_train ()","title":"after_train"},{"location":"reference/mot/object_detection/eval/#before_epoch","text":"def before_epoch ( self ) View Source def before_epoch ( self ): self . _before_epoch ()","title":"before_epoch"},{"location":"reference/mot/object_detection/eval/#before_run","text":"def before_run ( self , ctx ) View Source def before_run ( self , ctx ): fetches = self . _before_run ( ctx ) if fetches is None : return None if isinstance ( fetches , tf . train . SessionRunArgs ): return fetches # also support list of names assert isinstance ( fetches , list ), fetches ret = [] for f in fetches : if isinstance ( f , ( tf . Tensor , tf . Operation )): ret . append ( f ) else : # warn about speed ret . append ( get_op_or_tensor_by_name ( f )) return tf . train . SessionRunArgs ( fetches = ret )","title":"before_run"},{"location":"reference/mot/object_detection/eval/#before_train","text":"def before_train ( self ) View Source def before_train ( self ): self . _before_train ()","title":"before_train"},{"location":"reference/mot/object_detection/eval/#get_tensors_maybe_in_tower","text":"def get_tensors_maybe_in_tower ( self , names ) Get tensors in the graph with the given names. Will automatically check for the first training tower if no existing tensor is found with the name. Returns: [tf.Tensor] View Source def get_tensors_maybe_in_tower ( self , names ): \"\"\" Get tensors in the graph with the given names. Will automatically check for the *first training tower* if no existing tensor is found with the name. Returns: [tf.Tensor] \"\"\" from ..train.tower import TowerTrainer # noqa def get_tensor ( name ): msg = \"Tensor {} not found in the graph!\" . format ( name ) try : return get_op_or_tensor_by_name ( name ) except KeyError : pass if not isinstance ( self . trainer , TowerTrainer ): raise KeyError ( msg ) towers = self . trainer . towers try : return towers . training ()[ 0 ][ name ] except KeyError : raise KeyError ( msg ) return [ get_tensor ( name ) for name in names ]","title":"get_tensors_maybe_in_tower"},{"location":"reference/mot/object_detection/eval/#set_chief_only","text":"def set_chief_only ( self , v = True ) Set chief_only property, and returns the callback itself. View Source def set_chief_only ( self , v = True ): \"\"\" Set chief_only property, and returns the callback itself. \"\"\" self . _chief_only = v return self","title":"set_chief_only"},{"location":"reference/mot/object_detection/eval/#setup_graph","text":"def setup_graph ( self , trainer ) View Source def setup_graph ( self , trainer ): self . trainer = trainer self . graph = tf . get_default_graph () scope_name = self . name_scope or type ( self ). __name__ scope_name = scope_name . replace ( '_' , '' ) with tf . name_scope ( scope_name ): self . _setup_graph ()","title":"setup_graph"},{"location":"reference/mot/object_detection/eval/#trigger","text":"def trigger ( self ) View Source def trigger ( self ): self . _trigger ()","title":"trigger"},{"location":"reference/mot/object_detection/eval/#trigger_epoch","text":"def trigger_epoch ( self ) View Source def trigger_epoch ( self ): self . _trigger_epoch ()","title":"trigger_epoch"},{"location":"reference/mot/object_detection/eval/#trigger_step","text":"def trigger_step ( self ) View Source def trigger_step ( self ): self . _trigger_step ()","title":"trigger_step"},{"location":"reference/mot/object_detection/predict/","text":"Module mot.object_detection.predict View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 #!/usr/bin/env python # -*- coding: utf-8 -*- import argparse import itertools import numpy as np import os import shutil import tensorflow as tf import cv2 import six import tqdm assert six . PY3 , \"This example requires Python 3!\" import tensorpack.utils.viz as tpviz from tensorpack.predict import MultiTowerOfflinePredictor , OfflinePredictor , PredictConfig from tensorpack.tfutils import SmartInit , get_tf_version_tuple from tensorpack.tfutils.export import ModelExporter from tensorpack.utils import fs , logger from mot.object_detection.dataset import DatasetRegistry , register_mot from mot.object_detection.config import config as cfg from mot.object_detection.config import finalize_configs from mot.object_detection.data import get_eval_dataflow , get_train_dataflow from mot.object_detection.eval import DetectionResult , multithread_predict_dataflow , predict_image from mot.object_detection.modeling.generalized_rcnn import ResNetC4Model , ResNetFPNModel from mot.object_detection.viz import ( draw_annotation , draw_final_outputs , draw_predictions , draw_proposal_recall , draw_final_outputs_blackwhite ) def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ): \"\"\" Visualize some intermediate results (proposals, raw predictions) inside the pipeline. \"\"\" df = get_train_dataflow () df . reset_state () pred = OfflinePredictor ( PredictConfig ( model = model , session_init = SmartInit ( model_path ), input_names = [ 'image' , 'gt_boxes' , 'gt_labels' ], output_names = [ 'generate_ {} _proposals/boxes' . format ( 'fpn' if cfg . MODE_FPN else 'rpn' ), 'generate_ {} _proposals/scores' . format ( 'fpn' if cfg . MODE_FPN else 'rpn' ), 'fastrcnn_all_scores' , 'output/boxes' , 'output/scores' , 'output/labels' , ])) if os . path . isdir ( output_dir ): shutil . rmtree ( output_dir ) fs . mkdir_p ( output_dir ) with tqdm . tqdm ( total = nr_visualize ) as pbar : for idx , dp in itertools . islice ( enumerate ( df ), nr_visualize ): img , gt_boxes , gt_labels = dp [ 'image' ], dp [ 'gt_boxes' ], dp [ 'gt_labels' ] rpn_boxes , rpn_scores , all_scores , \\ final_boxes , final_scores , final_labels = pred ( img , gt_boxes , gt_labels ) # draw groundtruth boxes gt_viz = draw_annotation ( img , gt_boxes , gt_labels ) # draw best proposals for each groundtruth, to show recall proposal_viz , good_proposals_ind = draw_proposal_recall ( img , rpn_boxes , rpn_scores , gt_boxes ) # draw the scores for the above proposals score_viz = draw_predictions ( img , rpn_boxes [ good_proposals_ind ], all_scores [ good_proposals_ind ]) results = [ DetectionResult ( * args ) for args in zip ( final_boxes , final_scores , final_labels , [ None ] * len ( final_labels ))] final_viz = draw_final_outputs ( img , results ) viz = tpviz . stack_patches ([ gt_viz , proposal_viz , score_viz , final_viz ], 2 , 2 ) if os . environ . get ( 'DISPLAY' , None ): tpviz . interactive_imshow ( viz ) cv2 . imwrite ( \" {} / {:03d} .png\" . format ( output_dir , idx ), viz ) pbar . update () def do_evaluate ( pred_config , output_file ): num_tower = max ( cfg . TRAIN . NUM_GPUS , 1 ) graph_funcs = MultiTowerOfflinePredictor ( pred_config , list ( range ( num_tower ))) . get_predictors () for dataset in cfg . DATA . VAL : logger . info ( \"Evaluating {} ...\" . format ( dataset )) dataflows = [ get_eval_dataflow ( dataset , shard = k , num_shards = num_tower ) for k in range ( num_tower )] all_results = multithread_predict_dataflow ( dataflows , graph_funcs ) output = output_file + '-' + dataset DatasetRegistry . get ( dataset ) . eval_inference_results ( all_results , output ) def do_predict ( pred_func , input_file , visualize = False ): img = cv2 . imread ( input_file , cv2 . IMREAD_COLOR ) if not visualize : return predict_image ( img , pred_func ) results = predict_image ( img , pred_func , as_named_tuple = True ) if cfg . MODE_MASK : final = draw_final_outputs_blackwhite ( img , results ) else : final = draw_final_outputs ( img , results ) viz = np . concatenate (( img , final ), axis = 1 ) cv2 . imwrite ( \"output.png\" , viz ) logger . info ( \"Inference output for {} written to output.png\" . format ( input_file )) tpviz . interactive_imshow ( viz ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--load' , help = 'load a model for evaluation.' , required = True ) parser . add_argument ( '--visualize' , action = 'store_true' , help = 'visualize intermediate results' ) parser . add_argument ( '--evaluate' , help = \"Run evaluation. \" \"This argument is the path to the output json evaluation file\" ) parser . add_argument ( '--predict' , help = \"Run prediction on a given image. \" \"This argument is the path to the input image file\" , nargs = '+' ) parser . add_argument ( '--benchmark' , action = 'store_true' , help = \"Benchmark the speed of the model + postprocessing\" ) parser . add_argument ( '--config' , help = \"A list of KEY=VALUE to overwrite those defined in config.py\" , nargs = '+' ) parser . add_argument ( '--compact' , help = 'Save a model to .pb' ) parser . add_argument ( '--serving' , help = 'Save a model to serving file' ) args = parser . parse_args () if args . config : cfg . update_args ( args . config ) register_mot ( cfg . DATA . BASEDIR ) # add the mot datasets to the registry MODEL = ResNetFPNModel () if cfg . MODE_FPN else ResNetC4Model () if not tf . test . is_gpu_available (): from tensorflow.python.framework import test_util assert get_tf_version_tuple () >= ( 1 , 7 ) and test_util . IsMklEnabled (), \\ \"Inference requires either GPU support or MKL support!\" assert args . load finalize_configs ( is_training = False ) if args . predict or args . visualize : cfg . TEST . RESULT_SCORE_THRESH = cfg . TEST . RESULT_SCORE_THRESH_VIS if args . visualize : do_visualize ( MODEL , args . load ) else : predcfg = PredictConfig ( model = MODEL , session_init = SmartInit ( args . load ), input_names = MODEL . get_inference_tensor_names ()[ 0 ], output_names = MODEL . get_inference_tensor_names ()[ 1 ]) if args . compact : ModelExporter ( predcfg ) . export_compact ( args . compact ) elif args . serving : ModelExporter ( predcfg ) . export_serving ( args . serving , signature_name = \"serving_default\" ) if args . predict : predictor = OfflinePredictor ( predcfg ) for image_file in args . predict : do_predict ( predictor , image_file , visualize = True ) elif args . evaluate : assert args . evaluate . endswith ( '.json' ), args . evaluate do_evaluate ( predcfg , args . evaluate ) elif args . benchmark : df = get_eval_dataflow ( cfg . DATA . VAL [ 0 ]) df . reset_state () predictor = OfflinePredictor ( predcfg ) for _ , img in enumerate ( tqdm . tqdm ( df , total = len ( df ), smoothing = 0.5 )): # This includes post-processing time, which is done on CPU and not optimized # To exclude it, modify `predict_image`. predict_image ( img [ 0 ], predictor ) Functions do_evaluate def do_evaluate ( pred_config , output_file ) View Source def do_evaluate ( pred_config , output_file ): num_tower = max ( cfg . TRAIN . NUM_GPUS , 1 ) graph_funcs = MultiTowerOfflinePredictor ( pred_config , list ( range ( num_tower ))). get_predictors () for dataset in cfg . DATA . VAL : logger . info ( \"Evaluating {} ...\" . format ( dataset )) dataflows = [ get_eval_dataflow ( dataset , shard = k , num_shards = num_tower ) for k in range ( num_tower )] all_results = multithread_predict_dataflow ( dataflows , graph_funcs ) output = output_file + '-' + dataset DatasetRegistry . get ( dataset ). eval_inference_results ( all_results , output ) do_predict def do_predict ( pred_func , input_file , visualize = False ) View Source def do_predict ( pred_func , input_file , visualize = False ): img = cv2 . imread ( input_file , cv2 . IMREAD_COLOR ) if not visualize : return predict_image ( img , pred_func ) results = predict_image ( img , pred_func , as_named_tuple = True ) if cfg . MODE_MASK : final = draw_final_outputs_blackwhite ( img , results ) else : final = draw_final_outputs ( img , results ) viz = np . concatenate (( img , final ), axis = 1 ) cv2 . imwrite ( \"output.png\" , viz ) logger . info ( \"Inference output for {} written to output.png\" . format ( input_file )) tpviz . interactive_imshow ( viz ) do_visualize def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ) Visualize some intermediate results (proposals, raw predictions) inside the pipeline. View Source def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ) : \"\"\" Visualize some intermediate results (proposals, raw predictions) inside the pipeline. \"\"\" df = get_train_dataflow () df . reset_state () pred = OfflinePredictor ( PredictConfig ( model = model , session_init = SmartInit ( model_path ), input_names =[ 'image', 'gt_boxes', 'gt_labels' ] , output_names =[ 'generate_{}_proposals/boxes'.format('fpn' if cfg.MODE_FPN else 'rpn'), 'generate_{}_proposals/scores'.format('fpn' if cfg.MODE_FPN else 'rpn'), 'fastrcnn_all_scores', 'output/boxes', 'output/scores', 'output/labels', ] )) if os . path . isdir ( output_dir ) : shutil . rmtree ( output_dir ) fs . mkdir_p ( output_dir ) with tqdm . tqdm ( total = nr_visualize ) as pbar : for idx , dp in itertools . islice ( enumerate ( df ), nr_visualize ) : img , gt_boxes , gt_labels = dp [ 'image' ] , dp [ 'gt_boxes' ] , dp [ 'gt_labels' ] rpn_boxes , rpn_scores , all_scores , \\ final_boxes , final_scores , final_labels = pred ( img , gt_boxes , gt_labels ) # draw groundtruth boxes gt_viz = draw_annotation ( img , gt_boxes , gt_labels ) # draw best proposals for each groundtruth , to show recall proposal_viz , good_proposals_ind = draw_proposal_recall ( img , rpn_boxes , rpn_scores , gt_boxes ) # draw the scores for the above proposals score_viz = draw_predictions ( img , rpn_boxes [ good_proposals_ind ] , all_scores [ good_proposals_ind ] ) results = [ DetectionResult(*args) for args in zip(final_boxes, final_scores, final_labels, [None ] * len ( final_labels )) ] final_viz = draw_final_outputs ( img , results ) viz = tpviz . stack_patches ( [ gt_viz, proposal_viz, score_viz, final_viz ] , 2 , 2 ) if os . environ . get ( 'DISPLAY' , None ) : tpviz . interactive_imshow ( viz ) cv2 . imwrite ( \"{}/{:03d}.png\" . format ( output_dir , idx ), viz ) pbar . update ()","title":"Predict"},{"location":"reference/mot/object_detection/predict/#module-motobject_detectionpredict","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 #!/usr/bin/env python # -*- coding: utf-8 -*- import argparse import itertools import numpy as np import os import shutil import tensorflow as tf import cv2 import six import tqdm assert six . PY3 , \"This example requires Python 3!\" import tensorpack.utils.viz as tpviz from tensorpack.predict import MultiTowerOfflinePredictor , OfflinePredictor , PredictConfig from tensorpack.tfutils import SmartInit , get_tf_version_tuple from tensorpack.tfutils.export import ModelExporter from tensorpack.utils import fs , logger from mot.object_detection.dataset import DatasetRegistry , register_mot from mot.object_detection.config import config as cfg from mot.object_detection.config import finalize_configs from mot.object_detection.data import get_eval_dataflow , get_train_dataflow from mot.object_detection.eval import DetectionResult , multithread_predict_dataflow , predict_image from mot.object_detection.modeling.generalized_rcnn import ResNetC4Model , ResNetFPNModel from mot.object_detection.viz import ( draw_annotation , draw_final_outputs , draw_predictions , draw_proposal_recall , draw_final_outputs_blackwhite ) def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ): \"\"\" Visualize some intermediate results (proposals, raw predictions) inside the pipeline. \"\"\" df = get_train_dataflow () df . reset_state () pred = OfflinePredictor ( PredictConfig ( model = model , session_init = SmartInit ( model_path ), input_names = [ 'image' , 'gt_boxes' , 'gt_labels' ], output_names = [ 'generate_ {} _proposals/boxes' . format ( 'fpn' if cfg . MODE_FPN else 'rpn' ), 'generate_ {} _proposals/scores' . format ( 'fpn' if cfg . MODE_FPN else 'rpn' ), 'fastrcnn_all_scores' , 'output/boxes' , 'output/scores' , 'output/labels' , ])) if os . path . isdir ( output_dir ): shutil . rmtree ( output_dir ) fs . mkdir_p ( output_dir ) with tqdm . tqdm ( total = nr_visualize ) as pbar : for idx , dp in itertools . islice ( enumerate ( df ), nr_visualize ): img , gt_boxes , gt_labels = dp [ 'image' ], dp [ 'gt_boxes' ], dp [ 'gt_labels' ] rpn_boxes , rpn_scores , all_scores , \\ final_boxes , final_scores , final_labels = pred ( img , gt_boxes , gt_labels ) # draw groundtruth boxes gt_viz = draw_annotation ( img , gt_boxes , gt_labels ) # draw best proposals for each groundtruth, to show recall proposal_viz , good_proposals_ind = draw_proposal_recall ( img , rpn_boxes , rpn_scores , gt_boxes ) # draw the scores for the above proposals score_viz = draw_predictions ( img , rpn_boxes [ good_proposals_ind ], all_scores [ good_proposals_ind ]) results = [ DetectionResult ( * args ) for args in zip ( final_boxes , final_scores , final_labels , [ None ] * len ( final_labels ))] final_viz = draw_final_outputs ( img , results ) viz = tpviz . stack_patches ([ gt_viz , proposal_viz , score_viz , final_viz ], 2 , 2 ) if os . environ . get ( 'DISPLAY' , None ): tpviz . interactive_imshow ( viz ) cv2 . imwrite ( \" {} / {:03d} .png\" . format ( output_dir , idx ), viz ) pbar . update () def do_evaluate ( pred_config , output_file ): num_tower = max ( cfg . TRAIN . NUM_GPUS , 1 ) graph_funcs = MultiTowerOfflinePredictor ( pred_config , list ( range ( num_tower ))) . get_predictors () for dataset in cfg . DATA . VAL : logger . info ( \"Evaluating {} ...\" . format ( dataset )) dataflows = [ get_eval_dataflow ( dataset , shard = k , num_shards = num_tower ) for k in range ( num_tower )] all_results = multithread_predict_dataflow ( dataflows , graph_funcs ) output = output_file + '-' + dataset DatasetRegistry . get ( dataset ) . eval_inference_results ( all_results , output ) def do_predict ( pred_func , input_file , visualize = False ): img = cv2 . imread ( input_file , cv2 . IMREAD_COLOR ) if not visualize : return predict_image ( img , pred_func ) results = predict_image ( img , pred_func , as_named_tuple = True ) if cfg . MODE_MASK : final = draw_final_outputs_blackwhite ( img , results ) else : final = draw_final_outputs ( img , results ) viz = np . concatenate (( img , final ), axis = 1 ) cv2 . imwrite ( \"output.png\" , viz ) logger . info ( \"Inference output for {} written to output.png\" . format ( input_file )) tpviz . interactive_imshow ( viz ) if __name__ == '__main__' : parser = argparse . ArgumentParser () parser . add_argument ( '--load' , help = 'load a model for evaluation.' , required = True ) parser . add_argument ( '--visualize' , action = 'store_true' , help = 'visualize intermediate results' ) parser . add_argument ( '--evaluate' , help = \"Run evaluation. \" \"This argument is the path to the output json evaluation file\" ) parser . add_argument ( '--predict' , help = \"Run prediction on a given image. \" \"This argument is the path to the input image file\" , nargs = '+' ) parser . add_argument ( '--benchmark' , action = 'store_true' , help = \"Benchmark the speed of the model + postprocessing\" ) parser . add_argument ( '--config' , help = \"A list of KEY=VALUE to overwrite those defined in config.py\" , nargs = '+' ) parser . add_argument ( '--compact' , help = 'Save a model to .pb' ) parser . add_argument ( '--serving' , help = 'Save a model to serving file' ) args = parser . parse_args () if args . config : cfg . update_args ( args . config ) register_mot ( cfg . DATA . BASEDIR ) # add the mot datasets to the registry MODEL = ResNetFPNModel () if cfg . MODE_FPN else ResNetC4Model () if not tf . test . is_gpu_available (): from tensorflow.python.framework import test_util assert get_tf_version_tuple () >= ( 1 , 7 ) and test_util . IsMklEnabled (), \\ \"Inference requires either GPU support or MKL support!\" assert args . load finalize_configs ( is_training = False ) if args . predict or args . visualize : cfg . TEST . RESULT_SCORE_THRESH = cfg . TEST . RESULT_SCORE_THRESH_VIS if args . visualize : do_visualize ( MODEL , args . load ) else : predcfg = PredictConfig ( model = MODEL , session_init = SmartInit ( args . load ), input_names = MODEL . get_inference_tensor_names ()[ 0 ], output_names = MODEL . get_inference_tensor_names ()[ 1 ]) if args . compact : ModelExporter ( predcfg ) . export_compact ( args . compact ) elif args . serving : ModelExporter ( predcfg ) . export_serving ( args . serving , signature_name = \"serving_default\" ) if args . predict : predictor = OfflinePredictor ( predcfg ) for image_file in args . predict : do_predict ( predictor , image_file , visualize = True ) elif args . evaluate : assert args . evaluate . endswith ( '.json' ), args . evaluate do_evaluate ( predcfg , args . evaluate ) elif args . benchmark : df = get_eval_dataflow ( cfg . DATA . VAL [ 0 ]) df . reset_state () predictor = OfflinePredictor ( predcfg ) for _ , img in enumerate ( tqdm . tqdm ( df , total = len ( df ), smoothing = 0.5 )): # This includes post-processing time, which is done on CPU and not optimized # To exclude it, modify `predict_image`. predict_image ( img [ 0 ], predictor )","title":"Module mot.object_detection.predict"},{"location":"reference/mot/object_detection/predict/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/predict/#do_evaluate","text":"def do_evaluate ( pred_config , output_file ) View Source def do_evaluate ( pred_config , output_file ): num_tower = max ( cfg . TRAIN . NUM_GPUS , 1 ) graph_funcs = MultiTowerOfflinePredictor ( pred_config , list ( range ( num_tower ))). get_predictors () for dataset in cfg . DATA . VAL : logger . info ( \"Evaluating {} ...\" . format ( dataset )) dataflows = [ get_eval_dataflow ( dataset , shard = k , num_shards = num_tower ) for k in range ( num_tower )] all_results = multithread_predict_dataflow ( dataflows , graph_funcs ) output = output_file + '-' + dataset DatasetRegistry . get ( dataset ). eval_inference_results ( all_results , output )","title":"do_evaluate"},{"location":"reference/mot/object_detection/predict/#do_predict","text":"def do_predict ( pred_func , input_file , visualize = False ) View Source def do_predict ( pred_func , input_file , visualize = False ): img = cv2 . imread ( input_file , cv2 . IMREAD_COLOR ) if not visualize : return predict_image ( img , pred_func ) results = predict_image ( img , pred_func , as_named_tuple = True ) if cfg . MODE_MASK : final = draw_final_outputs_blackwhite ( img , results ) else : final = draw_final_outputs ( img , results ) viz = np . concatenate (( img , final ), axis = 1 ) cv2 . imwrite ( \"output.png\" , viz ) logger . info ( \"Inference output for {} written to output.png\" . format ( input_file )) tpviz . interactive_imshow ( viz )","title":"do_predict"},{"location":"reference/mot/object_detection/predict/#do_visualize","text":"def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ) Visualize some intermediate results (proposals, raw predictions) inside the pipeline. View Source def do_visualize ( model , model_path , nr_visualize = 100 , output_dir = 'output' ) : \"\"\" Visualize some intermediate results (proposals, raw predictions) inside the pipeline. \"\"\" df = get_train_dataflow () df . reset_state () pred = OfflinePredictor ( PredictConfig ( model = model , session_init = SmartInit ( model_path ), input_names =[ 'image', 'gt_boxes', 'gt_labels' ] , output_names =[ 'generate_{}_proposals/boxes'.format('fpn' if cfg.MODE_FPN else 'rpn'), 'generate_{}_proposals/scores'.format('fpn' if cfg.MODE_FPN else 'rpn'), 'fastrcnn_all_scores', 'output/boxes', 'output/scores', 'output/labels', ] )) if os . path . isdir ( output_dir ) : shutil . rmtree ( output_dir ) fs . mkdir_p ( output_dir ) with tqdm . tqdm ( total = nr_visualize ) as pbar : for idx , dp in itertools . islice ( enumerate ( df ), nr_visualize ) : img , gt_boxes , gt_labels = dp [ 'image' ] , dp [ 'gt_boxes' ] , dp [ 'gt_labels' ] rpn_boxes , rpn_scores , all_scores , \\ final_boxes , final_scores , final_labels = pred ( img , gt_boxes , gt_labels ) # draw groundtruth boxes gt_viz = draw_annotation ( img , gt_boxes , gt_labels ) # draw best proposals for each groundtruth , to show recall proposal_viz , good_proposals_ind = draw_proposal_recall ( img , rpn_boxes , rpn_scores , gt_boxes ) # draw the scores for the above proposals score_viz = draw_predictions ( img , rpn_boxes [ good_proposals_ind ] , all_scores [ good_proposals_ind ] ) results = [ DetectionResult(*args) for args in zip(final_boxes, final_scores, final_labels, [None ] * len ( final_labels )) ] final_viz = draw_final_outputs ( img , results ) viz = tpviz . stack_patches ( [ gt_viz, proposal_viz, score_viz, final_viz ] , 2 , 2 ) if os . environ . get ( 'DISPLAY' , None ) : tpviz . interactive_imshow ( viz ) cv2 . imwrite ( \"{}/{:03d}.png\" . format ( output_dir , idx ), viz ) pbar . update ()","title":"do_visualize"},{"location":"reference/mot/object_detection/preprocessing/","text":"Module mot.object_detection.preprocessing View Source import numpy as np import cv2 def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : - *image*: A np.array of size [height, width, channels]. - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 - *scale_ratio*: The ratio used to scale back the boxes to the good shape \"\"\" image = image . astype ( np . float32 , copy = False ) im_size_min = np . min ( image . shape [ 0 : 2 ]) im_size_max = np . max ( image . shape [ 0 : 2 ]) scale_ratio = float ( min_dimension ) / float ( im_size_min ) # Prevent the biggest axis from being more than MAX_SIZE if np . round ( scale_ratio * im_size_max ) > max_dimension : scale_ratio = float ( max_dimension ) / float ( im_size_max ) resized_image = cv2 . resize ( image , None , None , fx = scale_ratio , fy = scale_ratio , interpolation = cv2 . INTER_LINEAR ) return resized_image , scale_ratio def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ): \"\"\"Adapt the preprocessing to the tensorpack Arguments: - *image*: A np.array of shape [height, width, channels] in BGR - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *input_signature*: A dictionary which match the server signature - *scaling_ratio*: A float representing the scaling to resize the image. \"\"\" resized_image , scale_ratio = resize_to_min_dimension ( image , min_dimension , max_dimension ) return { \"inputs\" : resized_image . tolist ()}, scale_ratio Functions preprocess_for_serving def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ) Adapt the preprocessing to the tensorpack Arguments: image : A np.array of shape [height, width, channels] in BGR min_dimension : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: input_signature : A dictionary which match the server signature scaling_ratio : A float representing the scaling to resize the image. View Source def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ): \"\"\"Adapt the preprocessing to the tensorpack Arguments: - *image*: A np.array of shape [height, width, channels] in BGR - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *input_signature*: A dictionary which match the server signature - *scaling_ratio*: A float representing the scaling to resize the image. \"\"\" resized_image , scale_ratio = resize_to_min_dimension ( image , min_dimension , max_dimension ) return { \"inputs\" : resized_image . tolist () } , scale_ratio resize_to_min_dimension def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : image : A np.array of size [height, width, channels]. min_dimension : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: resized_image : The input image resized with the aspect_ratio preserved in float32 scale_ratio : The ratio used to scale back the boxes to the good shape View Source def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : - *image*: A np.array of size [height, width, channels]. - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 - *scale_ratio*: The ratio used to scale back the boxes to the good shape \"\"\" image = image . astype ( np . float32 , copy = False ) im_size_min = np . min ( image . shape [ 0 : 2 ]) im_size_max = np . max ( image . shape [ 0 : 2 ]) scale_ratio = float ( min_dimension ) / float ( im_size_min ) # Prevent the biggest axis from being more than MAX_SIZE if np . round ( scale_ratio * im_size_max ) > max_dimension : scale_ratio = float ( max_dimension ) / float ( im_size_max ) resized_image = cv2 . resize ( image , None , None , fx = scale_ratio , fy = scale_ratio , interpolation = cv2 . INTER_LINEAR ) return resized_image , scale_ratio","title":"Preprocessing"},{"location":"reference/mot/object_detection/preprocessing/#module-motobject_detectionpreprocessing","text":"View Source import numpy as np import cv2 def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : - *image*: A np.array of size [height, width, channels]. - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 - *scale_ratio*: The ratio used to scale back the boxes to the good shape \"\"\" image = image . astype ( np . float32 , copy = False ) im_size_min = np . min ( image . shape [ 0 : 2 ]) im_size_max = np . max ( image . shape [ 0 : 2 ]) scale_ratio = float ( min_dimension ) / float ( im_size_min ) # Prevent the biggest axis from being more than MAX_SIZE if np . round ( scale_ratio * im_size_max ) > max_dimension : scale_ratio = float ( max_dimension ) / float ( im_size_max ) resized_image = cv2 . resize ( image , None , None , fx = scale_ratio , fy = scale_ratio , interpolation = cv2 . INTER_LINEAR ) return resized_image , scale_ratio def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ): \"\"\"Adapt the preprocessing to the tensorpack Arguments: - *image*: A np.array of shape [height, width, channels] in BGR - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *input_signature*: A dictionary which match the server signature - *scaling_ratio*: A float representing the scaling to resize the image. \"\"\" resized_image , scale_ratio = resize_to_min_dimension ( image , min_dimension , max_dimension ) return { \"inputs\" : resized_image . tolist ()}, scale_ratio","title":"Module mot.object_detection.preprocessing"},{"location":"reference/mot/object_detection/preprocessing/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/preprocessing/#preprocess_for_serving","text":"def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ) Adapt the preprocessing to the tensorpack Arguments: image : A np.array of shape [height, width, channels] in BGR min_dimension : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: input_signature : A dictionary which match the server signature scaling_ratio : A float representing the scaling to resize the image. View Source def preprocess_for_serving ( image , min_dimension = 800 , max_dimension = 1300 ): \"\"\"Adapt the preprocessing to the tensorpack Arguments: - *image*: A np.array of shape [height, width, channels] in BGR - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *input_signature*: A dictionary which match the server signature - *scaling_ratio*: A float representing the scaling to resize the image. \"\"\" resized_image , scale_ratio = resize_to_min_dimension ( image , min_dimension , max_dimension ) return { \"inputs\" : resized_image . tolist () } , scale_ratio","title":"preprocess_for_serving"},{"location":"reference/mot/object_detection/preprocessing/#resize_to_min_dimension","text":"def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : image : A np.array of size [height, width, channels]. min_dimension : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: resized_image : The input image resized with the aspect_ratio preserved in float32 scale_ratio : The ratio used to scale back the boxes to the good shape View Source def resize_to_min_dimension ( image , min_dimension : int , max_dimension : int ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Otherwise, will keep the image size as is. Arguments : - *image*: A np.array of size [height, width, channels]. - *min_dimension*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 - *scale_ratio*: The ratio used to scale back the boxes to the good shape \"\"\" image = image . astype ( np . float32 , copy = False ) im_size_min = np . min ( image . shape [ 0 : 2 ]) im_size_max = np . max ( image . shape [ 0 : 2 ]) scale_ratio = float ( min_dimension ) / float ( im_size_min ) # Prevent the biggest axis from being more than MAX_SIZE if np . round ( scale_ratio * im_size_max ) > max_dimension : scale_ratio = float ( max_dimension ) / float ( im_size_max ) resized_image = cv2 . resize ( image , None , None , fx = scale_ratio , fy = scale_ratio , interpolation = cv2 . INTER_LINEAR ) return resized_image , scale_ratio","title":"resize_to_min_dimension"},{"location":"reference/mot/object_detection/query_server/","text":"Module mot.object_detection.query_server View Source import json import os from typing import Dict import numpy as np import requests from tensorpack import logger from mot.object_detection.preprocessing import preprocess_for_serving def query_tensorflow_server ( signature : Dict , url : str ) -> Dict : \"\"\"Will send a REST query to the tensorflow server. Arguments: - *signature*: A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: ```bash saved_model_cli show --dir directory_containing_the_saved_model.pb --all ``` - *url*: Where you can find the tensorflow server Returns: - *Dict*: A dict with the answer signature. \"\"\" url_serving = os . path . join ( url , \"v1/models/serving:predict\" ) headers = { \"content-type\" : \"application/json\" } json_response = requests . post ( url_serving , data = json . dumps ( signature ), headers = headers ) response = json_response . json () if \"outputs\" in response : return response [ \"outputs\" ] if \"error\" in response : message = \"Tensorflow serving returned an error. It probably means that your SavedModel \" \\ \" doesn't have the correct signature_def, which must be 'serving_default'. You can inspect \" \\ \" that by doing `saved_model_cli show --dir /path/to/your/model --all`. If the signature_def\" \\ \" isn't 'serving_default' you should export your checkpoints following the instructions\" \\ \" in the main README, section `Export` Here is the original\" \\ \" error raised by tensorflow serving: \\n \" + str ( response [ \"error\" ]) raise ValueError ( message ) raise ValueError ( \"Unknwon response from tensorflow serving: {} \" . format ( response )) def localizer_tensorflow_serving_inference ( image : np . ndarray , url : str , return_all_scores : bool = False , ) -> Dict : \"\"\"Preprocess and query the tensorflow serving for the localizer Arguments: - *image*: A numpy array loaded in BGR. - *url*: A string representing the url. - *return_all_scores*: Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: - *Dict*: A dict with the predictions with the following format: ```python if return_all_scores: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], (y1, x1, y2, x2) 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [ [0.001, 0.001, 0.98], [0.87, 0.05, 0.03], [0.1, 0.76, 0.1], ] # sorted in descending order of the prediction } else: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" signature , ratio = preprocess_for_serving ( image ) predictions = query_tensorflow_server ( signature , url ) scores = np . array ( predictions [ 'output/scores:0' ]) if len ( predictions [ \"output/boxes:0\" ]) > 0 : predictions [ 'output/boxes:0' ] = np . array ( predictions [ 'output/boxes:0' ], np . int32 ) / ratio predictions [ \"output/boxes:0\" ][:, 0 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 1 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 2 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 3 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ 'output/boxes:0' ] = predictions [ 'output/boxes:0' ] . tolist () if return_all_scores and len ( scores . shape ) == 1 : raise ValueError ( \"return_all_scores is True but the model you're using only returns the score of \" \"the predicted entity. Try changing the model you're using.\" ) if not return_all_scores and len ( scores . shape ) == 2 : scores = np . max ( scores , axis = 1 ) predictions [ 'output/scores:0' ] = scores . tolist () return predictions Functions localizer_tensorflow_serving_inference def localizer_tensorflow_serving_inference ( image : numpy . ndarray , url : str , return_all_scores : bool = False ) -> Dict Preprocess and query the tensorflow serving for the localizer Arguments: image : A numpy array loaded in BGR. url : A string representing the url. return_all_scores : Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: Dict : A dict with the predictions with the following format: if return_all_scores : predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], ( y1 , x1 , y2 , x2 ) 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ [ 0.001 , 0.001 , 0.98 ], [ 0.87 , 0.05 , 0.03 ], [ 0.1 , 0.76 , 0.1 ], ] # sorted in descending order of the prediction } else : predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ 0.98 , 0.87 , 0.76 ] # sorted in descending order } View Source def localizer_tensorflow_serving_inference ( image : np . ndarray , url : str , return_all_scores : bool = False , ) -> Dict : \"\"\"Preprocess and query the tensorflow serving for the localizer Arguments: - *image*: A numpy array loaded in BGR. - *url*: A string representing the url. - *return_all_scores*: Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: - *Dict*: A dict with the predictions with the following format: ```python if return_all_scores: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], (y1, x1, y2, x2) 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [ [0.001, 0.001, 0.98], [0.87, 0.05, 0.03], [0.1, 0.76, 0.1], ] # sorted in descending order of the prediction } else: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" signature , ratio = preprocess_for_serving ( image ) predictions = query_tensorflow_server ( signature , url ) scores = np . array ( predictions [ 'output/scores:0' ]) if len ( predictions [ \"output/boxes:0\" ]) > 0 : predictions [ 'output/boxes:0' ] = np . array ( predictions [ 'output/boxes:0' ], np . int32 ) / ratio predictions [ \"output/boxes:0\" ][:, 0 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 1 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 2 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 3 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ 'output/boxes:0' ] = predictions [ 'output/boxes:0' ] . tolist () if return_all_scores and len ( scores . shape ) == 1 : raise ValueError ( \"return_all_scores is True but the model you're using only returns the score of \" \"the predicted entity. Try changing the model you're using.\" ) if not return_all_scores and len ( scores . shape ) == 2 : scores = np . max ( scores , axis = 1 ) predictions [ 'output/scores:0' ] = scores . tolist () return predictions query_tensorflow_server def query_tensorflow_server ( signature : Dict , url : str ) -> Dict Will send a REST query to the tensorflow server. Arguments: signature : A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: saved_model_cli show --dir directory_containing_the_saved_model.pb --all url : Where you can find the tensorflow server Returns: Dict : A dict with the answer signature. View Source def query_tensorflow_server ( signature : Dict , url : str ) -> Dict : \" \"\" Will send a REST query to the tensorflow server. Arguments: - *signature*: A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: ```bash saved_model_cli show --dir directory_containing_the_saved_model.pb --all ``` - *url*: Where you can find the tensorflow server Returns: - *Dict*: A dict with the answer signature. \"\" \" url_serving = os . path . join ( url , \"v1/models/serving:predict\" ) headers = { \"content-type\" : \"application/json\" } json_response = requests . post ( url_serving , data = json . dumps ( signature ), headers = headers ) response = json_response . json () if \"outputs\" in response : return response [ \"outputs\" ] if \"error\" in response : message = \"Tensorflow serving returned an error. It probably means that your SavedModel \" \\ \" doesn't have the correct signature_def, which must be 'serving_default'. You can inspect \" \\ \" that by doing `saved_model_cli show --dir /path/to/your/model --all`. If the signature_def\" \\ \" isn't 'serving_default' you should export your checkpoints following the instructions\" \\ \" in the main README, section `Export` Here is the original\" \\ \" error raised by tensorflow serving: \\n \" + str ( response [ \"error\" ] ) raise ValueError ( message ) raise ValueError ( \"Unknwon response from tensorflow serving: {}\" . format ( response ))","title":"Query Server"},{"location":"reference/mot/object_detection/query_server/#module-motobject_detectionquery_server","text":"View Source import json import os from typing import Dict import numpy as np import requests from tensorpack import logger from mot.object_detection.preprocessing import preprocess_for_serving def query_tensorflow_server ( signature : Dict , url : str ) -> Dict : \"\"\"Will send a REST query to the tensorflow server. Arguments: - *signature*: A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: ```bash saved_model_cli show --dir directory_containing_the_saved_model.pb --all ``` - *url*: Where you can find the tensorflow server Returns: - *Dict*: A dict with the answer signature. \"\"\" url_serving = os . path . join ( url , \"v1/models/serving:predict\" ) headers = { \"content-type\" : \"application/json\" } json_response = requests . post ( url_serving , data = json . dumps ( signature ), headers = headers ) response = json_response . json () if \"outputs\" in response : return response [ \"outputs\" ] if \"error\" in response : message = \"Tensorflow serving returned an error. It probably means that your SavedModel \" \\ \" doesn't have the correct signature_def, which must be 'serving_default'. You can inspect \" \\ \" that by doing `saved_model_cli show --dir /path/to/your/model --all`. If the signature_def\" \\ \" isn't 'serving_default' you should export your checkpoints following the instructions\" \\ \" in the main README, section `Export` Here is the original\" \\ \" error raised by tensorflow serving: \\n \" + str ( response [ \"error\" ]) raise ValueError ( message ) raise ValueError ( \"Unknwon response from tensorflow serving: {} \" . format ( response )) def localizer_tensorflow_serving_inference ( image : np . ndarray , url : str , return_all_scores : bool = False , ) -> Dict : \"\"\"Preprocess and query the tensorflow serving for the localizer Arguments: - *image*: A numpy array loaded in BGR. - *url*: A string representing the url. - *return_all_scores*: Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: - *Dict*: A dict with the predictions with the following format: ```python if return_all_scores: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], (y1, x1, y2, x2) 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [ [0.001, 0.001, 0.98], [0.87, 0.05, 0.03], [0.1, 0.76, 0.1], ] # sorted in descending order of the prediction } else: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" signature , ratio = preprocess_for_serving ( image ) predictions = query_tensorflow_server ( signature , url ) scores = np . array ( predictions [ 'output/scores:0' ]) if len ( predictions [ \"output/boxes:0\" ]) > 0 : predictions [ 'output/boxes:0' ] = np . array ( predictions [ 'output/boxes:0' ], np . int32 ) / ratio predictions [ \"output/boxes:0\" ][:, 0 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 1 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 2 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 3 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ 'output/boxes:0' ] = predictions [ 'output/boxes:0' ] . tolist () if return_all_scores and len ( scores . shape ) == 1 : raise ValueError ( \"return_all_scores is True but the model you're using only returns the score of \" \"the predicted entity. Try changing the model you're using.\" ) if not return_all_scores and len ( scores . shape ) == 2 : scores = np . max ( scores , axis = 1 ) predictions [ 'output/scores:0' ] = scores . tolist () return predictions","title":"Module mot.object_detection.query_server"},{"location":"reference/mot/object_detection/query_server/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/query_server/#localizer_tensorflow_serving_inference","text":"def localizer_tensorflow_serving_inference ( image : numpy . ndarray , url : str , return_all_scores : bool = False ) -> Dict Preprocess and query the tensorflow serving for the localizer Arguments: image : A numpy array loaded in BGR. url : A string representing the url. return_all_scores : Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: Dict : A dict with the predictions with the following format: if return_all_scores : predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], ( y1 , x1 , y2 , x2 ) 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ [ 0.001 , 0.001 , 0.98 ], [ 0.87 , 0.05 , 0.03 ], [ 0.1 , 0.76 , 0.1 ], ] # sorted in descending order of the prediction } else : predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ 0.98 , 0.87 , 0.76 ] # sorted in descending order } View Source def localizer_tensorflow_serving_inference ( image : np . ndarray , url : str , return_all_scores : bool = False , ) -> Dict : \"\"\"Preprocess and query the tensorflow serving for the localizer Arguments: - *image*: A numpy array loaded in BGR. - *url*: A string representing the url. - *return_all_scores*: Wheter to return scores for all classes. The SavedModel you're querying must return all scores. Return: - *Dict*: A dict with the predictions with the following format: ```python if return_all_scores: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], (y1, x1, y2, x2) 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [ [0.001, 0.001, 0.98], [0.87, 0.05, 0.03], [0.1, 0.76, 0.1], ] # sorted in descending order of the prediction } else: predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" signature , ratio = preprocess_for_serving ( image ) predictions = query_tensorflow_server ( signature , url ) scores = np . array ( predictions [ 'output/scores:0' ]) if len ( predictions [ \"output/boxes:0\" ]) > 0 : predictions [ 'output/boxes:0' ] = np . array ( predictions [ 'output/boxes:0' ], np . int32 ) / ratio predictions [ \"output/boxes:0\" ][:, 0 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 1 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 2 ] /= image . shape [ 0 ] # scaling coords to [0, 1] predictions [ \"output/boxes:0\" ][:, 3 ] /= image . shape [ 1 ] # scaling coords to [0, 1] predictions [ 'output/boxes:0' ] = predictions [ 'output/boxes:0' ] . tolist () if return_all_scores and len ( scores . shape ) == 1 : raise ValueError ( \"return_all_scores is True but the model you're using only returns the score of \" \"the predicted entity. Try changing the model you're using.\" ) if not return_all_scores and len ( scores . shape ) == 2 : scores = np . max ( scores , axis = 1 ) predictions [ 'output/scores:0' ] = scores . tolist () return predictions","title":"localizer_tensorflow_serving_inference"},{"location":"reference/mot/object_detection/query_server/#query_tensorflow_server","text":"def query_tensorflow_server ( signature : Dict , url : str ) -> Dict Will send a REST query to the tensorflow server. Arguments: signature : A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: saved_model_cli show --dir directory_containing_the_saved_model.pb --all url : Where you can find the tensorflow server Returns: Dict : A dict with the answer signature. View Source def query_tensorflow_server ( signature : Dict , url : str ) -> Dict : \" \"\" Will send a REST query to the tensorflow server. Arguments: - *signature*: A dict with the signature required by your tensorflow server. If you have installed the tensorflow serving package you can inspect the signature as follow: ```bash saved_model_cli show --dir directory_containing_the_saved_model.pb --all ``` - *url*: Where you can find the tensorflow server Returns: - *Dict*: A dict with the answer signature. \"\" \" url_serving = os . path . join ( url , \"v1/models/serving:predict\" ) headers = { \"content-type\" : \"application/json\" } json_response = requests . post ( url_serving , data = json . dumps ( signature ), headers = headers ) response = json_response . json () if \"outputs\" in response : return response [ \"outputs\" ] if \"error\" in response : message = \"Tensorflow serving returned an error. It probably means that your SavedModel \" \\ \" doesn't have the correct signature_def, which must be 'serving_default'. You can inspect \" \\ \" that by doing `saved_model_cli show --dir /path/to/your/model --all`. If the signature_def\" \\ \" isn't 'serving_default' you should export your checkpoints following the instructions\" \\ \" in the main README, section `Export` Here is the original\" \\ \" error raised by tensorflow serving: \\n \" + str ( response [ \"error\" ] ) raise ValueError ( message ) raise ValueError ( \"Unknwon response from tensorflow serving: {}\" . format ( response ))","title":"query_tensorflow_server"},{"location":"reference/mot/object_detection/train/","text":"Module mot.object_detection.train View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 #!/usr/bin/env python # -*- coding: utf-8 -*- # File: train.py import argparse import six assert six . PY3 , \"This example requires Python 3!\" from tensorpack import * from tensorpack.tfutils import collect_env_info from tensorpack.tfutils.common import get_tf_version_tuple from mot.object_detection.dataset import register_mot from mot.object_detection.config import config as cfg from mot.object_detection.config import finalize_configs from mot.object_detection.data import get_train_dataflow from mot.object_detection.eval import EvalCallback from mot.object_detection.modeling.generalized_rcnn import ResNetC4Model , ResNetFPNModel try : import horovod.tensorflow as hvd except ImportError : pass if __name__ == '__main__' : # \"spawn/forkserver\" is safer than the default \"fork\" method and # produce more deterministic behavior & memory saving # However its limitation is you cannot pass a lambda function to subprocesses. import multiprocessing as mp mp . set_start_method ( 'spawn' ) parser = argparse . ArgumentParser () parser . add_argument ( '--load' , help = 'Load a model to start training from. It overwrites BACKBONE.WEIGHTS' ) parser . add_argument ( '--logdir' , help = 'Log directory. Will remove the old one if already exists.' , default = 'train_log/maskrcnn' ) parser . add_argument ( '--config' , help = \"A list of KEY=VALUE to overwrite those defined in config.py\" , nargs = '+' ) if get_tf_version_tuple () < ( 1 , 6 ): # https://github.com/tensorflow/tensorflow/issues/14657 logger . warn ( \"TF<1.6 has a bug which may lead to crash in FasterRCNN if you're unlucky.\" ) args = parser . parse_args () if args . config : cfg . update_args ( args . config ) register_mot ( cfg . DATA . BASEDIR ) # add the mot datasets to the registry # Setup logging ... is_horovod = cfg . TRAINER == 'horovod' if is_horovod : hvd . init () if not is_horovod or hvd . rank () == 0 : logger . set_logger_dir ( args . logdir , 'd' ) logger . info ( \"Environment Information: \\n \" + collect_env_info ()) finalize_configs ( is_training = True ) # Create model MODEL = ResNetFPNModel () if cfg . MODE_FPN else ResNetC4Model () # Compute the training schedule from the number of GPUs ... stepnum = cfg . TRAIN . STEPS_PER_EPOCH # warmup is step based, lr is epoch based init_lr = cfg . TRAIN . WARMUP_INIT_LR * min ( 8. / cfg . TRAIN . NUM_GPUS , 1. ) warmup_schedule = [( 0 , init_lr ), ( cfg . TRAIN . WARMUP , cfg . TRAIN . BASE_LR )] warmup_end_epoch = cfg . TRAIN . WARMUP * 1. / stepnum lr_schedule = [( int ( warmup_end_epoch + 0.5 ), cfg . TRAIN . BASE_LR )] factor = 8. / cfg . TRAIN . NUM_GPUS for idx , steps in enumerate ( cfg . TRAIN . LR_SCHEDULE [: - 1 ]): mult = 0.1 ** ( idx + 1 ) lr_schedule . append ( ( steps * factor // stepnum , cfg . TRAIN . BASE_LR * mult )) logger . info ( \"Warm Up Schedule (steps, value): \" + str ( warmup_schedule )) logger . info ( \"LR Schedule (epochs, value): \" + str ( lr_schedule )) train_dataflow = get_train_dataflow () # This is what's commonly referred to as \"epochs\" total_passes = cfg . TRAIN . LR_SCHEDULE [ - 1 ] * 8 / train_dataflow . size () logger . info ( \"Total passes of the training set is: {:.5g} \" . format ( total_passes )) # Create callbacks ... callbacks = [ PeriodicCallback ( ModelSaver ( max_to_keep = 10 , keep_checkpoint_every_n_hours = 1 ), every_k_epochs = cfg . TRAIN . CHECKPOINT_PERIOD ), # linear warmup ScheduledHyperParamSetter ( 'learning_rate' , warmup_schedule , interp = 'linear' , step_based = True ), ScheduledHyperParamSetter ( 'learning_rate' , lr_schedule ), GPUMemoryTracker (), HostMemoryTracker (), ThroughputTracker ( samples_per_step = cfg . TRAIN . NUM_GPUS ), EstimatedTimeLeft ( median = True ), SessionRunTimeout ( 60000 ), # 1 minute timeout GPUUtilizationTracker () ] if cfg . TRAIN . EVAL_PERIOD > 0 : callbacks . extend ([ EvalCallback ( dataset , * MODEL . get_inference_tensor_names (), args . logdir ) for dataset in cfg . DATA . VAL ]) if is_horovod and hvd . rank () > 0 : session_init = None else : if args . load : # ignore mismatched values, so you can `--load` a model for fine-tuning session_init = SmartInit ( args . load , ignore_mismatch = True ) else : session_init = SmartInit ( cfg . BACKBONE . WEIGHTS ) traincfg = TrainConfig ( model = MODEL , data = QueueInput ( train_dataflow ), callbacks = callbacks , steps_per_epoch = stepnum , max_epoch = cfg . TRAIN . LR_SCHEDULE [ - 1 ] * factor // stepnum , session_init = session_init , starting_epoch = cfg . TRAIN . STARTING_EPOCH ) if is_horovod : trainer = HorovodTrainer ( average = False ) else : # nccl mode appears faster than cpu mode trainer = SyncMultiGPUTrainerReplicated ( cfg . TRAIN . NUM_GPUS , average = False , mode = 'nccl' ) launch_train_with_config ( traincfg , trainer ) Variables STATICA_HACK","title":"Train"},{"location":"reference/mot/object_detection/train/#module-motobject_detectiontrain","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 #!/usr/bin/env python # -*- coding: utf-8 -*- # File: train.py import argparse import six assert six . PY3 , \"This example requires Python 3!\" from tensorpack import * from tensorpack.tfutils import collect_env_info from tensorpack.tfutils.common import get_tf_version_tuple from mot.object_detection.dataset import register_mot from mot.object_detection.config import config as cfg from mot.object_detection.config import finalize_configs from mot.object_detection.data import get_train_dataflow from mot.object_detection.eval import EvalCallback from mot.object_detection.modeling.generalized_rcnn import ResNetC4Model , ResNetFPNModel try : import horovod.tensorflow as hvd except ImportError : pass if __name__ == '__main__' : # \"spawn/forkserver\" is safer than the default \"fork\" method and # produce more deterministic behavior & memory saving # However its limitation is you cannot pass a lambda function to subprocesses. import multiprocessing as mp mp . set_start_method ( 'spawn' ) parser = argparse . ArgumentParser () parser . add_argument ( '--load' , help = 'Load a model to start training from. It overwrites BACKBONE.WEIGHTS' ) parser . add_argument ( '--logdir' , help = 'Log directory. Will remove the old one if already exists.' , default = 'train_log/maskrcnn' ) parser . add_argument ( '--config' , help = \"A list of KEY=VALUE to overwrite those defined in config.py\" , nargs = '+' ) if get_tf_version_tuple () < ( 1 , 6 ): # https://github.com/tensorflow/tensorflow/issues/14657 logger . warn ( \"TF<1.6 has a bug which may lead to crash in FasterRCNN if you're unlucky.\" ) args = parser . parse_args () if args . config : cfg . update_args ( args . config ) register_mot ( cfg . DATA . BASEDIR ) # add the mot datasets to the registry # Setup logging ... is_horovod = cfg . TRAINER == 'horovod' if is_horovod : hvd . init () if not is_horovod or hvd . rank () == 0 : logger . set_logger_dir ( args . logdir , 'd' ) logger . info ( \"Environment Information: \\n \" + collect_env_info ()) finalize_configs ( is_training = True ) # Create model MODEL = ResNetFPNModel () if cfg . MODE_FPN else ResNetC4Model () # Compute the training schedule from the number of GPUs ... stepnum = cfg . TRAIN . STEPS_PER_EPOCH # warmup is step based, lr is epoch based init_lr = cfg . TRAIN . WARMUP_INIT_LR * min ( 8. / cfg . TRAIN . NUM_GPUS , 1. ) warmup_schedule = [( 0 , init_lr ), ( cfg . TRAIN . WARMUP , cfg . TRAIN . BASE_LR )] warmup_end_epoch = cfg . TRAIN . WARMUP * 1. / stepnum lr_schedule = [( int ( warmup_end_epoch + 0.5 ), cfg . TRAIN . BASE_LR )] factor = 8. / cfg . TRAIN . NUM_GPUS for idx , steps in enumerate ( cfg . TRAIN . LR_SCHEDULE [: - 1 ]): mult = 0.1 ** ( idx + 1 ) lr_schedule . append ( ( steps * factor // stepnum , cfg . TRAIN . BASE_LR * mult )) logger . info ( \"Warm Up Schedule (steps, value): \" + str ( warmup_schedule )) logger . info ( \"LR Schedule (epochs, value): \" + str ( lr_schedule )) train_dataflow = get_train_dataflow () # This is what's commonly referred to as \"epochs\" total_passes = cfg . TRAIN . LR_SCHEDULE [ - 1 ] * 8 / train_dataflow . size () logger . info ( \"Total passes of the training set is: {:.5g} \" . format ( total_passes )) # Create callbacks ... callbacks = [ PeriodicCallback ( ModelSaver ( max_to_keep = 10 , keep_checkpoint_every_n_hours = 1 ), every_k_epochs = cfg . TRAIN . CHECKPOINT_PERIOD ), # linear warmup ScheduledHyperParamSetter ( 'learning_rate' , warmup_schedule , interp = 'linear' , step_based = True ), ScheduledHyperParamSetter ( 'learning_rate' , lr_schedule ), GPUMemoryTracker (), HostMemoryTracker (), ThroughputTracker ( samples_per_step = cfg . TRAIN . NUM_GPUS ), EstimatedTimeLeft ( median = True ), SessionRunTimeout ( 60000 ), # 1 minute timeout GPUUtilizationTracker () ] if cfg . TRAIN . EVAL_PERIOD > 0 : callbacks . extend ([ EvalCallback ( dataset , * MODEL . get_inference_tensor_names (), args . logdir ) for dataset in cfg . DATA . VAL ]) if is_horovod and hvd . rank () > 0 : session_init = None else : if args . load : # ignore mismatched values, so you can `--load` a model for fine-tuning session_init = SmartInit ( args . load , ignore_mismatch = True ) else : session_init = SmartInit ( cfg . BACKBONE . WEIGHTS ) traincfg = TrainConfig ( model = MODEL , data = QueueInput ( train_dataflow ), callbacks = callbacks , steps_per_epoch = stepnum , max_epoch = cfg . TRAIN . LR_SCHEDULE [ - 1 ] * factor // stepnum , session_init = session_init , starting_epoch = cfg . TRAIN . STARTING_EPOCH ) if is_horovod : trainer = HorovodTrainer ( average = False ) else : # nccl mode appears faster than cpu mode trainer = SyncMultiGPUTrainerReplicated ( cfg . TRAIN . NUM_GPUS , average = False , mode = 'nccl' ) launch_train_with_config ( traincfg , trainer )","title":"Module mot.object_detection.train"},{"location":"reference/mot/object_detection/train/#variables","text":"STATICA_HACK","title":"Variables"},{"location":"reference/mot/object_detection/viz/","text":"Module mot.object_detection.viz View Source # -*- coding: utf-8 -*- # File: viz.py import numpy as np from six.moves import zip from tensorpack.utils import viz from tensorpack.utils.palette import PALETTE_RGB from mot.object_detection.config import config as cfg from mot.object_detection.utils.np_box_ops import area as np_area from mot.object_detection.utils.np_box_ops import iou as np_iou from mot.object_detection.common import polygons_to_mask def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ): \"\"\"Will not modify img\"\"\" labels = [] assert len ( boxes ) == len ( klass ) if is_crowd is not None : assert len ( boxes ) == len ( is_crowd ) for cls , crd in zip ( klass , is_crowd ): clsname = cfg . DATA . CLASS_NAMES [ cls ] if crd == 1 : clsname += ';Crowd' labels . append ( clsname ) else : for cls in klass : labels . append ( cfg . DATA . CLASS_NAMES [ cls ]) img = viz . draw_boxes ( img , boxes , labels ) if polygons is not None : for p in polygons : mask = polygons_to_mask ( p , img . shape [ 0 ], img . shape [ 1 ]) img = draw_mask ( img , mask ) return img def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ): \"\"\" Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG \"\"\" box_ious = np_iou ( gt_boxes , proposals ) # ng x np box_ious_argsort = np . argsort ( - box_ious , axis = 1 ) good_proposals_ind = box_ious_argsort [:, : 3 ] # for each gt, find 3 best proposals good_proposals_ind = np . unique ( good_proposals_ind . ravel ()) proposals = proposals [ good_proposals_ind , :] tags = list ( map ( str , proposal_scores [ good_proposals_ind ])) img = viz . draw_boxes ( img , proposals , tags ) return img , good_proposals_ind def draw_predictions ( img , boxes , scores ): \"\"\" Args: boxes: kx4 scores: kxC \"\"\" if len ( boxes ) == 0 : return img labels = scores . argmax ( axis = 1 ) scores = scores . max ( axis = 1 ) tags = [ \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ lb ], score ) for lb , score in zip ( labels , scores )] return viz . draw_boxes ( img , boxes , tags ) def draw_final_outputs ( img , results ): \"\"\" Args: results: [DetectionResult] \"\"\" if len ( results ) == 0 : return img # Display in largest to smallest order to reduce occlusion boxes = np . asarray ([ r . box for r in results ]) areas = np_area ( boxes ) sorted_inds = np . argsort ( - areas ) ret = img tags = [] for result_id in sorted_inds : r = results [ result_id ] if r . mask is not None : ret = draw_mask ( ret , r . mask ) for r in results : tags . append ( \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ r . class_id ], r . score )) ret = viz . draw_boxes ( ret , boxes , tags ) return ret def draw_final_outputs_blackwhite ( img , results ): \"\"\" Args: results: [DetectionResult] \"\"\" img_bw = img . mean ( axis = 2 ) img_bw = np . stack ([ img_bw ] * 3 , axis = 2 ) if len ( results ) == 0 : return img_bw boxes = np . asarray ([ r . box for r in results ]) all_masks = [ r . mask for r in results ] if all_masks [ 0 ] is not None : m = all_masks [ 0 ] > 0 for m2 in all_masks [ 1 :]: m = m | ( m2 > 0 ) img_bw [ m ] = img [ m ] tags = [ \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ r . class_id ], r . score ) for r in results ] ret = viz . draw_boxes ( img_bw , boxes , tags ) return ret def draw_mask ( im , mask , alpha = 0.5 , color = None ): \"\"\" Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically \"\"\" if color is None : color = PALETTE_RGB [ np . random . choice ( len ( PALETTE_RGB ))][:: - 1 ] color = np . asarray ( color , dtype = np . float32 ) im = np . where ( np . repeat (( mask > 0 )[:, :, None ], 3 , axis = 2 ), im * ( 1 - alpha ) + color * alpha , im ) im = im . astype ( 'uint8' ) return im Variables PALETTE_RGB Functions draw_annotation def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ) Will not modify img View Source def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ) : \"\"\"Will not modify img\"\"\" labels = [] assert len ( boxes ) == len ( klass ) if is_crowd is not None : assert len ( boxes ) == len ( is_crowd ) for cls , crd in zip ( klass , is_crowd ) : clsname = cfg . DATA . CLASS_NAMES [ cls ] if crd == 1 : clsname += ';Crowd' labels . append ( clsname ) else : for cls in klass : labels . append ( cfg . DATA . CLASS_NAMES [ cls ] ) img = viz . draw_boxes ( img , boxes , labels ) if polygons is not None : for p in polygons : mask = polygons_to_mask ( p , img . shape [ 0 ] , img . shape [ 1 ] ) img = draw_mask ( img , mask ) return img draw_final_outputs def draw_final_outputs ( img , results ) Args: results: [DetectionResult] View Source def draw_final_outputs ( img , results ) : \"\"\" Args: results: [DetectionResult] \"\"\" if len ( results ) == 0 : return img # Display in largest to smallest order to reduce occlusion boxes = np . asarray ( [ r.box for r in results ] ) areas = np_area ( boxes ) sorted_inds = np . argsort ( - areas ) ret = img tags = [] for result_id in sorted_inds : r = results [ result_id ] if r . mask is not None : ret = draw_mask ( ret , r . mask ) for r in results : tags . append ( \"{},{:.2f}\" . format ( cfg . DATA . CLASS_NAMES [ r.class_id ] , r . score )) ret = viz . draw_boxes ( ret , boxes , tags ) return ret draw_final_outputs_blackwhite def draw_final_outputs_blackwhite ( img , results ) Args: results: [DetectionResult] View Source def draw_final_outputs_blackwhite ( img , results ) : \"\"\" Args: results: [DetectionResult] \"\"\" img_bw = img . mean ( axis = 2 ) img_bw = np . stack ( [ img_bw ] * 3 , axis = 2 ) if len ( results ) == 0 : return img_bw boxes = np . asarray ( [ r.box for r in results ] ) all_masks = [ r.mask for r in results ] if all_masks [ 0 ] is not None : m = all_masks [ 0 ] > 0 for m2 in all_masks [ 1: ] : m = m | ( m2 > 0 ) img_bw [ m ] = img [ m ] tags = [ \"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[r.class_id ] , r . score ) for r in results ] ret = viz . draw_boxes ( img_bw , boxes , tags ) return ret draw_mask def draw_mask ( im , mask , alpha = 0.5 , color = None ) Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically View Source def draw_mask ( im , mask , alpha = 0.5 , color = None ) : \"\"\" Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically \"\"\" if color is None : color = PALETTE_RGB [ np . random . choice ( len ( PALETTE_RGB ))][ ::- 1 ] color = np . asarray ( color , dtype = np . float32 ) im = np . where ( np . repeat (( mask > 0 )[ : , : , None ], 3 , axis = 2 ), im * ( 1 - alpha ) + color * alpha , im ) im = im . astype ( 'uint8' ) return im draw_predictions def draw_predictions ( img , boxes , scores ) Args: boxes: kx4 scores: kxC View Source def draw_predictions ( img , boxes , scores ) : \"\"\" Args: boxes: kx4 scores: kxC \"\"\" if len ( boxes ) == 0 : return img labels = scores . argmax ( axis = 1 ) scores = scores . max ( axis = 1 ) tags = [ \"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[lb ] , score ) for lb , score in zip ( labels , scores ) ] return viz . draw_boxes ( img , boxes , tags ) draw_proposal_recall def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ) Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG View Source def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ) : \"\"\" Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG \"\"\" box_ious = np_iou ( gt_boxes , proposals ) # ng x np box_ious_argsort = np . argsort ( - box_ious , axis = 1 ) good_proposals_ind = box_ious_argsort [ :, :3 ] # for each gt , find 3 best proposals good_proposals_ind = np . unique ( good_proposals_ind . ravel ()) proposals = proposals [ good_proposals_ind, : ] tags = list ( map ( str , proposal_scores [ good_proposals_ind ] )) img = viz . draw_boxes ( img , proposals , tags ) return img , good_proposals_ind","title":"Viz"},{"location":"reference/mot/object_detection/viz/#module-motobject_detectionviz","text":"View Source # -*- coding: utf-8 -*- # File: viz.py import numpy as np from six.moves import zip from tensorpack.utils import viz from tensorpack.utils.palette import PALETTE_RGB from mot.object_detection.config import config as cfg from mot.object_detection.utils.np_box_ops import area as np_area from mot.object_detection.utils.np_box_ops import iou as np_iou from mot.object_detection.common import polygons_to_mask def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ): \"\"\"Will not modify img\"\"\" labels = [] assert len ( boxes ) == len ( klass ) if is_crowd is not None : assert len ( boxes ) == len ( is_crowd ) for cls , crd in zip ( klass , is_crowd ): clsname = cfg . DATA . CLASS_NAMES [ cls ] if crd == 1 : clsname += ';Crowd' labels . append ( clsname ) else : for cls in klass : labels . append ( cfg . DATA . CLASS_NAMES [ cls ]) img = viz . draw_boxes ( img , boxes , labels ) if polygons is not None : for p in polygons : mask = polygons_to_mask ( p , img . shape [ 0 ], img . shape [ 1 ]) img = draw_mask ( img , mask ) return img def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ): \"\"\" Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG \"\"\" box_ious = np_iou ( gt_boxes , proposals ) # ng x np box_ious_argsort = np . argsort ( - box_ious , axis = 1 ) good_proposals_ind = box_ious_argsort [:, : 3 ] # for each gt, find 3 best proposals good_proposals_ind = np . unique ( good_proposals_ind . ravel ()) proposals = proposals [ good_proposals_ind , :] tags = list ( map ( str , proposal_scores [ good_proposals_ind ])) img = viz . draw_boxes ( img , proposals , tags ) return img , good_proposals_ind def draw_predictions ( img , boxes , scores ): \"\"\" Args: boxes: kx4 scores: kxC \"\"\" if len ( boxes ) == 0 : return img labels = scores . argmax ( axis = 1 ) scores = scores . max ( axis = 1 ) tags = [ \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ lb ], score ) for lb , score in zip ( labels , scores )] return viz . draw_boxes ( img , boxes , tags ) def draw_final_outputs ( img , results ): \"\"\" Args: results: [DetectionResult] \"\"\" if len ( results ) == 0 : return img # Display in largest to smallest order to reduce occlusion boxes = np . asarray ([ r . box for r in results ]) areas = np_area ( boxes ) sorted_inds = np . argsort ( - areas ) ret = img tags = [] for result_id in sorted_inds : r = results [ result_id ] if r . mask is not None : ret = draw_mask ( ret , r . mask ) for r in results : tags . append ( \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ r . class_id ], r . score )) ret = viz . draw_boxes ( ret , boxes , tags ) return ret def draw_final_outputs_blackwhite ( img , results ): \"\"\" Args: results: [DetectionResult] \"\"\" img_bw = img . mean ( axis = 2 ) img_bw = np . stack ([ img_bw ] * 3 , axis = 2 ) if len ( results ) == 0 : return img_bw boxes = np . asarray ([ r . box for r in results ]) all_masks = [ r . mask for r in results ] if all_masks [ 0 ] is not None : m = all_masks [ 0 ] > 0 for m2 in all_masks [ 1 :]: m = m | ( m2 > 0 ) img_bw [ m ] = img [ m ] tags = [ \" {} , {:.2f} \" . format ( cfg . DATA . CLASS_NAMES [ r . class_id ], r . score ) for r in results ] ret = viz . draw_boxes ( img_bw , boxes , tags ) return ret def draw_mask ( im , mask , alpha = 0.5 , color = None ): \"\"\" Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically \"\"\" if color is None : color = PALETTE_RGB [ np . random . choice ( len ( PALETTE_RGB ))][:: - 1 ] color = np . asarray ( color , dtype = np . float32 ) im = np . where ( np . repeat (( mask > 0 )[:, :, None ], 3 , axis = 2 ), im * ( 1 - alpha ) + color * alpha , im ) im = im . astype ( 'uint8' ) return im","title":"Module mot.object_detection.viz"},{"location":"reference/mot/object_detection/viz/#variables","text":"PALETTE_RGB","title":"Variables"},{"location":"reference/mot/object_detection/viz/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/viz/#draw_annotation","text":"def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ) Will not modify img View Source def draw_annotation ( img , boxes , klass , polygons = None , is_crowd = None ) : \"\"\"Will not modify img\"\"\" labels = [] assert len ( boxes ) == len ( klass ) if is_crowd is not None : assert len ( boxes ) == len ( is_crowd ) for cls , crd in zip ( klass , is_crowd ) : clsname = cfg . DATA . CLASS_NAMES [ cls ] if crd == 1 : clsname += ';Crowd' labels . append ( clsname ) else : for cls in klass : labels . append ( cfg . DATA . CLASS_NAMES [ cls ] ) img = viz . draw_boxes ( img , boxes , labels ) if polygons is not None : for p in polygons : mask = polygons_to_mask ( p , img . shape [ 0 ] , img . shape [ 1 ] ) img = draw_mask ( img , mask ) return img","title":"draw_annotation"},{"location":"reference/mot/object_detection/viz/#draw_final_outputs","text":"def draw_final_outputs ( img , results ) Args: results: [DetectionResult] View Source def draw_final_outputs ( img , results ) : \"\"\" Args: results: [DetectionResult] \"\"\" if len ( results ) == 0 : return img # Display in largest to smallest order to reduce occlusion boxes = np . asarray ( [ r.box for r in results ] ) areas = np_area ( boxes ) sorted_inds = np . argsort ( - areas ) ret = img tags = [] for result_id in sorted_inds : r = results [ result_id ] if r . mask is not None : ret = draw_mask ( ret , r . mask ) for r in results : tags . append ( \"{},{:.2f}\" . format ( cfg . DATA . CLASS_NAMES [ r.class_id ] , r . score )) ret = viz . draw_boxes ( ret , boxes , tags ) return ret","title":"draw_final_outputs"},{"location":"reference/mot/object_detection/viz/#draw_final_outputs_blackwhite","text":"def draw_final_outputs_blackwhite ( img , results ) Args: results: [DetectionResult] View Source def draw_final_outputs_blackwhite ( img , results ) : \"\"\" Args: results: [DetectionResult] \"\"\" img_bw = img . mean ( axis = 2 ) img_bw = np . stack ( [ img_bw ] * 3 , axis = 2 ) if len ( results ) == 0 : return img_bw boxes = np . asarray ( [ r.box for r in results ] ) all_masks = [ r.mask for r in results ] if all_masks [ 0 ] is not None : m = all_masks [ 0 ] > 0 for m2 in all_masks [ 1: ] : m = m | ( m2 > 0 ) img_bw [ m ] = img [ m ] tags = [ \"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[r.class_id ] , r . score ) for r in results ] ret = viz . draw_boxes ( img_bw , boxes , tags ) return ret","title":"draw_final_outputs_blackwhite"},{"location":"reference/mot/object_detection/viz/#draw_mask","text":"def draw_mask ( im , mask , alpha = 0.5 , color = None ) Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically View Source def draw_mask ( im , mask , alpha = 0.5 , color = None ) : \"\"\" Overlay a mask on top of the image. Args: im: a 3-channel uint8 image in BGR mask: a binary 1-channel image of the same size color: if None, will choose automatically \"\"\" if color is None : color = PALETTE_RGB [ np . random . choice ( len ( PALETTE_RGB ))][ ::- 1 ] color = np . asarray ( color , dtype = np . float32 ) im = np . where ( np . repeat (( mask > 0 )[ : , : , None ], 3 , axis = 2 ), im * ( 1 - alpha ) + color * alpha , im ) im = im . astype ( 'uint8' ) return im","title":"draw_mask"},{"location":"reference/mot/object_detection/viz/#draw_predictions","text":"def draw_predictions ( img , boxes , scores ) Args: boxes: kx4 scores: kxC View Source def draw_predictions ( img , boxes , scores ) : \"\"\" Args: boxes: kx4 scores: kxC \"\"\" if len ( boxes ) == 0 : return img labels = scores . argmax ( axis = 1 ) scores = scores . max ( axis = 1 ) tags = [ \"{},{:.2f}\".format(cfg.DATA.CLASS_NAMES[lb ] , score ) for lb , score in zip ( labels , scores ) ] return viz . draw_boxes ( img , boxes , tags )","title":"draw_predictions"},{"location":"reference/mot/object_detection/viz/#draw_proposal_recall","text":"def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ) Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG View Source def draw_proposal_recall ( img , proposals , proposal_scores , gt_boxes ) : \"\"\" Draw top3 proposals for each gt. Args: proposals: NPx4 proposal_scores: NP gt_boxes: NG \"\"\" box_ious = np_iou ( gt_boxes , proposals ) # ng x np box_ious_argsort = np . argsort ( - box_ious , axis = 1 ) good_proposals_ind = box_ious_argsort [ :, :3 ] # for each gt , find 3 best proposals good_proposals_ind = np . unique ( good_proposals_ind . ravel ()) proposals = proposals [ good_proposals_ind, : ] tags = list ( map ( str , proposal_scores [ good_proposals_ind ] )) img = viz . draw_boxes ( img , proposals , tags ) return img , good_proposals_ind","title":"draw_proposal_recall"},{"location":"reference/mot/object_detection/dataset/","text":"Module mot.object_detection.dataset View Source from mot.object_detection.dataset.dataset import * from mot.object_detection.dataset.mot import * Sub-modules mot.object_detection.dataset.dataset mot.object_detection.dataset.mot","title":"Index"},{"location":"reference/mot/object_detection/dataset/#module-motobject_detectiondataset","text":"View Source from mot.object_detection.dataset.dataset import * from mot.object_detection.dataset.mot import *","title":"Module mot.object_detection.dataset"},{"location":"reference/mot/object_detection/dataset/#sub-modules","text":"mot.object_detection.dataset.dataset mot.object_detection.dataset.mot","title":"Sub-modules"},{"location":"reference/mot/object_detection/dataset/dataset/","text":"Module mot.object_detection.dataset.dataset View Source # -*- coding: utf-8 -*- from collections import defaultdict from typing import List class DatasetSplit (): \"\"\"A class to load datasets, evaluate results for a datast split (e.g., \"coco_train_2017\") To use your own dataset that's not in COCO format, write a subclass that implements the interfaces. \"\"\" def training_roidbs ( self ): \"\"\" Returns: - *(list[dict])*: Produce \"roidbs\" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: - file_name: str, full path to the image - boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] - class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) - is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. - segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. \"\"\" raise NotImplementedError () def inference_roidbs ( self ): \"\"\" Returns: - *(list[dict])*: Each dict corresponds to one image to run inference on. The following keys in the dict are expected: - file_name (str): full path to the image - image_id (str): an id for the image. The inference results will be stored with this id. \"\"\" raise NotImplementedError () def eval_inference_results ( self , results : List [ dict ], output : str = None ): \"\"\" Arguments: - *results*: (list[dict]): the inference results as dicts. Each dict corresponds to one __instance__. It contains the following keys: - image_id (str): the id that matches `inference_roidbs`. - category_id (int): the category prediction, in range [1, #category] - bbox (list[float]): x1, y1, x2, y2 - score (float): - segmentation: the segmentation mask in COCO's rle format. - *output*: (str): the output file or directory to optionally save the results to. Returns: - dict: the evaluation results. \"\"\" raise NotImplementedError () class DatasetRegistry (): _registry = {} _metadata_registry = defaultdict ( dict ) @staticmethod def register ( name , func ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *func*: a function which returns an instance of `DatasetSplit` \"\"\" assert name not in DatasetRegistry . _registry , \"Dataset {} was registered already!\" . format ( name ) DatasetRegistry . _registry [ name ] = func @staticmethod def get ( name ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" Returns: - *DatasetSplit* \"\"\" assert name in DatasetRegistry . _registry , \"Dataset {} was not registered!\" . format ( name ) return DatasetRegistry . _registry [ name ]() @staticmethod def register_metadata ( name , key , value ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *key*: the key of the metadata, e.g., \"class_names\" - *value*: the value of the metadata \"\"\" DatasetRegistry . _metadata_registry [ name ][ key ] = value @staticmethod def get_metadata ( name , key ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *key*: the key of the metadata, e.g., \"class_names\" Returns: - *value* \"\"\" return DatasetRegistry . _metadata_registry [ name ][ key ] Classes DatasetRegistry class DatasetRegistry ( / , * args , ** kwargs ) Static methods get def get ( name ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" Returns: DatasetSplit View Source @staticmethod def get ( name ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" Returns: - *DatasetSplit* \"\"\" assert name in DatasetRegistry . _registry , \"Dataset {} was not registered!\" . format ( name ) return DatasetRegistry . _registry [ name ] () get_metadata def get_metadata ( name , key ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" key : the key of the metadata, e.g., \"class_names\" Returns: value View Source @staticmethod def get_metadata ( name , key ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *key*: the key of the metadata, e.g., \" class_names \" Returns: - *value* \"\"\" return DatasetRegistry . _metadata_registry [ name ][ key ] register def register ( name , func ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" func : a function which returns an instance of DatasetSplit View Source @staticmethod def register ( name , func ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *func*: a function which returns an instance of `DatasetSplit` \"\"\" assert name not in DatasetRegistry . _registry , \"Dataset {} was registered already!\" . format ( name ) DatasetRegistry . _registry [ name ] = func register_metadata def register_metadata ( name , key , value ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" key : the key of the metadata, e.g., \"class_names\" value : the value of the metadata View Source @staticmethod def register_metadata ( name , key , value ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *key*: the key of the metadata, e.g., \" class_names \" - *value*: the value of the metadata \"\"\" DatasetRegistry . _metadata_registry [ name ][ key ] = value DatasetSplit class DatasetSplit ( / , * args , ** kwargs ) A class to load datasets, evaluate results for a datast split (e.g., \"coco_train_2017\") To use your own dataset that's not in COCO format, write a subclass that implements the interfaces. Methods eval_inference_results def eval_inference_results ( self , results : List [ dict ], output : str = None ) Arguments: results : (list[dict]): the inference results as dicts. Each dict corresponds to one instance . It contains the following keys: image_id (str): the id that matches inference_roidbs . category_id (int): the category prediction, in range [1, #category] bbox (list[float]): x1, y1, x2, y2 score (float): segmentation: the segmentation mask in COCO's rle format. output : (str): the output file or directory to optionally save the results to. Returns: dict: the evaluation results. View Source def eval_inference_results ( self , results : List [ dict ] , output : str = None ) : \"\"\" Arguments: - *results*: (list[dict]): the inference results as dicts. Each dict corresponds to one __instance__. It contains the following keys: - image_id (str): the id that matches `inference_roidbs`. - category_id (int): the category prediction, in range [1, #category] - bbox (list[float]): x1, y1, x2, y2 - score (float): - segmentation: the segmentation mask in COCO's rle format. - *output*: (str): the output file or directory to optionally save the results to. Returns: - dict: the evaluation results. \"\"\" raise NotImplementedError () inference_roidbs def inference_roidbs ( self ) Returns: (list[dict]) : Each dict corresponds to one image to run inference on. The following keys in the dict are expected: file_name (str): full path to the image image_id (str): an id for the image. The inference results will be stored with this id. View Source def inference_roidbs ( self ) : \"\"\" Returns: - *(list[dict])*: Each dict corresponds to one image to run inference on. The following keys in the dict are expected: - file_name (str): full path to the image - image_id (str): an id for the image. The inference results will be stored with this id. \"\"\" raise NotImplementedError () training_roidbs def training_roidbs ( self ) Returns: (list[dict]) : Produce \"roidbs\" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: file_name: str, full path to the image boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. View Source def training_roidbs ( self ) : \"\"\" Returns: - *(list[dict])*: Produce \" roidbs \" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: - file_name: str, full path to the image - boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] - class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) - is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. - segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. \"\"\" raise NotImplementedError ()","title":"Dataset"},{"location":"reference/mot/object_detection/dataset/dataset/#module-motobject_detectiondatasetdataset","text":"View Source # -*- coding: utf-8 -*- from collections import defaultdict from typing import List class DatasetSplit (): \"\"\"A class to load datasets, evaluate results for a datast split (e.g., \"coco_train_2017\") To use your own dataset that's not in COCO format, write a subclass that implements the interfaces. \"\"\" def training_roidbs ( self ): \"\"\" Returns: - *(list[dict])*: Produce \"roidbs\" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: - file_name: str, full path to the image - boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] - class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) - is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. - segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. \"\"\" raise NotImplementedError () def inference_roidbs ( self ): \"\"\" Returns: - *(list[dict])*: Each dict corresponds to one image to run inference on. The following keys in the dict are expected: - file_name (str): full path to the image - image_id (str): an id for the image. The inference results will be stored with this id. \"\"\" raise NotImplementedError () def eval_inference_results ( self , results : List [ dict ], output : str = None ): \"\"\" Arguments: - *results*: (list[dict]): the inference results as dicts. Each dict corresponds to one __instance__. It contains the following keys: - image_id (str): the id that matches `inference_roidbs`. - category_id (int): the category prediction, in range [1, #category] - bbox (list[float]): x1, y1, x2, y2 - score (float): - segmentation: the segmentation mask in COCO's rle format. - *output*: (str): the output file or directory to optionally save the results to. Returns: - dict: the evaluation results. \"\"\" raise NotImplementedError () class DatasetRegistry (): _registry = {} _metadata_registry = defaultdict ( dict ) @staticmethod def register ( name , func ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *func*: a function which returns an instance of `DatasetSplit` \"\"\" assert name not in DatasetRegistry . _registry , \"Dataset {} was registered already!\" . format ( name ) DatasetRegistry . _registry [ name ] = func @staticmethod def get ( name ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" Returns: - *DatasetSplit* \"\"\" assert name in DatasetRegistry . _registry , \"Dataset {} was not registered!\" . format ( name ) return DatasetRegistry . _registry [ name ]() @staticmethod def register_metadata ( name , key , value ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *key*: the key of the metadata, e.g., \"class_names\" - *value*: the value of the metadata \"\"\" DatasetRegistry . _metadata_registry [ name ][ key ] = value @staticmethod def get_metadata ( name , key ): \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \"coco_train2017\" - *key*: the key of the metadata, e.g., \"class_names\" Returns: - *value* \"\"\" return DatasetRegistry . _metadata_registry [ name ][ key ]","title":"Module mot.object_detection.dataset.dataset"},{"location":"reference/mot/object_detection/dataset/dataset/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/dataset/dataset/#datasetregistry","text":"class DatasetRegistry ( / , * args , ** kwargs )","title":"DatasetRegistry"},{"location":"reference/mot/object_detection/dataset/dataset/#static-methods","text":"","title":"Static methods"},{"location":"reference/mot/object_detection/dataset/dataset/#get","text":"def get ( name ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" Returns: DatasetSplit View Source @staticmethod def get ( name ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" Returns: - *DatasetSplit* \"\"\" assert name in DatasetRegistry . _registry , \"Dataset {} was not registered!\" . format ( name ) return DatasetRegistry . _registry [ name ] ()","title":"get"},{"location":"reference/mot/object_detection/dataset/dataset/#get_metadata","text":"def get_metadata ( name , key ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" key : the key of the metadata, e.g., \"class_names\" Returns: value View Source @staticmethod def get_metadata ( name , key ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *key*: the key of the metadata, e.g., \" class_names \" Returns: - *value* \"\"\" return DatasetRegistry . _metadata_registry [ name ][ key ]","title":"get_metadata"},{"location":"reference/mot/object_detection/dataset/dataset/#register","text":"def register ( name , func ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" func : a function which returns an instance of DatasetSplit View Source @staticmethod def register ( name , func ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *func*: a function which returns an instance of `DatasetSplit` \"\"\" assert name not in DatasetRegistry . _registry , \"Dataset {} was registered already!\" . format ( name ) DatasetRegistry . _registry [ name ] = func","title":"register"},{"location":"reference/mot/object_detection/dataset/dataset/#register_metadata","text":"def register_metadata ( name , key , value ) Arguments: name : the name of the dataset split, e.g. \"coco_train2017\" key : the key of the metadata, e.g., \"class_names\" value : the value of the metadata View Source @staticmethod def register_metadata ( name , key , value ) : \"\"\" Arguments: - *name*: the name of the dataset split, e.g. \" coco_train2017 \" - *key*: the key of the metadata, e.g., \" class_names \" - *value*: the value of the metadata \"\"\" DatasetRegistry . _metadata_registry [ name ][ key ] = value","title":"register_metadata"},{"location":"reference/mot/object_detection/dataset/dataset/#datasetsplit","text":"class DatasetSplit ( / , * args , ** kwargs ) A class to load datasets, evaluate results for a datast split (e.g., \"coco_train_2017\") To use your own dataset that's not in COCO format, write a subclass that implements the interfaces.","title":"DatasetSplit"},{"location":"reference/mot/object_detection/dataset/dataset/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/dataset/dataset/#eval_inference_results","text":"def eval_inference_results ( self , results : List [ dict ], output : str = None ) Arguments: results : (list[dict]): the inference results as dicts. Each dict corresponds to one instance . It contains the following keys: image_id (str): the id that matches inference_roidbs . category_id (int): the category prediction, in range [1, #category] bbox (list[float]): x1, y1, x2, y2 score (float): segmentation: the segmentation mask in COCO's rle format. output : (str): the output file or directory to optionally save the results to. Returns: dict: the evaluation results. View Source def eval_inference_results ( self , results : List [ dict ] , output : str = None ) : \"\"\" Arguments: - *results*: (list[dict]): the inference results as dicts. Each dict corresponds to one __instance__. It contains the following keys: - image_id (str): the id that matches `inference_roidbs`. - category_id (int): the category prediction, in range [1, #category] - bbox (list[float]): x1, y1, x2, y2 - score (float): - segmentation: the segmentation mask in COCO's rle format. - *output*: (str): the output file or directory to optionally save the results to. Returns: - dict: the evaluation results. \"\"\" raise NotImplementedError ()","title":"eval_inference_results"},{"location":"reference/mot/object_detection/dataset/dataset/#inference_roidbs","text":"def inference_roidbs ( self ) Returns: (list[dict]) : Each dict corresponds to one image to run inference on. The following keys in the dict are expected: file_name (str): full path to the image image_id (str): an id for the image. The inference results will be stored with this id. View Source def inference_roidbs ( self ) : \"\"\" Returns: - *(list[dict])*: Each dict corresponds to one image to run inference on. The following keys in the dict are expected: - file_name (str): full path to the image - image_id (str): an id for the image. The inference results will be stored with this id. \"\"\" raise NotImplementedError ()","title":"inference_roidbs"},{"location":"reference/mot/object_detection/dataset/dataset/#training_roidbs","text":"def training_roidbs ( self ) Returns: (list[dict]) : Produce \"roidbs\" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: file_name: str, full path to the image boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. View Source def training_roidbs ( self ) : \"\"\" Returns: - *(list[dict])*: Produce \" roidbs \" as a list of dict, each dict corresponds to one image with k>=0 instances. and the following keys are expected for training: - file_name: str, full path to the image - boxes: numpy array of kx4 floats, each row is [x1, y1, x2, y2] - class: numpy array of k integers, in the range of [1, #categories], NOT [0, #categories) - is_crowd: numpy array of k 0 or 1. Use k False if you don't know what it means. - segmentation: k lists of numpy arrays (one for each instance). Each list of numpy arrays corresponds to the mask for one instance. Each numpy array in the list is a polygon of shape Nx2, because one mask can be represented by N polygons. Each row in the Nx2 array is a (x, y) coordinate. If your segmentation annotations are originally masks rather than polygons, either convert it, or the augmentation will need to be changed or skipped accordingly. Include this field only if training Mask R-CNN. \"\"\" raise NotImplementedError ()","title":"training_roidbs"},{"location":"reference/mot/object_detection/dataset/mot/","text":"Module mot.object_detection.dataset.mot View Source import json import os from typing import Dict , List import numpy as np from mot.object_detection.dataset import DatasetRegistry , DatasetSplit from mot.object_detection.utils.np_box_ops import area as np_area class MotDataset ( DatasetSplit ): \"\"\"For more details on each methods see the description of dataset.py Arguments: - *base_dir*: the directory where the dataset is located. There should be inside a JSON named {dataset_name}. - *split*: \"train\" or \"val\" - *dataset_file*: the name of the JSON to use as dataset in base_dir. A line of this JSON should look like: ```json { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` - *classes_file*: the name of the JSON to use as classes in base_dir. A line of this JSON should look like: ```json [\"bottles\", \"others\", \"fragments\"] ``` - *images_folder*: Where the images are stored, and named by their md5. - if it starts with \"/\", the absolute path will be used - if not, the path os.path.join(base_dir, images_folder) will be used Raises: - *FileNotFoundError*: if the dataset or classes are missing. - *NotADirectoryError*: if the images folder is missing. \"\"\" def __init__ ( self , base_dir : str , split : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ): assert split in [ \"train\" , \"val\" ] self . split = split self . ratio = 0.99 self . base_dir = base_dir self . dataset_path = os . path . join ( base_dir , dataset_file ) if not os . path . isfile ( self . dataset_path ): raise FileNotFoundError ( \"Missing file {} . Make sure your file is named correctly.\" . format ( self . dataset_path ) ) self . classes_path = os . path . join ( base_dir , classes_file ) if not os . path . isfile ( self . classes_path ): raise FileNotFoundError ( \"Missing file {} . Make sure your file is named correctly.\" . format ( self . classes_path ) ) with open ( self . classes_path ) as f : classes = json . load ( f ) self . class_to_idx = { name : i + 1 for i , name in enumerate ( classes )} # yapf: disable if images_folder [ 0 ] == \"/\" : # absolute path if the images are not stored inside the dataset folder for example self . images_folder = images_folder else : # relative path, then it means that the images are stored inside the dataset folder self . images_folder = os . path . join ( base_dir , images_folder ) # yapf: enable if not os . path . isdir ( self . images_folder ): raise NotADirectoryError ( \"Missing images folder {} .\" . format ( self . images_folder )) def read_labels ( self , labels : List [ Dict [ str , object ]]) -> ( np . ndarray , np . ndarray ): \"\"\"[summary] Arguments: - *labels*: A list of dicts such as: ```python3 labels = [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ] ``` Returns: - *np.ndarray, np.ndarray*: The boxes and classes. Classes starts at 1 since 0 is for background. \"\"\" boxes = [] classes = [] for crop in labels : boxes . append ([ int ( coord ) for coord in crop [ \"bbox\" ]]) classes . append ( self . class_to_idx [ crop [ \"label\" ]]) return np . array ( boxes ), np . array ( classes ) def read_lines ( self ) -> List [ Dict [ str , object ]]: \"\"\"Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: - *List[Dict[str, object]]*: Each dict should look like: ```python { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` \"\"\" with open ( self . dataset_path , \"r\" ) as f : lines = [ json . loads ( line ) for line in f ] if self . split == \"train\" : return lines [: int ( len ( lines ) * self . ratio )] else : return lines [ int ( len ( lines ) * self . ratio ):] def read_file_name ( self , md5 : str ) -> str : return os . path . join ( self . images_folder , md5 ) def training_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : file_name = self . read_file_name ( line [ \"md5\" ]) if os . path . isfile ( file_name ): boxes , classes = self . read_labels ( line [ \"labels\" ]) # Remove boxes with empty area if boxes . size : non_zero_area = np_area ( boxes ) > 0 boxes = boxes [ non_zero_area , :] classes = classes [ non_zero_area ] boxes = np . float32 ( boxes ) roidb = { \"file_name\" : file_name , \"boxes\" : boxes , \"class\" : classes , \"is_crowd\" : np . zeros (( classes . shape [ 0 ])) } roidbs . append ( roidb ) return roidbs def inference_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : roidb = { \"file_name\" : self . read_file_name ( line [ \"md5\" ]), \"image_id\" : line [ \"md5\" ]} roidbs . append ( roidb ) return roidbs def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ): id_to_entity_id = { v : k for k , v in self . class_to_idx . items ()} for res in results : res [ \"category_id\" ] = id_to_entity_id [ res [ \"category_id\" ]] if output is not None : with open ( output , 'w' ) as f : json . dump ( results , f ) # TODO : implement metrics calculation to return them return {} def register_mot ( base_dir : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ): \"\"\"Register a dataset to the registry. See `MotDataset` for more details on arguments. \"\"\" base_dir = os . path . expanduser ( base_dir ) classes_path = os . path . join ( base_dir , \"classes.json\" ) class_names = [ \"BG\" ] + get_class_names ( classes_path ) for split in [ \"train\" , \"val\" ]: name = \"mot_\" + split DatasetRegistry . register ( name , lambda x = split : MotDataset ( base_dir , split = x )) DatasetRegistry . register_metadata ( name , \"class_names\" , class_names ) def get_class_names ( classes_path : str ) -> List [ str ]: \"\"\" Arguments: - *classes_path*: The path to a file containing class names, for example [\"bottles\", \"others\", \"fragments\"] Returns: - *List[str]*: A list: [\"bottles\", \"others\", \"fragments\"] \"\"\" if not os . path . isfile ( classes_path ): raise FileNotFoundError ( \"Missing file for classes at {} \" . format ( classes_path )) with open ( classes_path , \"r\" ) as f : lines = f . readlines () class_names = json . loads ( lines [ 0 ]) return class_names Functions get_class_names def get_class_names ( classes_path : str ) -> List [ str ] Arguments: classes_path : The path to a file containing class names, for example [\"bottles\", \"others\", \"fragments\"] Returns: List[str] : A list: [\"bottles\", \"others\", \"fragments\"] View Source def get_class_names ( classes_path : str ) -> List [ str ] : \"\"\" Arguments: - *classes_path*: The path to a file containing class names, for example [\" bottles \", \" others \", \" fragments \"] Returns: - *List[str]*: A list: [\" bottles \", \" others \", \" fragments \"] \"\"\" if not os . path . isfile ( classes_path ) : raise FileNotFoundError ( \"Missing file for classes at {}\" . format ( classes_path )) with open ( classes_path , \"r\" ) as f : lines = f . readlines () class_names = json . loads ( lines [ 0 ] ) return class_names register_mot def register_mot ( base_dir : str , dataset_file : str = 'dataset.json' , classes_file : str = 'classes.json' , images_folder : str = 'Images_md5' ) Register a dataset to the registry. See MotDataset for more details on arguments. View Source def register_mot ( base_dir : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ) : \" \"\" Register a dataset to the registry. See `MotDataset` for more details on arguments. \"\" \" base_dir = os . path . expanduser ( base_dir ) classes_path = os . path . join ( base_dir , \"classes.json\" ) class_names = [ \"BG\" ] + get_class_names ( classes_path ) for split in [ \"train\" , \"val\" ] : name = \"mot_\" + split DatasetRegistry . register ( name , lambda x = split : MotDataset ( base_dir , split = x )) DatasetRegistry . register_metadata ( name , \"class_names\" , class_names ) Classes MotDataset class MotDataset ( base_dir : str , split : str , dataset_file : str = 'dataset.json' , classes_file : str = 'classes.json' , images_folder : str = 'Images_md5' ) For more details on each methods see the description of dataset.py Arguments: base_dir : the directory where the dataset is located. There should be inside a JSON named {dataset_name}. split : \"train\" or \"val\" dataset_file : the name of the JSON to use as dataset in base_dir. A line of this JSON should look like: { \"md5\" : \"159af544cd304c3ee50f63c281afa032\" , \"labels\" : [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ], } classes_file : the name of the JSON to use as classes in base_dir. A line of this JSON should look like: [ \"bottles\" , \"others\" , \"fragments\" ] images_folder : Where the images are stored, and named by their md5. if it starts with \"/\", the absolute path will be used if not, the path os.path.join(base_dir, images_folder) will be used Raises: FileNotFoundError : if the dataset or classes are missing. NotADirectoryError : if the images folder is missing. Ancestors (in MRO) mot.object_detection.dataset.dataset.DatasetSplit Methods eval_inference_results def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ) View Source def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ): id_to_entity_id = { v : k for k , v in self . class_to_idx . items () } for res in results : res [ \"category_id\" ] = id_to_entity_id [ res [ \"category_id\" ]] if output is not None : with open ( output , 'w' ) as f : json . dump ( results , f ) # TODO : implement metrics calculation to return them return {} inference_roidbs def inference_roidbs ( self ) View Source def inference_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : roidb = { \"file_name\" : self . read_file_name ( line [ \"md5\" ]), \"image_id\" : line [ \"md5\" ] } roidbs . append ( roidb ) return roidbs read_file_name def read_file_name ( self , md5 : str ) -> str View Source def read_file_name ( self , md5 : str ) -> str : return os . path . join ( self . images_folder , md5 ) read_labels def read_labels ( self , labels : List [ Dict [ str , object ]] ) -> ( < class ' numpy . ndarray '>, <class ' numpy . ndarray '>) [summary] Arguments: labels : A list of dicts such as: labels = [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ] Returns: np.ndarray, np.ndarray : The boxes and classes. Classes starts at 1 since 0 is for background. View Source def read_labels ( self , labels : List [ Dict[str, object ] ] ) -> ( np . ndarray , np . ndarray ) : \"\"\"[summary] Arguments: - *labels*: A list of dicts such as: ```python3 labels = [ { \" bbox \": [\" 250 \", \" 255 \", \" 278 \", \" 289 \"], \" label \": \" fragments \" }, { \" bbox \": [\" 432 \", \" 231 \", \" 459 \", \" 280 \"], \" label \": \" fragments \" } ] ``` Returns: - *np.ndarray, np.ndarray*: The boxes and classes. Classes starts at 1 since 0 is for background. \"\"\" boxes = [] classes = [] for crop in labels : boxes . append ( [ int(coord) for coord in crop[\"bbox\" ] ] ) classes . append ( self . class_to_idx [ crop[\"label\" ] ] ) return np . array ( boxes ), np . array ( classes ) read_lines def read_lines ( self ) -> List [ Dict [ str , object ]] Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: List[Dict[str, object]] : Each dict should look like: { \"md5\" : \"159af544cd304c3ee50f63c281afa032\" , \"labels\" : [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ], } View Source def read_lines ( self ) -> List [ Dict [ str , object ]]: \"\"\"Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: - *List[Dict[str, object]]*: Each dict should look like: ```python { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` \"\"\" with open ( self . dataset_path , \"r\" ) as f : lines = [ json . loads ( line ) for line in f ] if self . split == \"train\" : return lines [: int ( len ( lines ) * self . ratio )] else : return lines [ int ( len ( lines ) * self . ratio ):] training_roidbs def training_roidbs ( self ) View Source def training_roidbs ( self ) : lines = self . read_lines () roidbs = [] for line in lines : file_name = self . read_file_name ( line [ \"md5\" ] ) if os . path . isfile ( file_name ) : boxes , classes = self . read_labels ( line [ \"labels\" ] ) # Remove boxes with empty area if boxes . size : non_zero_area = np_area ( boxes ) > 0 boxes = boxes [ non_zero_area, : ] classes = classes [ non_zero_area ] boxes = np . float32 ( boxes ) roidb = { \"file_name\" : file_name , \"boxes\" : boxes , \"class\" : classes , \"is_crowd\" : np . zeros (( classes . shape [ 0 ] )) } roidbs . append ( roidb ) return roidbs","title":"Mot"},{"location":"reference/mot/object_detection/dataset/mot/#module-motobject_detectiondatasetmot","text":"View Source import json import os from typing import Dict , List import numpy as np from mot.object_detection.dataset import DatasetRegistry , DatasetSplit from mot.object_detection.utils.np_box_ops import area as np_area class MotDataset ( DatasetSplit ): \"\"\"For more details on each methods see the description of dataset.py Arguments: - *base_dir*: the directory where the dataset is located. There should be inside a JSON named {dataset_name}. - *split*: \"train\" or \"val\" - *dataset_file*: the name of the JSON to use as dataset in base_dir. A line of this JSON should look like: ```json { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` - *classes_file*: the name of the JSON to use as classes in base_dir. A line of this JSON should look like: ```json [\"bottles\", \"others\", \"fragments\"] ``` - *images_folder*: Where the images are stored, and named by their md5. - if it starts with \"/\", the absolute path will be used - if not, the path os.path.join(base_dir, images_folder) will be used Raises: - *FileNotFoundError*: if the dataset or classes are missing. - *NotADirectoryError*: if the images folder is missing. \"\"\" def __init__ ( self , base_dir : str , split : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ): assert split in [ \"train\" , \"val\" ] self . split = split self . ratio = 0.99 self . base_dir = base_dir self . dataset_path = os . path . join ( base_dir , dataset_file ) if not os . path . isfile ( self . dataset_path ): raise FileNotFoundError ( \"Missing file {} . Make sure your file is named correctly.\" . format ( self . dataset_path ) ) self . classes_path = os . path . join ( base_dir , classes_file ) if not os . path . isfile ( self . classes_path ): raise FileNotFoundError ( \"Missing file {} . Make sure your file is named correctly.\" . format ( self . classes_path ) ) with open ( self . classes_path ) as f : classes = json . load ( f ) self . class_to_idx = { name : i + 1 for i , name in enumerate ( classes )} # yapf: disable if images_folder [ 0 ] == \"/\" : # absolute path if the images are not stored inside the dataset folder for example self . images_folder = images_folder else : # relative path, then it means that the images are stored inside the dataset folder self . images_folder = os . path . join ( base_dir , images_folder ) # yapf: enable if not os . path . isdir ( self . images_folder ): raise NotADirectoryError ( \"Missing images folder {} .\" . format ( self . images_folder )) def read_labels ( self , labels : List [ Dict [ str , object ]]) -> ( np . ndarray , np . ndarray ): \"\"\"[summary] Arguments: - *labels*: A list of dicts such as: ```python3 labels = [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ] ``` Returns: - *np.ndarray, np.ndarray*: The boxes and classes. Classes starts at 1 since 0 is for background. \"\"\" boxes = [] classes = [] for crop in labels : boxes . append ([ int ( coord ) for coord in crop [ \"bbox\" ]]) classes . append ( self . class_to_idx [ crop [ \"label\" ]]) return np . array ( boxes ), np . array ( classes ) def read_lines ( self ) -> List [ Dict [ str , object ]]: \"\"\"Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: - *List[Dict[str, object]]*: Each dict should look like: ```python { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` \"\"\" with open ( self . dataset_path , \"r\" ) as f : lines = [ json . loads ( line ) for line in f ] if self . split == \"train\" : return lines [: int ( len ( lines ) * self . ratio )] else : return lines [ int ( len ( lines ) * self . ratio ):] def read_file_name ( self , md5 : str ) -> str : return os . path . join ( self . images_folder , md5 ) def training_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : file_name = self . read_file_name ( line [ \"md5\" ]) if os . path . isfile ( file_name ): boxes , classes = self . read_labels ( line [ \"labels\" ]) # Remove boxes with empty area if boxes . size : non_zero_area = np_area ( boxes ) > 0 boxes = boxes [ non_zero_area , :] classes = classes [ non_zero_area ] boxes = np . float32 ( boxes ) roidb = { \"file_name\" : file_name , \"boxes\" : boxes , \"class\" : classes , \"is_crowd\" : np . zeros (( classes . shape [ 0 ])) } roidbs . append ( roidb ) return roidbs def inference_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : roidb = { \"file_name\" : self . read_file_name ( line [ \"md5\" ]), \"image_id\" : line [ \"md5\" ]} roidbs . append ( roidb ) return roidbs def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ): id_to_entity_id = { v : k for k , v in self . class_to_idx . items ()} for res in results : res [ \"category_id\" ] = id_to_entity_id [ res [ \"category_id\" ]] if output is not None : with open ( output , 'w' ) as f : json . dump ( results , f ) # TODO : implement metrics calculation to return them return {} def register_mot ( base_dir : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ): \"\"\"Register a dataset to the registry. See `MotDataset` for more details on arguments. \"\"\" base_dir = os . path . expanduser ( base_dir ) classes_path = os . path . join ( base_dir , \"classes.json\" ) class_names = [ \"BG\" ] + get_class_names ( classes_path ) for split in [ \"train\" , \"val\" ]: name = \"mot_\" + split DatasetRegistry . register ( name , lambda x = split : MotDataset ( base_dir , split = x )) DatasetRegistry . register_metadata ( name , \"class_names\" , class_names ) def get_class_names ( classes_path : str ) -> List [ str ]: \"\"\" Arguments: - *classes_path*: The path to a file containing class names, for example [\"bottles\", \"others\", \"fragments\"] Returns: - *List[str]*: A list: [\"bottles\", \"others\", \"fragments\"] \"\"\" if not os . path . isfile ( classes_path ): raise FileNotFoundError ( \"Missing file for classes at {} \" . format ( classes_path )) with open ( classes_path , \"r\" ) as f : lines = f . readlines () class_names = json . loads ( lines [ 0 ]) return class_names","title":"Module mot.object_detection.dataset.mot"},{"location":"reference/mot/object_detection/dataset/mot/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/dataset/mot/#get_class_names","text":"def get_class_names ( classes_path : str ) -> List [ str ] Arguments: classes_path : The path to a file containing class names, for example [\"bottles\", \"others\", \"fragments\"] Returns: List[str] : A list: [\"bottles\", \"others\", \"fragments\"] View Source def get_class_names ( classes_path : str ) -> List [ str ] : \"\"\" Arguments: - *classes_path*: The path to a file containing class names, for example [\" bottles \", \" others \", \" fragments \"] Returns: - *List[str]*: A list: [\" bottles \", \" others \", \" fragments \"] \"\"\" if not os . path . isfile ( classes_path ) : raise FileNotFoundError ( \"Missing file for classes at {}\" . format ( classes_path )) with open ( classes_path , \"r\" ) as f : lines = f . readlines () class_names = json . loads ( lines [ 0 ] ) return class_names","title":"get_class_names"},{"location":"reference/mot/object_detection/dataset/mot/#register_mot","text":"def register_mot ( base_dir : str , dataset_file : str = 'dataset.json' , classes_file : str = 'classes.json' , images_folder : str = 'Images_md5' ) Register a dataset to the registry. See MotDataset for more details on arguments. View Source def register_mot ( base_dir : str , dataset_file : str = \"dataset.json\" , classes_file : str = \"classes.json\" , images_folder : str = \"Images_md5\" ) : \" \"\" Register a dataset to the registry. See `MotDataset` for more details on arguments. \"\" \" base_dir = os . path . expanduser ( base_dir ) classes_path = os . path . join ( base_dir , \"classes.json\" ) class_names = [ \"BG\" ] + get_class_names ( classes_path ) for split in [ \"train\" , \"val\" ] : name = \"mot_\" + split DatasetRegistry . register ( name , lambda x = split : MotDataset ( base_dir , split = x )) DatasetRegistry . register_metadata ( name , \"class_names\" , class_names )","title":"register_mot"},{"location":"reference/mot/object_detection/dataset/mot/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/dataset/mot/#motdataset","text":"class MotDataset ( base_dir : str , split : str , dataset_file : str = 'dataset.json' , classes_file : str = 'classes.json' , images_folder : str = 'Images_md5' ) For more details on each methods see the description of dataset.py Arguments: base_dir : the directory where the dataset is located. There should be inside a JSON named {dataset_name}. split : \"train\" or \"val\" dataset_file : the name of the JSON to use as dataset in base_dir. A line of this JSON should look like: { \"md5\" : \"159af544cd304c3ee50f63c281afa032\" , \"labels\" : [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ], } classes_file : the name of the JSON to use as classes in base_dir. A line of this JSON should look like: [ \"bottles\" , \"others\" , \"fragments\" ] images_folder : Where the images are stored, and named by their md5. if it starts with \"/\", the absolute path will be used if not, the path os.path.join(base_dir, images_folder) will be used Raises: FileNotFoundError : if the dataset or classes are missing. NotADirectoryError : if the images folder is missing.","title":"MotDataset"},{"location":"reference/mot/object_detection/dataset/mot/#ancestors-in-mro","text":"mot.object_detection.dataset.dataset.DatasetSplit","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/dataset/mot/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/dataset/mot/#eval_inference_results","text":"def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ) View Source def eval_inference_results ( self , results : Dict [ str , object ], output : str = None ): id_to_entity_id = { v : k for k , v in self . class_to_idx . items () } for res in results : res [ \"category_id\" ] = id_to_entity_id [ res [ \"category_id\" ]] if output is not None : with open ( output , 'w' ) as f : json . dump ( results , f ) # TODO : implement metrics calculation to return them return {}","title":"eval_inference_results"},{"location":"reference/mot/object_detection/dataset/mot/#inference_roidbs","text":"def inference_roidbs ( self ) View Source def inference_roidbs ( self ): lines = self . read_lines () roidbs = [] for line in lines : roidb = { \"file_name\" : self . read_file_name ( line [ \"md5\" ]), \"image_id\" : line [ \"md5\" ] } roidbs . append ( roidb ) return roidbs","title":"inference_roidbs"},{"location":"reference/mot/object_detection/dataset/mot/#read_file_name","text":"def read_file_name ( self , md5 : str ) -> str View Source def read_file_name ( self , md5 : str ) -> str : return os . path . join ( self . images_folder , md5 )","title":"read_file_name"},{"location":"reference/mot/object_detection/dataset/mot/#read_labels","text":"def read_labels ( self , labels : List [ Dict [ str , object ]] ) -> ( < class ' numpy . ndarray '>, <class ' numpy . ndarray '>) [summary] Arguments: labels : A list of dicts such as: labels = [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ] Returns: np.ndarray, np.ndarray : The boxes and classes. Classes starts at 1 since 0 is for background. View Source def read_labels ( self , labels : List [ Dict[str, object ] ] ) -> ( np . ndarray , np . ndarray ) : \"\"\"[summary] Arguments: - *labels*: A list of dicts such as: ```python3 labels = [ { \" bbox \": [\" 250 \", \" 255 \", \" 278 \", \" 289 \"], \" label \": \" fragments \" }, { \" bbox \": [\" 432 \", \" 231 \", \" 459 \", \" 280 \"], \" label \": \" fragments \" } ] ``` Returns: - *np.ndarray, np.ndarray*: The boxes and classes. Classes starts at 1 since 0 is for background. \"\"\" boxes = [] classes = [] for crop in labels : boxes . append ( [ int(coord) for coord in crop[\"bbox\" ] ] ) classes . append ( self . class_to_idx [ crop[\"label\" ] ] ) return np . array ( boxes ), np . array ( classes )","title":"read_labels"},{"location":"reference/mot/object_detection/dataset/mot/#read_lines","text":"def read_lines ( self ) -> List [ Dict [ str , object ]] Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: List[Dict[str, object]] : Each dict should look like: { \"md5\" : \"159af544cd304c3ee50f63c281afa032\" , \"labels\" : [ { \"bbox\" : [ \"250\" , \"255\" , \"278\" , \"289\" ], \"label\" : \"fragments\" }, { \"bbox\" : [ \"432\" , \"231\" , \"459\" , \"280\" ], \"label\" : \"fragments\" } ], } View Source def read_lines ( self ) -> List [ Dict [ str , object ]]: \"\"\"Reads the int(len(lines) * self.ratio) first lines if training, and the last ones otherwise. Arguments: Returns: - *List[Dict[str, object]]*: Each dict should look like: ```python { \"md5\": \"159af544cd304c3ee50f63c281afa032\", \"labels\": [ { \"bbox\": [\"250\", \"255\", \"278\", \"289\"], \"label\": \"fragments\" }, { \"bbox\": [\"432\", \"231\", \"459\", \"280\"], \"label\": \"fragments\" } ], } ``` \"\"\" with open ( self . dataset_path , \"r\" ) as f : lines = [ json . loads ( line ) for line in f ] if self . split == \"train\" : return lines [: int ( len ( lines ) * self . ratio )] else : return lines [ int ( len ( lines ) * self . ratio ):]","title":"read_lines"},{"location":"reference/mot/object_detection/dataset/mot/#training_roidbs","text":"def training_roidbs ( self ) View Source def training_roidbs ( self ) : lines = self . read_lines () roidbs = [] for line in lines : file_name = self . read_file_name ( line [ \"md5\" ] ) if os . path . isfile ( file_name ) : boxes , classes = self . read_labels ( line [ \"labels\" ] ) # Remove boxes with empty area if boxes . size : non_zero_area = np_area ( boxes ) > 0 boxes = boxes [ non_zero_area, : ] classes = classes [ non_zero_area ] boxes = np . float32 ( boxes ) roidb = { \"file_name\" : file_name , \"boxes\" : boxes , \"class\" : classes , \"is_crowd\" : np . zeros (( classes . shape [ 0 ] )) } roidbs . append ( roidb ) return roidbs","title":"training_roidbs"},{"location":"reference/mot/object_detection/modeling/","text":"Module mot.object_detection.modeling Sub-modules mot.object_detection.modeling.backbone mot.object_detection.modeling.generalized_rcnn mot.object_detection.modeling.model_box mot.object_detection.modeling.model_cascade mot.object_detection.modeling.model_fpn mot.object_detection.modeling.model_frcnn mot.object_detection.modeling.model_mrcnn mot.object_detection.modeling.model_rpn","title":"Index"},{"location":"reference/mot/object_detection/modeling/#module-motobject_detectionmodeling","text":"","title":"Module mot.object_detection.modeling"},{"location":"reference/mot/object_detection/modeling/#sub-modules","text":"mot.object_detection.modeling.backbone mot.object_detection.modeling.generalized_rcnn mot.object_detection.modeling.model_box mot.object_detection.modeling.model_cascade mot.object_detection.modeling.model_fpn mot.object_detection.modeling.model_frcnn mot.object_detection.modeling.model_mrcnn mot.object_detection.modeling.model_rpn","title":"Sub-modules"},{"location":"reference/mot/object_detection/modeling/backbone/","text":"Module mot.object_detection.modeling.backbone View Source # -*- coding: utf-8 -*- # File: backbone.py import numpy as np import tensorflow as tf from contextlib import ExitStack , contextmanager from tensorpack.models import BatchNorm , Conv2D , MaxPooling , layer_register from tensorpack.tfutils import argscope from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope from tensorpack.tfutils.varreplace import custom_getter_scope , freeze_variables from mot.object_detection.config import config as cfg @layer_register ( log_shape = True ) def GroupNorm ( x , group = 32 , gamma_initializer = tf . constant_initializer ( 1. )): \"\"\" More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. \"\"\" shape = x . get_shape () . as_list () ndims = len ( shape ) assert ndims == 4 , shape chan = shape [ 1 ] assert chan % group == 0 , chan group_size = chan // group orig_shape = tf . shape ( x ) h , w = orig_shape [ 2 ], orig_shape [ 3 ] x = tf . reshape ( x , tf . stack ([ - 1 , group , group_size , h , w ])) mean , var = tf . nn . moments ( x , [ 2 , 3 , 4 ], keep_dims = True ) new_shape = [ 1 , group , group_size , 1 , 1 ] beta = tf . get_variable ( 'beta' , [ chan ], initializer = tf . constant_initializer ()) beta = tf . reshape ( beta , new_shape ) gamma = tf . get_variable ( 'gamma' , [ chan ], initializer = gamma_initializer ) gamma = tf . reshape ( gamma , new_shape ) out = tf . nn . batch_normalization ( x , mean , var , beta , gamma , 1e-5 , name = 'output' ) return tf . reshape ( out , orig_shape , name = 'output' ) def freeze_affine_getter ( getter , * args , ** kwargs ): # custom getter to freeze affine params inside bn name = args [ 0 ] if len ( args ) else kwargs . get ( 'name' ) if name . endswith ( '/gamma' ) or name . endswith ( '/beta' ): kwargs [ 'trainable' ] = False ret = getter ( * args , ** kwargs ) tf . add_to_collection ( tf . GraphKeys . MODEL_VARIABLES , ret ) else : ret = getter ( * args , ** kwargs ) return ret def maybe_reverse_pad ( topleft , bottomright ): if cfg . BACKBONE . TF_PAD_MODE : return [ topleft , bottomright ] return [ bottomright , topleft ] @contextmanager def backbone_scope ( freeze ): \"\"\" Args: freeze (bool): whether to freeze all the variables under the scope \"\"\" def nonlin ( x ): x = get_norm ()( x ) return tf . nn . relu ( x ) with argscope ([ Conv2D , MaxPooling , BatchNorm ], data_format = 'channels_first' ), \\ argscope ( Conv2D , use_bias = False , activation = nonlin , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' )), \\ ExitStack () as stack : if cfg . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' ]: if freeze or cfg . BACKBONE . NORM == 'FreezeBN' : stack . enter_context ( argscope ( BatchNorm , training = False )) else : stack . enter_context ( argscope ( BatchNorm , sync_statistics = 'nccl' if cfg . TRAINER == 'replicated' else 'horovod' )) if freeze : stack . enter_context ( freeze_variables ( stop_gradient = False , skip_collection = True )) else : # the layers are not completely freezed, but we may want to only freeze the affine if cfg . BACKBONE . FREEZE_AFFINE : stack . enter_context ( custom_getter_scope ( freeze_affine_getter )) yield def image_preprocess ( image , bgr = True ): with tf . name_scope ( 'image_preprocess' ): if image . dtype . base_dtype != tf . float32 : image = tf . cast ( image , tf . float32 ) mean = cfg . PREPROC . PIXEL_MEAN std = np . asarray ( cfg . PREPROC . PIXEL_STD ) if bgr : mean = mean [:: - 1 ] std = std [:: - 1 ] image_mean = tf . constant ( mean , dtype = tf . float32 ) image_invstd = tf . constant ( 1.0 / std , dtype = tf . float32 ) image = ( image - image_mean ) * image_invstd return image def get_norm ( zero_init = False ): if cfg . BACKBONE . NORM == 'None' : return lambda x : x if cfg . BACKBONE . NORM == 'GN' : Norm = GroupNorm layer_name = 'gn' else : Norm = BatchNorm layer_name = 'bn' return lambda x : Norm ( layer_name , x , gamma_initializer = tf . zeros_initializer () if zero_init else None ) def resnet_shortcut ( l , n_out , stride , activation = tf . identity ): n_in = l . shape [ 1 ] if n_in != n_out : # change dimension when channel is not the same # TF's SAME mode output ceil(x/stride), which is NOT what we want when x is odd and stride is 2 # In FPN mode, the images are pre-padded already. if not cfg . MODE_FPN and stride == 2 : l = l [:, :, : - 1 , : - 1 ] return Conv2D ( 'convshortcut' , l , n_out , 1 , strides = stride , activation = activation ) else : return l def resnet_bottleneck ( l , ch_out , stride ): shortcut = l if cfg . BACKBONE . STRIDE_1X1 : if stride == 2 : l = l [:, :, : - 1 , : - 1 ] l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = stride ) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 1 ) else : l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = 1 ) if stride == 2 : l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 2 , padding = 'VALID' ) else : l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = stride ) if cfg . BACKBONE . NORM != 'None' : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = get_norm ( zero_init = True )) else : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = tf . identity , kernel_initializer = tf . constant_initializer ()) ret = l + resnet_shortcut ( shortcut , ch_out * 4 , stride , activation = get_norm ( zero_init = False )) return tf . nn . relu ( ret , name = 'output' ) def resnet_group ( name , l , block_func , features , count , stride ): with tf . variable_scope ( name ): for i in range ( 0 , count ): with tf . variable_scope ( 'block {} ' . format ( i )): l = block_func ( l , features , stride if i == 0 else 1 ) return l def resnet_c4_backbone ( image , num_blocks ): assert len ( num_blocks ) == 3 freeze_at = cfg . BACKBONE . FREEZE_AT with backbone_scope ( freeze = freeze_at > 0 ): l = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 2 , 3 ), maybe_reverse_pad ( 2 , 3 )]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) # 16x downsampling up to now return c4 @auto_reuse_variable_scope def resnet_conv5 ( image , num_block ): with backbone_scope ( freeze = False ): l = resnet_group ( 'group3' , image , resnet_bottleneck , 512 , num_block , 2 ) return l def resnet_fpn_backbone ( image , num_blocks ): freeze_at = cfg . BACKBONE . FREEZE_AT shape2d = tf . shape ( image )[ 2 :] mult = float ( cfg . FPN . RESOLUTION_REQUIREMENT ) new_shape2d = tf . cast ( tf . ceil ( tf . cast ( shape2d , tf . float32 ) / mult ) * mult , tf . int32 ) pad_shape2d = new_shape2d - shape2d assert len ( num_blocks ) == 4 , num_blocks with backbone_scope ( freeze = freeze_at > 0 ): chan = image . shape [ 1 ] pad_base = maybe_reverse_pad ( 2 , 3 ) l = tf . pad ( image , tf . stack ( [[ 0 , 0 ], [ 0 , 0 ], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 0 ]], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 1 ]]])) l . set_shape ([ None , chan , None , None ]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) c5 = resnet_group ( 'group3' , c4 , resnet_bottleneck , 512 , num_blocks [ 3 ], 2 ) # 32x downsampling up to now # size of c5: ceil(input/32) return c2 , c3 , c4 , c5 Functions GroupNorm def GroupNorm ( x , group = 32 , gamma_initializer =< tensorflow . python . ops . init_ops . Constant object at 0x7f049aa24e80 > ) More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. View Source @layer_register ( log_shape = True ) def GroupNorm ( x , group = 32 , gamma_initializer = tf . constant_initializer ( 1. )) : \"\"\" More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. \"\"\" shape = x . get_shape (). as_list () ndims = len ( shape ) assert ndims == 4 , shape chan = shape [ 1 ] assert chan % group == 0 , chan group_size = chan // group orig_shape = tf . shape ( x ) h , w = orig_shape [ 2 ] , orig_shape [ 3 ] x = tf . reshape ( x , tf . stack ( [ -1, group, group_size, h, w ] )) mean , var = tf . nn . moments ( x , [ 2, 3, 4 ] , keep_dims = True ) new_shape = [ 1, group, group_size, 1, 1 ] beta = tf . get_variable ( 'beta' , [ chan ] , initializer = tf . constant_initializer ()) beta = tf . reshape ( beta , new_shape ) gamma = tf . get_variable ( 'gamma' , [ chan ] , initializer = gamma_initializer ) gamma = tf . reshape ( gamma , new_shape ) out = tf . nn . batch_normalization ( x , mean , var , beta , gamma , 1e-5 , name = 'output' ) return tf . reshape ( out , orig_shape , name = 'output' ) backbone_scope def backbone_scope ( freeze ) Args: freeze (bool): whether to freeze all the variables under the scope View Source @ contextmanager def backbone_scope ( freeze ): \"\"\" Args: freeze (bool): whether to freeze all the variables under the scope \"\"\" def nonlin ( x ): x = get_norm ()( x ) return tf . nn . relu ( x ) with argscope ([ Conv2D , MaxPooling , BatchNorm ], data_format = 'channels_first' ), \\ argscope ( Conv2D , use_bias = False , activation = nonlin , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' )), \\ ExitStack () as stack : if cfg . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' ]: if freeze or cfg . BACKBONE . NORM == 'FreezeBN' : stack . enter_context ( argscope ( BatchNorm , training = False )) else : stack . enter_context ( argscope ( BatchNorm , sync_statistics = 'nccl' if cfg . TRAINER == 'replicated' else 'horovod' )) if freeze : stack . enter_context ( freeze_variables ( stop_gradient = False , skip_collection = True )) else : # the layers are not completely freezed, but we may want to only freeze the affine if cfg . BACKBONE . FREEZE_AFFINE : stack . enter_context ( custom_getter_scope ( freeze_affine_getter )) yield freeze_affine_getter def freeze_affine_getter ( getter , * args , ** kwargs ) View Source def freeze_affine_getter ( getter , * args , ** kwargs ): # custom getter to freeze affine params inside bn name = args [ 0 ] if len ( args ) else kwargs . get ( 'name' ) if name . endswith ( '/gamma' ) or name . endswith ( '/beta' ): kwargs [ 'trainable' ] = False ret = getter ( * args , ** kwargs ) tf . add_to_collection ( tf . GraphKeys . MODEL_VARIABLES , ret ) else : ret = getter ( * args , ** kwargs ) return ret get_norm def get_norm ( zero_init = False ) View Source def get_norm ( zero_init = False ): if cfg . BACKBONE . NORM == 'None' : return lambda x : x if cfg . BACKBONE . NORM == 'GN' : Norm = GroupNorm layer_name = 'gn' else : Norm = BatchNorm layer_name = 'bn' return lambda x : Norm ( layer_name , x , gamma_initializer = tf . zeros_initializer () if zero_init else None ) image_preprocess def image_preprocess ( image , bgr = True ) View Source def image_preprocess ( image , bgr = True ) : with tf . name_scope ( 'image_preprocess' ) : if image . dtype . base_dtype ! = tf . float32: image = tf . cast ( image , tf . float32 ) mean = cfg . PREPROC . PIXEL_MEAN std = np . asarray ( cfg . PREPROC . PIXEL_STD ) if bgr : mean = mean [ ::- 1 ] std = std [ ::- 1 ] image_mean = tf . constant ( mean , dtype = tf . float32 ) image_invstd = tf . constant ( 1.0 / std , dtype = tf . float32 ) image = ( image - image_mean ) * image_invstd return image maybe_reverse_pad def maybe_reverse_pad ( topleft , bottomright ) View Source def maybe_reverse_pad ( topleft , bottomright ): if cfg . BACKBONE . TF_PAD_MODE : return [ topleft , bottomright ] return [ bottomright , topleft ] resnet_bottleneck def resnet_bottleneck ( l , ch_out , stride ) View Source def resnet_bottleneck ( l , ch_out , stride ) : shortcut = l if cfg . BACKBONE . STRIDE_1X1 : if stride == 2 : l = l [ : , : , :- 1 , :- 1 ] l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = stride ) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 1 ) else : l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = 1 ) if stride == 2 : l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 2 , padding='VALID' ) else : l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = stride ) if cfg . BACKBONE . NORM ! = 'None': l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = get_norm ( zero_init = True )) else : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = tf . identity , kernel_initializer = tf . constant_initializer ()) ret = l + resnet_shortcut ( shortcut , ch_out * 4 , stride , activation = get_norm ( zero_init = False )) return tf . nn . relu ( ret , name='output' ) resnet_c4_backbone def resnet_c4_backbone ( image , num_blocks ) View Source def resnet_c4_backbone ( image , num_blocks ): assert len ( num_blocks ) == 3 freeze_at = cfg . BACKBONE . FREEZE_AT with backbone_scope ( freeze = freeze_at > 0 ): l = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 2 , 3 ), maybe_reverse_pad ( 2 , 3 )]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) # 16 x downsampling up to now return c4 resnet_conv5 def resnet_conv5 ( image , num_block ) View Source @ auto_reuse_variable_scope def resnet_conv5 ( image , num_block ): with backbone_scope ( freeze = False ): l = resnet_group ( 'group3' , image , resnet_bottleneck , 512 , num_block , 2 ) return l resnet_fpn_backbone def resnet_fpn_backbone ( image , num_blocks ) View Source def resnet_fpn_backbone ( image , num_blocks ): freeze_at = cfg . BACKBONE . FREEZE_AT shape2d = tf . shape ( image )[ 2 :] mult = float ( cfg . FPN . RESOLUTION_REQUIREMENT ) new_shape2d = tf . cast ( tf . ceil ( tf . cast ( shape2d , tf . float32 ) / mult ) * mult , tf . int32 ) pad_shape2d = new_shape2d - shape2d assert len ( num_blocks ) == 4 , num_blocks with backbone_scope ( freeze = freeze_at > 0 ): chan = image . shape [ 1 ] pad_base = maybe_reverse_pad ( 2 , 3 ) l = tf . pad ( image , tf . stack ( [[ 0 , 0 ], [ 0 , 0 ], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 0 ]], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 1 ]]])) l . set_shape ([ None , chan , None , None ]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) c5 = resnet_group ( 'group3' , c4 , resnet_bottleneck , 512 , num_blocks [ 3 ], 2 ) # 32 x downsampling up to now # size of c5 : ceil ( input / 32 ) return c2 , c3 , c4 , c5 resnet_group def resnet_group ( name , l , block_func , features , count , stride ) View Source def resnet_group ( name , l , block_func , features , count , stride ): with tf . variable_scope ( name ): for i in range ( 0 , count ): with tf . variable_scope ( 'block{}' . format ( i )): l = block_func ( l , features , stride if i == 0 else 1 ) return l resnet_shortcut def resnet_shortcut ( l , n_out , stride , activation =< function identity at 0x7f04adadc730 > ) View Source def resnet_shortcut ( l , n_out , stride , activation = tf . identity ) : n_in = l . shape [ 1 ] if n_in ! = n_out: # change dimension when channel is not the same # TF 's SAME mode output ceil(x/stride), which is NOT what we want when x is odd and stride is 2 # In FPN mode, the images are pre-padded already. if not cfg.MODE_FPN and stride == 2: l = l[:, :, :-1, :-1] return Conv2D('convshortcut ' , l , n_out , 1 , strides = stride , activation = activation ) else : return l","title":"Backbone"},{"location":"reference/mot/object_detection/modeling/backbone/#module-motobject_detectionmodelingbackbone","text":"View Source # -*- coding: utf-8 -*- # File: backbone.py import numpy as np import tensorflow as tf from contextlib import ExitStack , contextmanager from tensorpack.models import BatchNorm , Conv2D , MaxPooling , layer_register from tensorpack.tfutils import argscope from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope from tensorpack.tfutils.varreplace import custom_getter_scope , freeze_variables from mot.object_detection.config import config as cfg @layer_register ( log_shape = True ) def GroupNorm ( x , group = 32 , gamma_initializer = tf . constant_initializer ( 1. )): \"\"\" More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. \"\"\" shape = x . get_shape () . as_list () ndims = len ( shape ) assert ndims == 4 , shape chan = shape [ 1 ] assert chan % group == 0 , chan group_size = chan // group orig_shape = tf . shape ( x ) h , w = orig_shape [ 2 ], orig_shape [ 3 ] x = tf . reshape ( x , tf . stack ([ - 1 , group , group_size , h , w ])) mean , var = tf . nn . moments ( x , [ 2 , 3 , 4 ], keep_dims = True ) new_shape = [ 1 , group , group_size , 1 , 1 ] beta = tf . get_variable ( 'beta' , [ chan ], initializer = tf . constant_initializer ()) beta = tf . reshape ( beta , new_shape ) gamma = tf . get_variable ( 'gamma' , [ chan ], initializer = gamma_initializer ) gamma = tf . reshape ( gamma , new_shape ) out = tf . nn . batch_normalization ( x , mean , var , beta , gamma , 1e-5 , name = 'output' ) return tf . reshape ( out , orig_shape , name = 'output' ) def freeze_affine_getter ( getter , * args , ** kwargs ): # custom getter to freeze affine params inside bn name = args [ 0 ] if len ( args ) else kwargs . get ( 'name' ) if name . endswith ( '/gamma' ) or name . endswith ( '/beta' ): kwargs [ 'trainable' ] = False ret = getter ( * args , ** kwargs ) tf . add_to_collection ( tf . GraphKeys . MODEL_VARIABLES , ret ) else : ret = getter ( * args , ** kwargs ) return ret def maybe_reverse_pad ( topleft , bottomright ): if cfg . BACKBONE . TF_PAD_MODE : return [ topleft , bottomright ] return [ bottomright , topleft ] @contextmanager def backbone_scope ( freeze ): \"\"\" Args: freeze (bool): whether to freeze all the variables under the scope \"\"\" def nonlin ( x ): x = get_norm ()( x ) return tf . nn . relu ( x ) with argscope ([ Conv2D , MaxPooling , BatchNorm ], data_format = 'channels_first' ), \\ argscope ( Conv2D , use_bias = False , activation = nonlin , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' )), \\ ExitStack () as stack : if cfg . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' ]: if freeze or cfg . BACKBONE . NORM == 'FreezeBN' : stack . enter_context ( argscope ( BatchNorm , training = False )) else : stack . enter_context ( argscope ( BatchNorm , sync_statistics = 'nccl' if cfg . TRAINER == 'replicated' else 'horovod' )) if freeze : stack . enter_context ( freeze_variables ( stop_gradient = False , skip_collection = True )) else : # the layers are not completely freezed, but we may want to only freeze the affine if cfg . BACKBONE . FREEZE_AFFINE : stack . enter_context ( custom_getter_scope ( freeze_affine_getter )) yield def image_preprocess ( image , bgr = True ): with tf . name_scope ( 'image_preprocess' ): if image . dtype . base_dtype != tf . float32 : image = tf . cast ( image , tf . float32 ) mean = cfg . PREPROC . PIXEL_MEAN std = np . asarray ( cfg . PREPROC . PIXEL_STD ) if bgr : mean = mean [:: - 1 ] std = std [:: - 1 ] image_mean = tf . constant ( mean , dtype = tf . float32 ) image_invstd = tf . constant ( 1.0 / std , dtype = tf . float32 ) image = ( image - image_mean ) * image_invstd return image def get_norm ( zero_init = False ): if cfg . BACKBONE . NORM == 'None' : return lambda x : x if cfg . BACKBONE . NORM == 'GN' : Norm = GroupNorm layer_name = 'gn' else : Norm = BatchNorm layer_name = 'bn' return lambda x : Norm ( layer_name , x , gamma_initializer = tf . zeros_initializer () if zero_init else None ) def resnet_shortcut ( l , n_out , stride , activation = tf . identity ): n_in = l . shape [ 1 ] if n_in != n_out : # change dimension when channel is not the same # TF's SAME mode output ceil(x/stride), which is NOT what we want when x is odd and stride is 2 # In FPN mode, the images are pre-padded already. if not cfg . MODE_FPN and stride == 2 : l = l [:, :, : - 1 , : - 1 ] return Conv2D ( 'convshortcut' , l , n_out , 1 , strides = stride , activation = activation ) else : return l def resnet_bottleneck ( l , ch_out , stride ): shortcut = l if cfg . BACKBONE . STRIDE_1X1 : if stride == 2 : l = l [:, :, : - 1 , : - 1 ] l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = stride ) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 1 ) else : l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = 1 ) if stride == 2 : l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 2 , padding = 'VALID' ) else : l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = stride ) if cfg . BACKBONE . NORM != 'None' : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = get_norm ( zero_init = True )) else : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = tf . identity , kernel_initializer = tf . constant_initializer ()) ret = l + resnet_shortcut ( shortcut , ch_out * 4 , stride , activation = get_norm ( zero_init = False )) return tf . nn . relu ( ret , name = 'output' ) def resnet_group ( name , l , block_func , features , count , stride ): with tf . variable_scope ( name ): for i in range ( 0 , count ): with tf . variable_scope ( 'block {} ' . format ( i )): l = block_func ( l , features , stride if i == 0 else 1 ) return l def resnet_c4_backbone ( image , num_blocks ): assert len ( num_blocks ) == 3 freeze_at = cfg . BACKBONE . FREEZE_AT with backbone_scope ( freeze = freeze_at > 0 ): l = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 2 , 3 ), maybe_reverse_pad ( 2 , 3 )]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) # 16x downsampling up to now return c4 @auto_reuse_variable_scope def resnet_conv5 ( image , num_block ): with backbone_scope ( freeze = False ): l = resnet_group ( 'group3' , image , resnet_bottleneck , 512 , num_block , 2 ) return l def resnet_fpn_backbone ( image , num_blocks ): freeze_at = cfg . BACKBONE . FREEZE_AT shape2d = tf . shape ( image )[ 2 :] mult = float ( cfg . FPN . RESOLUTION_REQUIREMENT ) new_shape2d = tf . cast ( tf . ceil ( tf . cast ( shape2d , tf . float32 ) / mult ) * mult , tf . int32 ) pad_shape2d = new_shape2d - shape2d assert len ( num_blocks ) == 4 , num_blocks with backbone_scope ( freeze = freeze_at > 0 ): chan = image . shape [ 1 ] pad_base = maybe_reverse_pad ( 2 , 3 ) l = tf . pad ( image , tf . stack ( [[ 0 , 0 ], [ 0 , 0 ], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 0 ]], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 1 ]]])) l . set_shape ([ None , chan , None , None ]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) c5 = resnet_group ( 'group3' , c4 , resnet_bottleneck , 512 , num_blocks [ 3 ], 2 ) # 32x downsampling up to now # size of c5: ceil(input/32) return c2 , c3 , c4 , c5","title":"Module mot.object_detection.modeling.backbone"},{"location":"reference/mot/object_detection/modeling/backbone/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/backbone/#groupnorm","text":"def GroupNorm ( x , group = 32 , gamma_initializer =< tensorflow . python . ops . init_ops . Constant object at 0x7f049aa24e80 > ) More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. View Source @layer_register ( log_shape = True ) def GroupNorm ( x , group = 32 , gamma_initializer = tf . constant_initializer ( 1. )) : \"\"\" More code that reproduces the paper can be found at https://github.com/ppwwyyxx/GroupNorm-reproduce/. \"\"\" shape = x . get_shape (). as_list () ndims = len ( shape ) assert ndims == 4 , shape chan = shape [ 1 ] assert chan % group == 0 , chan group_size = chan // group orig_shape = tf . shape ( x ) h , w = orig_shape [ 2 ] , orig_shape [ 3 ] x = tf . reshape ( x , tf . stack ( [ -1, group, group_size, h, w ] )) mean , var = tf . nn . moments ( x , [ 2, 3, 4 ] , keep_dims = True ) new_shape = [ 1, group, group_size, 1, 1 ] beta = tf . get_variable ( 'beta' , [ chan ] , initializer = tf . constant_initializer ()) beta = tf . reshape ( beta , new_shape ) gamma = tf . get_variable ( 'gamma' , [ chan ] , initializer = gamma_initializer ) gamma = tf . reshape ( gamma , new_shape ) out = tf . nn . batch_normalization ( x , mean , var , beta , gamma , 1e-5 , name = 'output' ) return tf . reshape ( out , orig_shape , name = 'output' )","title":"GroupNorm"},{"location":"reference/mot/object_detection/modeling/backbone/#backbone_scope","text":"def backbone_scope ( freeze ) Args: freeze (bool): whether to freeze all the variables under the scope View Source @ contextmanager def backbone_scope ( freeze ): \"\"\" Args: freeze (bool): whether to freeze all the variables under the scope \"\"\" def nonlin ( x ): x = get_norm ()( x ) return tf . nn . relu ( x ) with argscope ([ Conv2D , MaxPooling , BatchNorm ], data_format = 'channels_first' ), \\ argscope ( Conv2D , use_bias = False , activation = nonlin , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' )), \\ ExitStack () as stack : if cfg . BACKBONE . NORM in [ 'FreezeBN' , 'SyncBN' ]: if freeze or cfg . BACKBONE . NORM == 'FreezeBN' : stack . enter_context ( argscope ( BatchNorm , training = False )) else : stack . enter_context ( argscope ( BatchNorm , sync_statistics = 'nccl' if cfg . TRAINER == 'replicated' else 'horovod' )) if freeze : stack . enter_context ( freeze_variables ( stop_gradient = False , skip_collection = True )) else : # the layers are not completely freezed, but we may want to only freeze the affine if cfg . BACKBONE . FREEZE_AFFINE : stack . enter_context ( custom_getter_scope ( freeze_affine_getter )) yield","title":"backbone_scope"},{"location":"reference/mot/object_detection/modeling/backbone/#freeze_affine_getter","text":"def freeze_affine_getter ( getter , * args , ** kwargs ) View Source def freeze_affine_getter ( getter , * args , ** kwargs ): # custom getter to freeze affine params inside bn name = args [ 0 ] if len ( args ) else kwargs . get ( 'name' ) if name . endswith ( '/gamma' ) or name . endswith ( '/beta' ): kwargs [ 'trainable' ] = False ret = getter ( * args , ** kwargs ) tf . add_to_collection ( tf . GraphKeys . MODEL_VARIABLES , ret ) else : ret = getter ( * args , ** kwargs ) return ret","title":"freeze_affine_getter"},{"location":"reference/mot/object_detection/modeling/backbone/#get_norm","text":"def get_norm ( zero_init = False ) View Source def get_norm ( zero_init = False ): if cfg . BACKBONE . NORM == 'None' : return lambda x : x if cfg . BACKBONE . NORM == 'GN' : Norm = GroupNorm layer_name = 'gn' else : Norm = BatchNorm layer_name = 'bn' return lambda x : Norm ( layer_name , x , gamma_initializer = tf . zeros_initializer () if zero_init else None )","title":"get_norm"},{"location":"reference/mot/object_detection/modeling/backbone/#image_preprocess","text":"def image_preprocess ( image , bgr = True ) View Source def image_preprocess ( image , bgr = True ) : with tf . name_scope ( 'image_preprocess' ) : if image . dtype . base_dtype ! = tf . float32: image = tf . cast ( image , tf . float32 ) mean = cfg . PREPROC . PIXEL_MEAN std = np . asarray ( cfg . PREPROC . PIXEL_STD ) if bgr : mean = mean [ ::- 1 ] std = std [ ::- 1 ] image_mean = tf . constant ( mean , dtype = tf . float32 ) image_invstd = tf . constant ( 1.0 / std , dtype = tf . float32 ) image = ( image - image_mean ) * image_invstd return image","title":"image_preprocess"},{"location":"reference/mot/object_detection/modeling/backbone/#maybe_reverse_pad","text":"def maybe_reverse_pad ( topleft , bottomright ) View Source def maybe_reverse_pad ( topleft , bottomright ): if cfg . BACKBONE . TF_PAD_MODE : return [ topleft , bottomright ] return [ bottomright , topleft ]","title":"maybe_reverse_pad"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_bottleneck","text":"def resnet_bottleneck ( l , ch_out , stride ) View Source def resnet_bottleneck ( l , ch_out , stride ) : shortcut = l if cfg . BACKBONE . STRIDE_1X1 : if stride == 2 : l = l [ : , : , :- 1 , :- 1 ] l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = stride ) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 1 ) else : l = Conv2D ( 'conv1' , l , ch_out , 1 , strides = 1 ) if stride == 2 : l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = 2 , padding='VALID' ) else : l = Conv2D ( 'conv2' , l , ch_out , 3 , strides = stride ) if cfg . BACKBONE . NORM ! = 'None': l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = get_norm ( zero_init = True )) else : l = Conv2D ( 'conv3' , l , ch_out * 4 , 1 , activation = tf . identity , kernel_initializer = tf . constant_initializer ()) ret = l + resnet_shortcut ( shortcut , ch_out * 4 , stride , activation = get_norm ( zero_init = False )) return tf . nn . relu ( ret , name='output' )","title":"resnet_bottleneck"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_c4_backbone","text":"def resnet_c4_backbone ( image , num_blocks ) View Source def resnet_c4_backbone ( image , num_blocks ): assert len ( num_blocks ) == 3 freeze_at = cfg . BACKBONE . FREEZE_AT with backbone_scope ( freeze = freeze_at > 0 ): l = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 2 , 3 ), maybe_reverse_pad ( 2 , 3 )]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) # 16 x downsampling up to now return c4","title":"resnet_c4_backbone"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_conv5","text":"def resnet_conv5 ( image , num_block ) View Source @ auto_reuse_variable_scope def resnet_conv5 ( image , num_block ): with backbone_scope ( freeze = False ): l = resnet_group ( 'group3' , image , resnet_bottleneck , 512 , num_block , 2 ) return l","title":"resnet_conv5"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_fpn_backbone","text":"def resnet_fpn_backbone ( image , num_blocks ) View Source def resnet_fpn_backbone ( image , num_blocks ): freeze_at = cfg . BACKBONE . FREEZE_AT shape2d = tf . shape ( image )[ 2 :] mult = float ( cfg . FPN . RESOLUTION_REQUIREMENT ) new_shape2d = tf . cast ( tf . ceil ( tf . cast ( shape2d , tf . float32 ) / mult ) * mult , tf . int32 ) pad_shape2d = new_shape2d - shape2d assert len ( num_blocks ) == 4 , num_blocks with backbone_scope ( freeze = freeze_at > 0 ): chan = image . shape [ 1 ] pad_base = maybe_reverse_pad ( 2 , 3 ) l = tf . pad ( image , tf . stack ( [[ 0 , 0 ], [ 0 , 0 ], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 0 ]], [ pad_base [ 0 ], pad_base [ 1 ] + pad_shape2d [ 1 ]]])) l . set_shape ([ None , chan , None , None ]) l = Conv2D ( 'conv0' , l , 64 , 7 , strides = 2 , padding = 'VALID' ) l = tf . pad ( l , [[ 0 , 0 ], [ 0 , 0 ], maybe_reverse_pad ( 0 , 1 ), maybe_reverse_pad ( 0 , 1 )]) l = MaxPooling ( 'pool0' , l , 3 , strides = 2 , padding = 'VALID' ) with backbone_scope ( freeze = freeze_at > 1 ): c2 = resnet_group ( 'group0' , l , resnet_bottleneck , 64 , num_blocks [ 0 ], 1 ) with backbone_scope ( freeze = False ): c3 = resnet_group ( 'group1' , c2 , resnet_bottleneck , 128 , num_blocks [ 1 ], 2 ) c4 = resnet_group ( 'group2' , c3 , resnet_bottleneck , 256 , num_blocks [ 2 ], 2 ) c5 = resnet_group ( 'group3' , c4 , resnet_bottleneck , 512 , num_blocks [ 3 ], 2 ) # 32 x downsampling up to now # size of c5 : ceil ( input / 32 ) return c2 , c3 , c4 , c5","title":"resnet_fpn_backbone"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_group","text":"def resnet_group ( name , l , block_func , features , count , stride ) View Source def resnet_group ( name , l , block_func , features , count , stride ): with tf . variable_scope ( name ): for i in range ( 0 , count ): with tf . variable_scope ( 'block{}' . format ( i )): l = block_func ( l , features , stride if i == 0 else 1 ) return l","title":"resnet_group"},{"location":"reference/mot/object_detection/modeling/backbone/#resnet_shortcut","text":"def resnet_shortcut ( l , n_out , stride , activation =< function identity at 0x7f04adadc730 > ) View Source def resnet_shortcut ( l , n_out , stride , activation = tf . identity ) : n_in = l . shape [ 1 ] if n_in ! = n_out: # change dimension when channel is not the same # TF 's SAME mode output ceil(x/stride), which is NOT what we want when x is odd and stride is 2 # In FPN mode, the images are pre-padded already. if not cfg.MODE_FPN and stride == 2: l = l[:, :, :-1, :-1] return Conv2D('convshortcut ' , l , n_out , 1 , strides = stride , activation = activation ) else : return l","title":"resnet_shortcut"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/","text":"Module mot.object_detection.modeling.generalized_rcnn View Source # -*- coding: utf-8 -*- # File: import tensorflow as tf from tensorpack import ModelDesc from tensorpack.models import GlobalAvgPooling , l2_regularizer , regularize_cost from tensorpack.tfutils import optimizer from tensorpack.tfutils.summary import add_moving_summary from mot.object_detection.config import config as cfg from mot.object_detection.data import get_all_anchors , get_all_anchors_fpn from mot.object_detection.modeling.model_mrcnn import ( maskrcnn_loss , maskrcnn_upXconv_head , unpackbits_masks ) from mot.object_detection.modeling.model_rpn import ( generate_rpn_proposals , rpn_head , rpn_losses ) from mot.object_detection.modeling.backbone import ( image_preprocess , resnet_c4_backbone , resnet_conv5 , resnet_fpn_backbone ) from mot.object_detection.modeling.model_box import ( RPNAnchors , clip_boxes , crop_and_resize , roi_align ) from mot.object_detection.modeling.model_cascade import CascadeRCNNHead from mot.object_detection.modeling.model_fpn import ( fpn_model , generate_fpn_proposals , multilevel_roi_align , multilevel_rpn_losses ) from mot.object_detection.modeling.model_frcnn import ( BoxProposals , FastRCNNHead , fastrcnn_outputs , fastrcnn_predictions , sample_fast_rcnn_targets ) from mot.object_detection.utils.box_ops import area as tf_area from mot.object_detection.modeling import model_frcnn , model_mrcnn class GeneralizedRCNN ( ModelDesc ): def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ]) def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt def get_inference_tensor_names ( self ): \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes' , 'output/scores' , 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ], out def build_graph ( self , * inputs ): inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ]) # 1CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' )} proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs? targets = [ inputs [ k ] for k in [ 'gt_boxes' , 'gt_labels' , 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ]), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ], 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models, they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names ()[ 1 ]: try : name = '/' . join ([ ns , name ]) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name )) class ResNetC4Model ( GeneralizedRCNN ): def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR ), tf . int32 , 'anchor_labels' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR , 4 ), tf . float32 , 'anchor_boxes' ), tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )] # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) # NR_GT x height x ceil(width/8), packed groundtruth masks return ret def backbone ( self , image ): return [ resnet_c4_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS [: 3 ])] def rpn ( self , image , features , inputs ): featuremap = features [ 0 ] rpn_label_logits , rpn_box_logits = rpn_head ( 'rpn' , featuremap , cfg . RPN . HEAD_DIM , cfg . RPN . NUM_ANCHOR ) anchors = RPNAnchors ( get_all_anchors ( stride = cfg . RPN . ANCHOR_STRIDE , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ), inputs [ 'anchor_labels' ], inputs [ 'anchor_boxes' ]) anchors = anchors . narrow_to ( featuremap ) image_shape2d = tf . shape ( image )[ 2 :] # h,w pred_boxes_decoded = anchors . decode_logits ( rpn_box_logits ) # fHxfWxNAx4, floatbox proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( rpn_label_logits , [ - 1 ]), image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if self . training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if self . training else cfg . RPN . TEST_POST_NMS_TOPK ) if self . training : losses = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), rpn_label_logits , rpn_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w featuremap = features [ 0 ] gt_boxes , gt_labels , * _ = targets if self . training : # sample proposal boxes in training proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) # The boxes to be used to crop RoIs. # Use all proposal boxes in inference boxes_on_featuremap = proposals . boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ) roi_resized = roi_align ( featuremap , boxes_on_featuremap , 14 ) feature_fastrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) # nxcx7x7 # Keep C5 feature to be shared with mask branch feature_gap = GlobalAvgPooling ( 'gap' , feature_fastrcnn , data_format = 'channels_first' ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn' , feature_gap , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss # In training, mask branch shares the same C5 feature. fg_feature = tf . gather ( feature_fastrcnn , proposals . fg_inds ()) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , fg_feature , cfg . DATA . NUM_CATEGORY , num_convs = 0 ) # #fg x #cat x 14x14 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 14 , pad_border = False ) # nfg x 1x14x14 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : roi_resized = roi_align ( featuremap , final_boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ), 14 ) feature_maskrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , feature_maskrcnn , cfg . DATA . NUM_CATEGORY , 0 ) # #result x #cat x 14x14 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx14x14 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return [] class ResNetFPNModel ( GeneralizedRCNN ): def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' )] num_anchors = len ( cfg . RPN . ANCHOR_RATIOS ) for k in range ( len ( cfg . FPN . ANCHOR_STRIDES )): ret . extend ([ tf . TensorSpec (( None , None , num_anchors ), tf . int32 , 'anchor_labels_lvl{}' . format ( k + 2 )), tf . TensorSpec (( None , None , num_anchors , 4 ), tf . float32 , 'anchor_boxes_lvl{}' . format ( k + 2 ))]) ret . extend ([ tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )]) # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) return ret def slice_feature_and_anchors ( self , p23456 , anchors ): for i , stride in enumerate ( cfg . FPN . ANCHOR_STRIDES ): with tf . name_scope ( 'FPN_slice_lvl{}' . format ( i )): anchors [ i ] = anchors [ i ] . narrow_to ( p23456 [ i ]) def backbone ( self , image ): c2345 = resnet_fpn_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS ) p23456 = fpn_model ( 'fpn' , c2345 ) return p23456 def rpn ( self , image , features , inputs ): assert len ( cfg . RPN . ANCHOR_SIZES ) == len ( cfg . FPN . ANCHOR_STRIDES ) image_shape2d = tf . shape ( image )[ 2 :] # h,w all_anchors_fpn = get_all_anchors_fpn ( strides = cfg . FPN . ANCHOR_STRIDES , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ) multilevel_anchors = [ RPNAnchors ( all_anchors_fpn [ i ], inputs [ 'anchor_labels_lvl{}' . format ( i + 2 )], inputs [ 'anchor_boxes_lvl{}' . format ( i + 2 )]) for i in range ( len ( all_anchors_fpn ))] self . slice_feature_and_anchors ( features , multilevel_anchors ) # Multi-Level RPN Proposals rpn_outputs = [ rpn_head ( 'rpn' , pi , cfg . FPN . NUM_CHANNEL , len ( cfg . RPN . ANCHOR_RATIOS )) for pi in features ] multilevel_label_logits = [ k [ 0 ] for k in rpn_outputs ] multilevel_box_logits = [ k [ 1 ] for k in rpn_outputs ] multilevel_pred_boxes = [ anchor . decode_logits ( logits ) for anchor , logits in zip ( multilevel_anchors , multilevel_box_logits )] proposal_boxes , proposal_scores = generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) if self . training : losses = multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w assert len ( features ) == 5 , \"Features have to be P23456!\" gt_boxes , gt_labels , * _ = targets if self . training : proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) fastrcnn_head_func = getattr ( model_frcnn , cfg . FPN . FRCNN_HEAD_FUNC ) if not cfg . FPN . CASCADE : roi_feature_fastrcnn = multilevel_roi_align ( features [: 4 ], proposals . boxes , 7 ) head_feature = fastrcnn_head_func ( 'fastrcnn' , roi_feature_fastrcnn ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn/outputs' , head_feature , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) else : def roi_func ( boxes ): return multilevel_roi_align ( features [: 4 ], boxes , 7 ) fastrcnn_head = CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , ( gt_boxes , gt_labels ), image_shape2d , cfg . DATA . NUM_CATEGORY ) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], proposals . fg_boxes (), 14 , name_scope = 'multilevel_roi_align_mask' ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 28 , pad_border = False ) # fg x 1x28x28 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : # Cascade inference needs roi transform with refined boxes. roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], final_boxes , 14 ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx28x28 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return [] Classes GeneralizedRCNN class GeneralizedRCNN ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method. Ancestors (in MRO) tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase Descendants mot.object_detection.modeling.generalized_rcnn.ResNetC4Model mot.object_detection.modeling.generalized_rcnn.ResNetFPNModel Instance variables input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not. Methods build_graph def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name )) get_inference_tensor_names def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out get_input_signature def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ] get_inputs_desc def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature () get_optimizer def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret inputs def inputs ( self ) Returns a list of :class: tf.TensorSpec or placeholders. A subclass is expected to implement this method. If returning placeholders, the placeholders have to be created inside this method. Don't return placeholders created in other places. Also, you should never call this method by yourself. Returns: list[tf.TensorSpec or tf.placeholder]. To be converted to :class: tf.TensorSpec . View Source def inputs ( self ): \"\"\" Returns a list of :class:`tf.TensorSpec` or placeholders. A subclass is expected to implement this method. If returning placeholders, the placeholders __have to__ be created inside this method. Don't return placeholders created in other places. Also, you should never call this method by yourself. Returns: list[tf.TensorSpec or tf.placeholder]. To be converted to :class:`tf.TensorSpec`. \"\"\" raise NotImplementedError () optimizer def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt preprocess def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ]) ResNetC4Model class ResNetC4Model ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method. Ancestors (in MRO) mot.object_detection.modeling.generalized_rcnn.GeneralizedRCNN tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase Instance variables input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not. Methods backbone def backbone ( self , image ) View Source def backbone ( self , image ): return [ resnet_c4_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS [: 3 ])] build_graph def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name )) get_inference_tensor_names def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out get_input_signature def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ] get_inputs_desc def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature () get_optimizer def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret inputs def inputs ( self ) View Source def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR ), tf . int32 , 'anchor_labels' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR , 4 ), tf . float32 , 'anchor_boxes' ), tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )] # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) # NR_GT x height x ceil ( width / 8 ), packed groundtruth masks return ret optimizer def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt preprocess def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ]) roi_heads def roi_heads ( self , image , features , proposals , targets ) View Source def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w featuremap = features [ 0 ] gt_boxes , gt_labels , * _ = targets if self . training : # sample proposal boxes in training proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) # The boxes to be used to crop RoIs. # Use all proposal boxes in inference boxes_on_featuremap = proposals . boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ) roi_resized = roi_align ( featuremap , boxes_on_featuremap , 14 ) feature_fastrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) # nxcx7x7 # Keep C5 feature to be shared with mask branch feature_gap = GlobalAvgPooling ( 'gap' , feature_fastrcnn , data_format = 'channels_first' ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn' , feature_gap , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss # In training, mask branch shares the same C5 feature. fg_feature = tf . gather ( feature_fastrcnn , proposals . fg_inds ()) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , fg_feature , cfg . DATA . NUM_CATEGORY , num_convs = 0 ) # #fg x #cat x 14x14 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 14 , pad_border = False ) # nfg x 1x14x14 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : roi_resized = roi_align ( featuremap , final_boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ), 14 ) feature_maskrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , feature_maskrcnn , cfg . DATA . NUM_CATEGORY , 0 ) # #result x #cat x 14x14 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx14x14 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return [] rpn def rpn ( self , image , features , inputs ) View Source def rpn ( self , image , features , inputs ): featuremap = features [ 0 ] rpn_label_logits , rpn_box_logits = rpn_head ( 'rpn' , featuremap , cfg . RPN . HEAD_DIM , cfg . RPN . NUM_ANCHOR ) anchors = RPNAnchors ( get_all_anchors ( stride = cfg . RPN . ANCHOR_STRIDE , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ), inputs [ 'anchor_labels' ], inputs [ 'anchor_boxes' ]) anchors = anchors . narrow_to ( featuremap ) image_shape2d = tf . shape ( image )[ 2 :] # h , w pred_boxes_decoded = anchors . decode_logits ( rpn_box_logits ) # fHxfWxNAx4 , floatbox proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( rpn_label_logits , [ - 1 ]), image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if self . training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if self . training else cfg . RPN . TEST_POST_NMS_TOPK ) if self . training : losses = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), rpn_label_logits , rpn_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses ResNetFPNModel class ResNetFPNModel ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method. Ancestors (in MRO) mot.object_detection.modeling.generalized_rcnn.GeneralizedRCNN tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase Instance variables input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not. Methods backbone def backbone ( self , image ) View Source def backbone ( self , image ): c2345 = resnet_fpn_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS ) p23456 = fpn_model ( 'fpn' , c2345 ) return p23456 build_graph def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name )) get_inference_tensor_names def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out get_input_signature def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ] get_inputs_desc def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature () get_optimizer def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret inputs def inputs ( self ) View Source def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' )] num_anchors = len ( cfg . RPN . ANCHOR_RATIOS ) for k in range ( len ( cfg . FPN . ANCHOR_STRIDES )): ret . extend ([ tf . TensorSpec (( None , None , num_anchors ), tf . int32 , 'anchor_labels_lvl{}' . format ( k + 2 )), tf . TensorSpec (( None , None , num_anchors , 4 ), tf . float32 , 'anchor_boxes_lvl{}' . format ( k + 2 ))]) ret . extend ([ tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )]) # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) return ret optimizer def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt preprocess def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ]) roi_heads def roi_heads ( self , image , features , proposals , targets ) View Source def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w assert len ( features ) == 5 , \"Features have to be P23456!\" gt_boxes , gt_labels , * _ = targets if self . training : proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) fastrcnn_head_func = getattr ( model_frcnn , cfg . FPN . FRCNN_HEAD_FUNC ) if not cfg . FPN . CASCADE : roi_feature_fastrcnn = multilevel_roi_align ( features [: 4 ], proposals . boxes , 7 ) head_feature = fastrcnn_head_func ( 'fastrcnn' , roi_feature_fastrcnn ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn/outputs' , head_feature , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) else : def roi_func ( boxes ): return multilevel_roi_align ( features [: 4 ], boxes , 7 ) fastrcnn_head = CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , ( gt_boxes , gt_labels ), image_shape2d , cfg . DATA . NUM_CATEGORY ) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], proposals . fg_boxes (), 14 , name_scope = 'multilevel_roi_align_mask' ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 28 , pad_border = False ) # fg x 1x28x28 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : # Cascade inference needs roi transform with refined boxes. roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], final_boxes , 14 ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx28x28 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return [] rpn def rpn ( self , image , features , inputs ) View Source def rpn ( self , image , features , inputs ) : assert len ( cfg . RPN . ANCHOR_SIZES ) == len ( cfg . FPN . ANCHOR_STRIDES ) image_shape2d = tf . shape ( image ) [ 2: ] # h , w all_anchors_fpn = get_all_anchors_fpn ( strides = cfg . FPN . ANCHOR_STRIDES , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ) multilevel_anchors = [ RPNAnchors( all_anchors_fpn[i ] , inputs [ 'anchor_labels_lvl{}'.format(i + 2) ] , inputs [ 'anchor_boxes_lvl{}'.format(i + 2) ] ) for i in range ( len ( all_anchors_fpn )) ] self . slice_feature_and_anchors ( features , multilevel_anchors ) # Multi - Level RPN Proposals rpn_outputs = [ rpn_head('rpn', pi, cfg.FPN.NUM_CHANNEL, len(cfg.RPN.ANCHOR_RATIOS)) for pi in features ] multilevel_label_logits = [ k[0 ] for k in rpn_outputs ] multilevel_box_logits = [ k[1 ] for k in rpn_outputs ] multilevel_pred_boxes = [ anchor.decode_logits(logits) for anchor, logits in zip(multilevel_anchors, multilevel_box_logits) ] proposal_boxes , proposal_scores = generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) if self . training : losses = multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses slice_feature_and_anchors def slice_feature_and_anchors ( self , p23456 , anchors ) View Source def slice_feature_and_anchors ( self , p23456 , anchors ) : for i , stride in enumerate ( cfg . FPN . ANCHOR_STRIDES ) : with tf . name_scope ( 'FPN_slice_lvl{}' . format ( i )) : anchors [ i ] = anchors [ i ] . narrow_to ( p23456 [ i ] )","title":"Generalized Rcnn"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#module-motobject_detectionmodelinggeneralized_rcnn","text":"View Source # -*- coding: utf-8 -*- # File: import tensorflow as tf from tensorpack import ModelDesc from tensorpack.models import GlobalAvgPooling , l2_regularizer , regularize_cost from tensorpack.tfutils import optimizer from tensorpack.tfutils.summary import add_moving_summary from mot.object_detection.config import config as cfg from mot.object_detection.data import get_all_anchors , get_all_anchors_fpn from mot.object_detection.modeling.model_mrcnn import ( maskrcnn_loss , maskrcnn_upXconv_head , unpackbits_masks ) from mot.object_detection.modeling.model_rpn import ( generate_rpn_proposals , rpn_head , rpn_losses ) from mot.object_detection.modeling.backbone import ( image_preprocess , resnet_c4_backbone , resnet_conv5 , resnet_fpn_backbone ) from mot.object_detection.modeling.model_box import ( RPNAnchors , clip_boxes , crop_and_resize , roi_align ) from mot.object_detection.modeling.model_cascade import CascadeRCNNHead from mot.object_detection.modeling.model_fpn import ( fpn_model , generate_fpn_proposals , multilevel_roi_align , multilevel_rpn_losses ) from mot.object_detection.modeling.model_frcnn import ( BoxProposals , FastRCNNHead , fastrcnn_outputs , fastrcnn_predictions , sample_fast_rcnn_targets ) from mot.object_detection.utils.box_ops import area as tf_area from mot.object_detection.modeling import model_frcnn , model_mrcnn class GeneralizedRCNN ( ModelDesc ): def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ]) def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt def get_inference_tensor_names ( self ): \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes' , 'output/scores' , 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ], out def build_graph ( self , * inputs ): inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ]) # 1CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' )} proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs? targets = [ inputs [ k ] for k in [ 'gt_boxes' , 'gt_labels' , 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ]), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ], 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models, they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names ()[ 1 ]: try : name = '/' . join ([ ns , name ]) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name )) class ResNetC4Model ( GeneralizedRCNN ): def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR ), tf . int32 , 'anchor_labels' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR , 4 ), tf . float32 , 'anchor_boxes' ), tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )] # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) # NR_GT x height x ceil(width/8), packed groundtruth masks return ret def backbone ( self , image ): return [ resnet_c4_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS [: 3 ])] def rpn ( self , image , features , inputs ): featuremap = features [ 0 ] rpn_label_logits , rpn_box_logits = rpn_head ( 'rpn' , featuremap , cfg . RPN . HEAD_DIM , cfg . RPN . NUM_ANCHOR ) anchors = RPNAnchors ( get_all_anchors ( stride = cfg . RPN . ANCHOR_STRIDE , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ), inputs [ 'anchor_labels' ], inputs [ 'anchor_boxes' ]) anchors = anchors . narrow_to ( featuremap ) image_shape2d = tf . shape ( image )[ 2 :] # h,w pred_boxes_decoded = anchors . decode_logits ( rpn_box_logits ) # fHxfWxNAx4, floatbox proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( rpn_label_logits , [ - 1 ]), image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if self . training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if self . training else cfg . RPN . TEST_POST_NMS_TOPK ) if self . training : losses = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), rpn_label_logits , rpn_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w featuremap = features [ 0 ] gt_boxes , gt_labels , * _ = targets if self . training : # sample proposal boxes in training proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) # The boxes to be used to crop RoIs. # Use all proposal boxes in inference boxes_on_featuremap = proposals . boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ) roi_resized = roi_align ( featuremap , boxes_on_featuremap , 14 ) feature_fastrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) # nxcx7x7 # Keep C5 feature to be shared with mask branch feature_gap = GlobalAvgPooling ( 'gap' , feature_fastrcnn , data_format = 'channels_first' ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn' , feature_gap , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss # In training, mask branch shares the same C5 feature. fg_feature = tf . gather ( feature_fastrcnn , proposals . fg_inds ()) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , fg_feature , cfg . DATA . NUM_CATEGORY , num_convs = 0 ) # #fg x #cat x 14x14 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 14 , pad_border = False ) # nfg x 1x14x14 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : roi_resized = roi_align ( featuremap , final_boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ), 14 ) feature_maskrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , feature_maskrcnn , cfg . DATA . NUM_CATEGORY , 0 ) # #result x #cat x 14x14 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx14x14 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return [] class ResNetFPNModel ( GeneralizedRCNN ): def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' )] num_anchors = len ( cfg . RPN . ANCHOR_RATIOS ) for k in range ( len ( cfg . FPN . ANCHOR_STRIDES )): ret . extend ([ tf . TensorSpec (( None , None , num_anchors ), tf . int32 , 'anchor_labels_lvl{}' . format ( k + 2 )), tf . TensorSpec (( None , None , num_anchors , 4 ), tf . float32 , 'anchor_boxes_lvl{}' . format ( k + 2 ))]) ret . extend ([ tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )]) # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) return ret def slice_feature_and_anchors ( self , p23456 , anchors ): for i , stride in enumerate ( cfg . FPN . ANCHOR_STRIDES ): with tf . name_scope ( 'FPN_slice_lvl{}' . format ( i )): anchors [ i ] = anchors [ i ] . narrow_to ( p23456 [ i ]) def backbone ( self , image ): c2345 = resnet_fpn_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS ) p23456 = fpn_model ( 'fpn' , c2345 ) return p23456 def rpn ( self , image , features , inputs ): assert len ( cfg . RPN . ANCHOR_SIZES ) == len ( cfg . FPN . ANCHOR_STRIDES ) image_shape2d = tf . shape ( image )[ 2 :] # h,w all_anchors_fpn = get_all_anchors_fpn ( strides = cfg . FPN . ANCHOR_STRIDES , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ) multilevel_anchors = [ RPNAnchors ( all_anchors_fpn [ i ], inputs [ 'anchor_labels_lvl{}' . format ( i + 2 )], inputs [ 'anchor_boxes_lvl{}' . format ( i + 2 )]) for i in range ( len ( all_anchors_fpn ))] self . slice_feature_and_anchors ( features , multilevel_anchors ) # Multi-Level RPN Proposals rpn_outputs = [ rpn_head ( 'rpn' , pi , cfg . FPN . NUM_CHANNEL , len ( cfg . RPN . ANCHOR_RATIOS )) for pi in features ] multilevel_label_logits = [ k [ 0 ] for k in rpn_outputs ] multilevel_box_logits = [ k [ 1 ] for k in rpn_outputs ] multilevel_pred_boxes = [ anchor . decode_logits ( logits ) for anchor , logits in zip ( multilevel_anchors , multilevel_box_logits )] proposal_boxes , proposal_scores = generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) if self . training : losses = multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w assert len ( features ) == 5 , \"Features have to be P23456!\" gt_boxes , gt_labels , * _ = targets if self . training : proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) fastrcnn_head_func = getattr ( model_frcnn , cfg . FPN . FRCNN_HEAD_FUNC ) if not cfg . FPN . CASCADE : roi_feature_fastrcnn = multilevel_roi_align ( features [: 4 ], proposals . boxes , 7 ) head_feature = fastrcnn_head_func ( 'fastrcnn' , roi_feature_fastrcnn ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn/outputs' , head_feature , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) else : def roi_func ( boxes ): return multilevel_roi_align ( features [: 4 ], boxes , 7 ) fastrcnn_head = CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , ( gt_boxes , gt_labels ), image_shape2d , cfg . DATA . NUM_CATEGORY ) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], proposals . fg_boxes (), 14 , name_scope = 'multilevel_roi_align_mask' ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 28 , pad_border = False ) # fg x 1x28x28 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : # Cascade inference needs roi transform with refined boxes. roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], final_boxes , 14 ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx28x28 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return []","title":"Module mot.object_detection.modeling.generalized_rcnn"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#generalizedrcnn","text":"class GeneralizedRCNN ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method.","title":"GeneralizedRCNN"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#ancestors-in-mro","text":"tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#descendants","text":"mot.object_detection.modeling.generalized_rcnn.ResNetC4Model mot.object_detection.modeling.generalized_rcnn.ResNetFPNModel","title":"Descendants"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#instance-variables","text":"input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not.","title":"Instance variables"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#build_graph","text":"def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name ))","title":"build_graph"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inference_tensor_names","text":"def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out","title":"get_inference_tensor_names"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_input_signature","text":"def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ]","title":"get_input_signature"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inputs_desc","text":"def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature ()","title":"get_inputs_desc"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_optimizer","text":"def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret","title":"get_optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#inputs","text":"def inputs ( self ) Returns a list of :class: tf.TensorSpec or placeholders. A subclass is expected to implement this method. If returning placeholders, the placeholders have to be created inside this method. Don't return placeholders created in other places. Also, you should never call this method by yourself. Returns: list[tf.TensorSpec or tf.placeholder]. To be converted to :class: tf.TensorSpec . View Source def inputs ( self ): \"\"\" Returns a list of :class:`tf.TensorSpec` or placeholders. A subclass is expected to implement this method. If returning placeholders, the placeholders __have to__ be created inside this method. Don't return placeholders created in other places. Also, you should never call this method by yourself. Returns: list[tf.TensorSpec or tf.placeholder]. To be converted to :class:`tf.TensorSpec`. \"\"\" raise NotImplementedError ()","title":"inputs"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#optimizer","text":"def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt","title":"optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#preprocess","text":"def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ])","title":"preprocess"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#resnetc4model","text":"class ResNetC4Model ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method.","title":"ResNetC4Model"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#ancestors-in-mro_1","text":"mot.object_detection.modeling.generalized_rcnn.GeneralizedRCNN tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#instance-variables_1","text":"input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not.","title":"Instance variables"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#backbone","text":"def backbone ( self , image ) View Source def backbone ( self , image ): return [ resnet_c4_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS [: 3 ])]","title":"backbone"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#build_graph_1","text":"def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name ))","title":"build_graph"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inference_tensor_names_1","text":"def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out","title":"get_inference_tensor_names"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_input_signature_1","text":"def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ]","title":"get_input_signature"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inputs_desc_1","text":"def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature ()","title":"get_inputs_desc"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_optimizer_1","text":"def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret","title":"get_optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#inputs_1","text":"def inputs ( self ) View Source def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR ), tf . int32 , 'anchor_labels' ), tf . TensorSpec (( None , None , cfg . RPN . NUM_ANCHOR , 4 ), tf . float32 , 'anchor_boxes' ), tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )] # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) # NR_GT x height x ceil ( width / 8 ), packed groundtruth masks return ret","title":"inputs"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#optimizer_1","text":"def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt","title":"optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#preprocess_1","text":"def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ])","title":"preprocess"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#roi_heads","text":"def roi_heads ( self , image , features , proposals , targets ) View Source def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w featuremap = features [ 0 ] gt_boxes , gt_labels , * _ = targets if self . training : # sample proposal boxes in training proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) # The boxes to be used to crop RoIs. # Use all proposal boxes in inference boxes_on_featuremap = proposals . boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ) roi_resized = roi_align ( featuremap , boxes_on_featuremap , 14 ) feature_fastrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) # nxcx7x7 # Keep C5 feature to be shared with mask branch feature_gap = GlobalAvgPooling ( 'gap' , feature_fastrcnn , data_format = 'channels_first' ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn' , feature_gap , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss # In training, mask branch shares the same C5 feature. fg_feature = tf . gather ( feature_fastrcnn , proposals . fg_inds ()) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , fg_feature , cfg . DATA . NUM_CATEGORY , num_convs = 0 ) # #fg x #cat x 14x14 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 14 , pad_border = False ) # nfg x 1x14x14 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : roi_resized = roi_align ( featuremap , final_boxes * ( 1.0 / cfg . RPN . ANCHOR_STRIDE ), 14 ) feature_maskrcnn = resnet_conv5 ( roi_resized , cfg . BACKBONE . RESNET_NUM_BLOCKS [ - 1 ]) mask_logits = maskrcnn_upXconv_head ( 'maskrcnn' , feature_maskrcnn , cfg . DATA . NUM_CATEGORY , 0 ) # #result x #cat x 14x14 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx14x14 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return []","title":"roi_heads"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#rpn","text":"def rpn ( self , image , features , inputs ) View Source def rpn ( self , image , features , inputs ): featuremap = features [ 0 ] rpn_label_logits , rpn_box_logits = rpn_head ( 'rpn' , featuremap , cfg . RPN . HEAD_DIM , cfg . RPN . NUM_ANCHOR ) anchors = RPNAnchors ( get_all_anchors ( stride = cfg . RPN . ANCHOR_STRIDE , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ), inputs [ 'anchor_labels' ], inputs [ 'anchor_boxes' ]) anchors = anchors . narrow_to ( featuremap ) image_shape2d = tf . shape ( image )[ 2 :] # h , w pred_boxes_decoded = anchors . decode_logits ( rpn_box_logits ) # fHxfWxNAx4 , floatbox proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( rpn_label_logits , [ - 1 ]), image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if self . training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if self . training else cfg . RPN . TEST_POST_NMS_TOPK ) if self . training : losses = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), rpn_label_logits , rpn_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses","title":"rpn"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#resnetfpnmodel","text":"class ResNetFPNModel ( / , * args , ** kwargs ) A ModelDesc with single cost and single optimizer . It has the following constraints in addition to :class: ModelDescBase : :meth: build_graph(...) method should return a cost when called under a training context. The cost will be the final cost to be optimized by the optimizer. Therefore it should include necessary regularization. Subclass is expected to implement :meth: optimizer() method.","title":"ResNetFPNModel"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#ancestors-in-mro_2","text":"mot.object_detection.modeling.generalized_rcnn.GeneralizedRCNN tensorpack.graph_builder.model_desc.ModelDesc tensorpack.graph_builder.model_desc.ModelDescBase","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#instance-variables_2","text":"input_names Returns: [str]: the names of all the inputs. training Returns: bool: whether the caller is under a training context or not.","title":"Instance variables"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#methods_2","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#backbone_1","text":"def backbone ( self , image ) View Source def backbone ( self , image ): c2345 = resnet_fpn_backbone ( image , cfg . BACKBONE . RESNET_NUM_BLOCKS ) p23456 = fpn_model ( 'fpn' , c2345 ) return p23456","title":"backbone"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#build_graph_2","text":"def build_graph ( self , * inputs ) View Source def build_graph ( self , * inputs ) : inputs = dict ( zip ( self . input_names , inputs )) if \"gt_masks_packed\" in inputs : gt_masks = tf . cast ( unpackbits_masks ( inputs . pop ( \"gt_masks_packed\" )), tf . uint8 , name = \"gt_masks\" ) inputs [ \"gt_masks\" ] = gt_masks image = self . preprocess ( inputs [ 'image' ] ) # 1 CHW features = self . backbone ( image ) anchor_inputs = { k : v for k , v in inputs . items () if k . startswith ( 'anchor_' ) } proposals , rpn_losses = self . rpn ( image , features , anchor_inputs ) # inputs ? targets = [ inputs[k ] for k in [ 'gt_boxes', 'gt_labels', 'gt_masks' ] if k in inputs ] gt_boxes_area = tf . reduce_mean ( tf_area ( inputs [ \"gt_boxes\" ] ), name = 'mean_gt_box_area' ) add_moving_summary ( gt_boxes_area ) head_losses = self . roi_heads ( image , features , proposals , targets ) if self . training : wd_cost = regularize_cost ( '.*/W' , l2_regularizer ( cfg . TRAIN . WEIGHT_DECAY ), name = 'wd_cost' ) total_cost = tf . add_n ( rpn_losses + head_losses + [ wd_cost ] , 'total_cost' ) add_moving_summary ( total_cost , wd_cost ) return total_cost else : # Check that the model defines the tensors it declares for inference # For existing models , they are defined in \"fastrcnn_predictions(name_scope='output')\" G = tf . get_default_graph () ns = G . get_name_scope () for name in self . get_inference_tensor_names () [ 1 ] : try : name = '/' . join ( [ ns, name ] ) if ns else name G . get_tensor_by_name ( name + ':0' ) except KeyError : raise KeyError ( \"Your model does not define the tensor '{}' in inference context.\" . format ( name ))","title":"build_graph"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inference_tensor_names_2","text":"def get_inference_tensor_names ( self ) Returns two lists of tensor names to be used to create an inference callable. build_graph must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names View Source def get_inference_tensor_names ( self ) : \"\"\" Returns two lists of tensor names to be used to create an inference callable. `build_graph` must create tensors of these names when called under inference context. Returns: [str]: input names [str]: output names \"\"\" out = [ 'output/boxes', 'output/scores', 'output/labels' ] if cfg . MODE_MASK : out . append ( 'output/masks' ) return [ 'image' ] , out","title":"get_inference_tensor_names"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_input_signature_2","text":"def get_input_signature ( self ) Returns: A list of :class: tf.TensorSpec , which describes the inputs of this model. The result is cached for each instance of :class: ModelDescBase . View Source @memoized_method def get_input_signature ( self ) : \" \"\" Returns: A list of :class:`tf.TensorSpec`, which describes the inputs of this model. The result is cached for each instance of :class:`ModelDescBase`. \"\" \" with tf . Graph (). as_default () as G : # create these placeholder in a temporary graph inputs = self . inputs () assert isinstance ( inputs , ( list , tuple )), \\ \"ModelDesc.inputs() should return a list of tf.TensorSpec objects! Got {} instead.\" . format ( str ( inputs )) if isinstance ( inputs [ 0 ] , tf . Tensor ) : for p in inputs : assert \"Placeholder\" in p . op . type , \\ \"inputs() have to return TensorSpec or placeholders! Found {} instead.\" . format ( p ) assert p . graph == G , \"Placeholders returned by inputs() should be created inside inputs()!\" return [ TensorSpec ( shape = p . shape , dtype = p . dtype , name = get_op_tensor_name ( p . name ) [ 0 ] ) for p in inputs ]","title":"get_input_signature"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_inputs_desc_2","text":"def get_inputs_desc ( self ) View Source @memoized_method def get_inputs_desc ( self ) : # TODO mark deprecated return self . get_input_signature ()","title":"get_inputs_desc"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#get_optimizer_2","text":"def get_optimizer ( self ) Return the memoized optimizer returned by optimizer() . Users of :class: ModelDesc will need to implement optimizer() , which will only be called once per each model. Returns: a :class: tf.train.Optimizer instance. View Source @memoized_method def get_optimizer ( self ) : \" \"\" Return the memoized optimizer returned by `optimizer()`. Users of :class:`ModelDesc` will need to implement `optimizer()`, which will only be called once per each model. Returns: a :class:`tf.train.Optimizer` instance. \"\" \" ret = self . optimizer () assert isinstance ( ret , tfv1 . train . Optimizer ), \\ \"ModelDesc.optimizer() must return a tf.train.Optimizer! Got {} instead.\" . format ( str ( ret )) return ret","title":"get_optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#inputs_2","text":"def inputs ( self ) View Source def inputs ( self ): ret = [ tf . TensorSpec (( None , None , 3 ), tf . float32 , 'image' )] num_anchors = len ( cfg . RPN . ANCHOR_RATIOS ) for k in range ( len ( cfg . FPN . ANCHOR_STRIDES )): ret . extend ([ tf . TensorSpec (( None , None , num_anchors ), tf . int32 , 'anchor_labels_lvl{}' . format ( k + 2 )), tf . TensorSpec (( None , None , num_anchors , 4 ), tf . float32 , 'anchor_boxes_lvl{}' . format ( k + 2 ))]) ret . extend ([ tf . TensorSpec (( None , 4 ), tf . float32 , 'gt_boxes' ), tf . TensorSpec (( None ,), tf . int64 , 'gt_labels' )]) # all > 0 if cfg . MODE_MASK : ret . append ( tf . TensorSpec (( None , None , None ), tf . uint8 , 'gt_masks_packed' ) ) return ret","title":"inputs"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#optimizer_2","text":"def optimizer ( self ) View Source def optimizer ( self ): lr = tf . get_variable ( 'learning_rate' , initializer = 0.003 , trainable = False ) tf . summary . scalar ( 'learning_rate-summary' , lr ) # The learning rate in the config is set for 8 GPUs, and we use trainers with average=False. lr = lr / 8. opt = tf . train . MomentumOptimizer ( lr , 0.9 ) if cfg . TRAIN . NUM_GPUS < 8 : opt = optimizer . AccumGradOptimizer ( opt , 8 // cfg . TRAIN . NUM_GPUS ) return opt","title":"optimizer"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#preprocess_2","text":"def preprocess ( self , image ) View Source def preprocess ( self , image ): image = tf . expand_dims ( image , 0 ) image = image_preprocess ( image , bgr = True ) return tf . transpose ( image , [ 0 , 3 , 1 , 2 ])","title":"preprocess"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#roi_heads_1","text":"def roi_heads ( self , image , features , proposals , targets ) View Source def roi_heads ( self , image , features , proposals , targets ): image_shape2d = tf . shape ( image )[ 2 :] # h,w assert len ( features ) == 5 , \"Features have to be P23456!\" gt_boxes , gt_labels , * _ = targets if self . training : proposals = sample_fast_rcnn_targets ( proposals . boxes , gt_boxes , gt_labels ) fastrcnn_head_func = getattr ( model_frcnn , cfg . FPN . FRCNN_HEAD_FUNC ) if not cfg . FPN . CASCADE : roi_feature_fastrcnn = multilevel_roi_align ( features [: 4 ], proposals . boxes , 7 ) head_feature = fastrcnn_head_func ( 'fastrcnn' , roi_feature_fastrcnn ) fastrcnn_label_logits , fastrcnn_box_logits = fastrcnn_outputs ( 'fastrcnn/outputs' , head_feature , cfg . DATA . NUM_CATEGORY ) fastrcnn_head = FastRCNNHead ( proposals , fastrcnn_box_logits , fastrcnn_label_logits , gt_boxes , tf . constant ( cfg . FRCNN . BBOX_REG_WEIGHTS , dtype = tf . float32 )) else : def roi_func ( boxes ): return multilevel_roi_align ( features [: 4 ], boxes , 7 ) fastrcnn_head = CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , ( gt_boxes , gt_labels ), image_shape2d , cfg . DATA . NUM_CATEGORY ) if self . training : all_losses = fastrcnn_head . losses () if cfg . MODE_MASK : gt_masks = targets [ 2 ] # maskrcnn loss roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], proposals . fg_boxes (), 14 , name_scope = 'multilevel_roi_align_mask' ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 target_masks_for_fg = crop_and_resize ( tf . expand_dims ( gt_masks , 1 ), proposals . fg_boxes (), proposals . fg_inds_wrt_gt , 28 , pad_border = False ) # fg x 1x28x28 target_masks_for_fg = tf . squeeze ( target_masks_for_fg , 1 , 'sampled_fg_mask_targets' ) all_losses . append ( maskrcnn_loss ( mask_logits , proposals . fg_labels (), target_masks_for_fg )) return all_losses else : decoded_boxes = fastrcnn_head . decoded_output_boxes () decoded_boxes = clip_boxes ( decoded_boxes , image_shape2d , name = 'fastrcnn_all_boxes' ) label_scores = fastrcnn_head . output_scores ( name = 'fastrcnn_all_scores' ) final_boxes , final_scores , final_labels = fastrcnn_predictions ( decoded_boxes , label_scores , name_scope = 'output' ) if cfg . MODE_MASK : # Cascade inference needs roi transform with refined boxes. roi_feature_maskrcnn = multilevel_roi_align ( features [: 4 ], final_boxes , 14 ) maskrcnn_head_func = getattr ( model_mrcnn , cfg . FPN . MRCNN_HEAD_FUNC ) mask_logits = maskrcnn_head_func ( 'maskrcnn' , roi_feature_maskrcnn , cfg . DATA . NUM_CATEGORY ) # #fg x #cat x 28 x 28 indices = tf . stack ([ tf . range ( tf . size ( final_labels )), tf . cast ( final_labels , tf . int32 ) - 1 ], axis = 1 ) final_mask_logits = tf . gather_nd ( mask_logits , indices ) # #resultx28x28 tf . sigmoid ( final_mask_logits , name = 'output/masks' ) return []","title":"roi_heads"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#rpn_1","text":"def rpn ( self , image , features , inputs ) View Source def rpn ( self , image , features , inputs ) : assert len ( cfg . RPN . ANCHOR_SIZES ) == len ( cfg . FPN . ANCHOR_STRIDES ) image_shape2d = tf . shape ( image ) [ 2: ] # h , w all_anchors_fpn = get_all_anchors_fpn ( strides = cfg . FPN . ANCHOR_STRIDES , sizes = cfg . RPN . ANCHOR_SIZES , ratios = cfg . RPN . ANCHOR_RATIOS , max_size = cfg . PREPROC . MAX_SIZE ) multilevel_anchors = [ RPNAnchors( all_anchors_fpn[i ] , inputs [ 'anchor_labels_lvl{}'.format(i + 2) ] , inputs [ 'anchor_boxes_lvl{}'.format(i + 2) ] ) for i in range ( len ( all_anchors_fpn )) ] self . slice_feature_and_anchors ( features , multilevel_anchors ) # Multi - Level RPN Proposals rpn_outputs = [ rpn_head('rpn', pi, cfg.FPN.NUM_CHANNEL, len(cfg.RPN.ANCHOR_RATIOS)) for pi in features ] multilevel_label_logits = [ k[0 ] for k in rpn_outputs ] multilevel_box_logits = [ k[1 ] for k in rpn_outputs ] multilevel_pred_boxes = [ anchor.decode_logits(logits) for anchor, logits in zip(multilevel_anchors, multilevel_box_logits) ] proposal_boxes , proposal_scores = generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) if self . training : losses = multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) else : losses = [] return BoxProposals ( proposal_boxes ), losses","title":"rpn"},{"location":"reference/mot/object_detection/modeling/generalized_rcnn/#slice_feature_and_anchors","text":"def slice_feature_and_anchors ( self , p23456 , anchors ) View Source def slice_feature_and_anchors ( self , p23456 , anchors ) : for i , stride in enumerate ( cfg . FPN . ANCHOR_STRIDES ) : with tf . name_scope ( 'FPN_slice_lvl{}' . format ( i )) : anchors [ i ] = anchors [ i ] . narrow_to ( p23456 [ i ] )","title":"slice_feature_and_anchors"},{"location":"reference/mot/object_detection/modeling/model_box/","text":"Module mot.object_detection.modeling.model_box View Source # -*- coding: utf-8 -*- # File: model_box.py import numpy as np import tensorflow as tf from collections import namedtuple from tensorpack.tfutils.scope_utils import under_name_scope from mot.object_detection.config import config @under_name_scope () def clip_boxes ( boxes , window , name = None ): \"\"\" Args: boxes: nx4, xyxy window: [h, w] \"\"\" boxes = tf . maximum ( boxes , 0.0 ) m = tf . tile ( tf . reverse ( window , [ 0 ]), [ 2 ]) # (4,) boxes = tf . minimum ( boxes , tf . cast ( m , tf . float32 ), name = name ) return boxes @under_name_scope () def decode_bbox_target ( box_predictions , anchors ): \"\"\" Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. \"\"\" orig_shape = tf . shape ( anchors ) box_pred_txtytwth = tf . reshape ( box_predictions , ( - 1 , 2 , 2 )) box_pred_txty , box_pred_twth = tf . split ( box_pred_txtytwth , 2 , axis = 1 ) # each is (...)x1x2 anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 clip = np . log ( config . PREPROC . MAX_SIZE / 16. ) wbhb = tf . exp ( tf . minimum ( box_pred_twth , clip )) * waha xbyb = box_pred_txty * waha + xaya x1y1 = xbyb - wbhb * 0.5 x2y2 = xbyb + wbhb * 0.5 # (...)x1x2 out = tf . concat ([ x1y1 , x2y2 ], axis =- 2 ) return tf . reshape ( out , orig_shape ) @under_name_scope () def encode_bbox_target ( boxes , anchors ): \"\"\" Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. \"\"\" anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 boxes_x1y1x2y2 = tf . reshape ( boxes , ( - 1 , 2 , 2 )) boxes_x1y1 , boxes_x2y2 = tf . split ( boxes_x1y1x2y2 , 2 , axis = 1 ) wbhb = boxes_x2y2 - boxes_x1y1 xbyb = ( boxes_x2y2 + boxes_x1y1 ) * 0.5 # Note that here not all boxes are valid. Some may be zero txty = ( xbyb - xaya ) / waha twth = tf . log ( wbhb / waha ) # may contain -inf for invalid boxes encoded = tf . concat ([ txty , twth ], axis = 1 ) # (-1x2x2) return tf . reshape ( encoded , tf . shape ( boxes )) @under_name_scope () def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ): \"\"\" Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size \"\"\" assert isinstance ( crop_size , int ), crop_size boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border if pad_border : # this can be quite slow image = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 @under_name_scope () def transform_fpcoor_for_tf ( boxes , image_shape , crop_shape ): \"\"\" The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize Returns: y1x1y2x2 \"\"\" x0 , y0 , x1 , y1 = tf . split ( boxes , 4 , axis = 1 ) spacing_w = ( x1 - x0 ) / tf . cast ( crop_shape [ 1 ], tf . float32 ) spacing_h = ( y1 - y0 ) / tf . cast ( crop_shape [ 0 ], tf . float32 ) imshape = [ tf . cast ( image_shape [ 0 ] - 1 , tf . float32 ), tf . cast ( image_shape [ 1 ] - 1 , tf . float32 )] nx0 = ( x0 + spacing_w / 2 - 0.5 ) / imshape [ 1 ] ny0 = ( y0 + spacing_h / 2 - 0.5 ) / imshape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , tf . float32 ) / imshape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , tf . float32 ) / imshape [ 0 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis = 1 ) image_shape = tf . shape ( image )[ 2 :] boxes = transform_fpcoor_for_tf ( boxes , image_shape , [ crop_size , crop_size ]) image = tf . transpose ( image , [ 0 , 2 , 3 , 1 ]) # nhwc ret = tf . image . crop_and_resize ( image , boxes , tf . cast ( box_ind , tf . int32 ), crop_size = [ crop_size , crop_size ]) ret = tf . transpose ( ret , [ 0 , 3 , 1 , 2 ]) # ncss return ret @under_name_scope () def roi_align ( featuremap , boxes , resolution ): \"\"\" Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res \"\"\" # sample 4 locations per roi bin ret = crop_and_resize ( featuremap , boxes , tf . zeros ([ tf . shape ( boxes )[ 0 ]], dtype = tf . int32 ), resolution * 2 ) try : avgpool = tf . nn . avg_pool2d except AttributeError : avgpool = tf . nn . avg_pool ret = avgpool ( ret , [ 1 , 1 , 2 , 2 ], [ 1 , 1 , 2 , 2 ], padding = 'SAME' , data_format = 'NCHW' ) return ret class RPNAnchors ( namedtuple ( '_RPNAnchors' , [ 'boxes' , 'gt_labels' , 'gt_boxes' ])): \"\"\" boxes (FS x FS x NA x 4): The anchor boxes. gt_labels (FS x FS x NA): gt_boxes (FS x FS x NA x 4): Groundtruth boxes corresponding to each anchor. \"\"\" def encoded_gt_boxes ( self ): return encode_bbox_target ( self . gt_boxes , self . boxes ) def decode_logits ( self , logits ): return decode_bbox_target ( logits , self . boxes ) @under_name_scope () def narrow_to ( self , featuremap ): \"\"\" Slice anchors to the spatial size of this featuremap. \"\"\" shape2d = tf . shape ( featuremap )[ 2 :] # h,w slice3d = tf . concat ([ shape2d , [ - 1 ]], axis = 0 ) slice4d = tf . concat ([ shape2d , [ - 1 , - 1 ]], axis = 0 ) boxes = tf . slice ( self . boxes , [ 0 , 0 , 0 , 0 ], slice4d ) gt_labels = tf . slice ( self . gt_labels , [ 0 , 0 , 0 ], slice3d ) gt_boxes = tf . slice ( self . gt_boxes , [ 0 , 0 , 0 , 0 ], slice4d ) return RPNAnchors ( boxes , gt_labels , gt_boxes ) if __name__ == '__main__' : \"\"\" Demonstrate what's wrong with tf.image.crop_and_resize. Also reported at https://github.com/tensorflow/tensorflow/issues/26278 \"\"\" import tensorflow.contrib.eager as tfe tfe . enable_eager_execution () # want to crop 2x2 out of a 5x5 image, and resize to 4x4 image = np . arange ( 25 ) . astype ( 'float32' ) . reshape ( 5 , 5 ) boxes = np . asarray ([[ 1 , 1 , 3 , 3 ]], dtype = 'float32' ) target = 4 print ( crop_and_resize ( image [ None , None , :, :], boxes , [ 0 ], target )[ 0 ][ 0 ]) \"\"\" Expected values: 4.5 5 5.5 6 7 7.5 8 8.5 9.5 10 10.5 11 12 12.5 13 13.5 You cannot easily get the above results with tf.image.crop_and_resize. Try out yourself here: \"\"\" print ( tf . image . crop_and_resize ( image [ None , :, :, None ], np . asarray ([[ 1 , 1 , 2 , 2 ]]) / 4.0 , [ 0 ], [ target , target ])[ 0 ][:, :, 0 ]) Functions clip_boxes def clip_boxes ( boxes , window , name = None ) Args: boxes: nx4, xyxy window: [h, w] View Source @under_name_scope () def clip_boxes ( boxes , window , name = None ) : \"\"\" Args: boxes: nx4, xyxy window: [h, w] \"\"\" boxes = tf . maximum ( boxes , 0.0 ) m = tf . tile ( tf . reverse ( window , [ 0 ] ), [ 2 ] ) # ( 4 ,) boxes = tf . minimum ( boxes , tf . cast ( m , tf . float32 ), name = name ) return boxes crop_and_resize def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size View Source @under_name_scope () def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ) : \"\"\" Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size \"\"\" assert isinstance ( crop_size , int ), crop_size boxes = tf . stop_gradient ( boxes ) # TF 's crop_and_resize produces zeros on border if pad_border: # this can be quite slow image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode=' SYMMETRIC ' ) boxes = boxes + 1 @under_name_scope () def transform_fpcoor_for_tf ( boxes , image_shape , crop_shape ) : \"\"\" The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize Returns: y1x1y2x2 \"\"\" x0 , y0 , x1 , y1 = tf . split ( boxes , 4 , axis = 1 ) spacing_w = ( x1 - x0 ) / tf . cast ( crop_shape [ 1 ] , tf . float32 ) spacing_h = ( y1 - y0 ) / tf . cast ( crop_shape [ 0 ] , tf . float32 ) imshape = [ tf.cast(image_shape[0 ] - 1 , tf . float32 ), tf . cast ( image_shape [ 1 ] - 1 , tf . float32 ) ] nx0 = ( x0 + spacing_w / 2 - 0.5 ) / imshape [ 1 ] ny0 = ( y0 + spacing_h / 2 - 0.5 ) / imshape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , tf . float32 ) / imshape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , tf . float32 ) / imshape [ 0 ] return tf . concat ( [ ny0, nx0, ny0 + nh, nx0 + nw ] , axis = 1 ) image_shape = tf . shape ( image ) [ 2: ] boxes = transform_fpcoor_for_tf ( boxes , image_shape , [ crop_size, crop_size ] ) image = tf . transpose ( image , [ 0, 2, 3, 1 ] ) # nhwc ret = tf . image . crop_and_resize ( image , boxes , tf . cast ( box_ind , tf . int32 ), crop_size =[ crop_size, crop_size ] ) ret = tf . transpose ( ret , [ 0, 3, 1, 2 ] ) # ncss return ret decode_bbox_target def decode_bbox_target ( box_predictions , anchors ) Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. View Source @under_name_scope () def decode_bbox_target ( box_predictions , anchors ) : \"\"\" Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. \"\"\" orig_shape = tf . shape ( anchors ) box_pred_txtytwth = tf . reshape ( box_predictions , ( - 1 , 2 , 2 )) box_pred_txty , box_pred_twth = tf . split ( box_pred_txtytwth , 2 , axis = 1 ) # each is (...) x1x2 anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 clip = np . log ( config . PREPROC . MAX_SIZE / 16. ) wbhb = tf . exp ( tf . minimum ( box_pred_twth , clip )) * waha xbyb = box_pred_txty * waha + xaya x1y1 = xbyb - wbhb * 0.5 x2y2 = xbyb + wbhb * 0.5 # (...) x1x2 out = tf . concat ( [ x1y1, x2y2 ] , axis =- 2 ) return tf . reshape ( out , orig_shape ) encode_bbox_target def encode_bbox_target ( boxes , anchors ) Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. View Source @under_name_scope () def encode_bbox_target ( boxes , anchors ) : \"\"\" Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. \"\"\" anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 boxes_x1y1x2y2 = tf . reshape ( boxes , ( - 1 , 2 , 2 )) boxes_x1y1 , boxes_x2y2 = tf . split ( boxes_x1y1x2y2 , 2 , axis = 1 ) wbhb = boxes_x2y2 - boxes_x1y1 xbyb = ( boxes_x2y2 + boxes_x1y1 ) * 0.5 # Note that here not all boxes are valid . Some may be zero txty = ( xbyb - xaya ) / waha twth = tf . log ( wbhb / waha ) # may contain - inf for invalid boxes encoded = tf . concat ( [ txty, twth ] , axis = 1 ) # ( - 1 x2x2 ) return tf . reshape ( encoded , tf . shape ( boxes )) roi_align def roi_align ( featuremap , boxes , resolution ) Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res View Source @under_name_scope () def roi_align ( featuremap , boxes , resolution ) : \"\"\" Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res \"\"\" # sample 4 locations per roi bin ret = crop_and_resize ( featuremap , boxes , tf . zeros ( [ tf.shape(boxes)[0 ] ] , dtype = tf . int32 ), resolution * 2 ) try : avgpool = tf . nn . avg_pool2d except AttributeError : avgpool = tf . nn . avg_pool ret = avgpool ( ret , [ 1, 1, 2, 2 ] , [ 1, 1, 2, 2 ] , padding = 'SAME' , data_format = 'NCHW' ) return ret Classes RPNAnchors class RPNAnchors ( / , * args , ** kwargs ) boxes (FS x FS x NA x 4): The anchor boxes. gt_labels (FS x FS x NA): gt_boxes (FS x FS x NA x 4): Groundtruth boxes corresponding to each anchor. Ancestors (in MRO) model_box._RPNAnchors builtins.tuple Instance variables boxes Alias for field number 0 gt_boxes Alias for field number 2 gt_labels Alias for field number 1 Methods count def count ( ... ) T.count(value) -> integer -- return number of occurrences of value decode_logits def decode_logits ( self , logits ) View Source def decode_logits ( self , logits ): return decode_bbox_target ( logits , self . boxes ) encoded_gt_boxes def encoded_gt_boxes ( self ) View Source def encoded_gt_boxes ( self ): return encode_bbox_target ( self . gt_boxes , self . boxes ) index def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present. narrow_to def narrow_to ( self , featuremap ) Slice anchors to the spatial size of this featuremap. View Source @under_name_scope () def narrow_to ( self , featuremap ) : \"\"\" Slice anchors to the spatial size of this featuremap. \"\"\" shape2d = tf . shape ( featuremap ) [ 2: ] # h , w slice3d = tf . concat ( [ shape2d, [-1 ] ] , axis = 0 ) slice4d = tf . concat ( [ shape2d, [-1, -1 ] ] , axis = 0 ) boxes = tf . slice ( self . boxes , [ 0, 0, 0, 0 ] , slice4d ) gt_labels = tf . slice ( self . gt_labels , [ 0, 0, 0 ] , slice3d ) gt_boxes = tf . slice ( self . gt_boxes , [ 0, 0, 0, 0 ] , slice4d ) return RPNAnchors ( boxes , gt_labels , gt_boxes )","title":"Model Box"},{"location":"reference/mot/object_detection/modeling/model_box/#module-motobject_detectionmodelingmodel_box","text":"View Source # -*- coding: utf-8 -*- # File: model_box.py import numpy as np import tensorflow as tf from collections import namedtuple from tensorpack.tfutils.scope_utils import under_name_scope from mot.object_detection.config import config @under_name_scope () def clip_boxes ( boxes , window , name = None ): \"\"\" Args: boxes: nx4, xyxy window: [h, w] \"\"\" boxes = tf . maximum ( boxes , 0.0 ) m = tf . tile ( tf . reverse ( window , [ 0 ]), [ 2 ]) # (4,) boxes = tf . minimum ( boxes , tf . cast ( m , tf . float32 ), name = name ) return boxes @under_name_scope () def decode_bbox_target ( box_predictions , anchors ): \"\"\" Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. \"\"\" orig_shape = tf . shape ( anchors ) box_pred_txtytwth = tf . reshape ( box_predictions , ( - 1 , 2 , 2 )) box_pred_txty , box_pred_twth = tf . split ( box_pred_txtytwth , 2 , axis = 1 ) # each is (...)x1x2 anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 clip = np . log ( config . PREPROC . MAX_SIZE / 16. ) wbhb = tf . exp ( tf . minimum ( box_pred_twth , clip )) * waha xbyb = box_pred_txty * waha + xaya x1y1 = xbyb - wbhb * 0.5 x2y2 = xbyb + wbhb * 0.5 # (...)x1x2 out = tf . concat ([ x1y1 , x2y2 ], axis =- 2 ) return tf . reshape ( out , orig_shape ) @under_name_scope () def encode_bbox_target ( boxes , anchors ): \"\"\" Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. \"\"\" anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 boxes_x1y1x2y2 = tf . reshape ( boxes , ( - 1 , 2 , 2 )) boxes_x1y1 , boxes_x2y2 = tf . split ( boxes_x1y1x2y2 , 2 , axis = 1 ) wbhb = boxes_x2y2 - boxes_x1y1 xbyb = ( boxes_x2y2 + boxes_x1y1 ) * 0.5 # Note that here not all boxes are valid. Some may be zero txty = ( xbyb - xaya ) / waha twth = tf . log ( wbhb / waha ) # may contain -inf for invalid boxes encoded = tf . concat ([ txty , twth ], axis = 1 ) # (-1x2x2) return tf . reshape ( encoded , tf . shape ( boxes )) @under_name_scope () def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ): \"\"\" Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size \"\"\" assert isinstance ( crop_size , int ), crop_size boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border if pad_border : # this can be quite slow image = tf . pad ( image , [[ 0 , 0 ], [ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 @under_name_scope () def transform_fpcoor_for_tf ( boxes , image_shape , crop_shape ): \"\"\" The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize Returns: y1x1y2x2 \"\"\" x0 , y0 , x1 , y1 = tf . split ( boxes , 4 , axis = 1 ) spacing_w = ( x1 - x0 ) / tf . cast ( crop_shape [ 1 ], tf . float32 ) spacing_h = ( y1 - y0 ) / tf . cast ( crop_shape [ 0 ], tf . float32 ) imshape = [ tf . cast ( image_shape [ 0 ] - 1 , tf . float32 ), tf . cast ( image_shape [ 1 ] - 1 , tf . float32 )] nx0 = ( x0 + spacing_w / 2 - 0.5 ) / imshape [ 1 ] ny0 = ( y0 + spacing_h / 2 - 0.5 ) / imshape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , tf . float32 ) / imshape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , tf . float32 ) / imshape [ 0 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis = 1 ) image_shape = tf . shape ( image )[ 2 :] boxes = transform_fpcoor_for_tf ( boxes , image_shape , [ crop_size , crop_size ]) image = tf . transpose ( image , [ 0 , 2 , 3 , 1 ]) # nhwc ret = tf . image . crop_and_resize ( image , boxes , tf . cast ( box_ind , tf . int32 ), crop_size = [ crop_size , crop_size ]) ret = tf . transpose ( ret , [ 0 , 3 , 1 , 2 ]) # ncss return ret @under_name_scope () def roi_align ( featuremap , boxes , resolution ): \"\"\" Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res \"\"\" # sample 4 locations per roi bin ret = crop_and_resize ( featuremap , boxes , tf . zeros ([ tf . shape ( boxes )[ 0 ]], dtype = tf . int32 ), resolution * 2 ) try : avgpool = tf . nn . avg_pool2d except AttributeError : avgpool = tf . nn . avg_pool ret = avgpool ( ret , [ 1 , 1 , 2 , 2 ], [ 1 , 1 , 2 , 2 ], padding = 'SAME' , data_format = 'NCHW' ) return ret class RPNAnchors ( namedtuple ( '_RPNAnchors' , [ 'boxes' , 'gt_labels' , 'gt_boxes' ])): \"\"\" boxes (FS x FS x NA x 4): The anchor boxes. gt_labels (FS x FS x NA): gt_boxes (FS x FS x NA x 4): Groundtruth boxes corresponding to each anchor. \"\"\" def encoded_gt_boxes ( self ): return encode_bbox_target ( self . gt_boxes , self . boxes ) def decode_logits ( self , logits ): return decode_bbox_target ( logits , self . boxes ) @under_name_scope () def narrow_to ( self , featuremap ): \"\"\" Slice anchors to the spatial size of this featuremap. \"\"\" shape2d = tf . shape ( featuremap )[ 2 :] # h,w slice3d = tf . concat ([ shape2d , [ - 1 ]], axis = 0 ) slice4d = tf . concat ([ shape2d , [ - 1 , - 1 ]], axis = 0 ) boxes = tf . slice ( self . boxes , [ 0 , 0 , 0 , 0 ], slice4d ) gt_labels = tf . slice ( self . gt_labels , [ 0 , 0 , 0 ], slice3d ) gt_boxes = tf . slice ( self . gt_boxes , [ 0 , 0 , 0 , 0 ], slice4d ) return RPNAnchors ( boxes , gt_labels , gt_boxes ) if __name__ == '__main__' : \"\"\" Demonstrate what's wrong with tf.image.crop_and_resize. Also reported at https://github.com/tensorflow/tensorflow/issues/26278 \"\"\" import tensorflow.contrib.eager as tfe tfe . enable_eager_execution () # want to crop 2x2 out of a 5x5 image, and resize to 4x4 image = np . arange ( 25 ) . astype ( 'float32' ) . reshape ( 5 , 5 ) boxes = np . asarray ([[ 1 , 1 , 3 , 3 ]], dtype = 'float32' ) target = 4 print ( crop_and_resize ( image [ None , None , :, :], boxes , [ 0 ], target )[ 0 ][ 0 ]) \"\"\" Expected values: 4.5 5 5.5 6 7 7.5 8 8.5 9.5 10 10.5 11 12 12.5 13 13.5 You cannot easily get the above results with tf.image.crop_and_resize. Try out yourself here: \"\"\" print ( tf . image . crop_and_resize ( image [ None , :, :, None ], np . asarray ([[ 1 , 1 , 2 , 2 ]]) / 4.0 , [ 0 ], [ target , target ])[ 0 ][:, :, 0 ])","title":"Module mot.object_detection.modeling.model_box"},{"location":"reference/mot/object_detection/modeling/model_box/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/model_box/#clip_boxes","text":"def clip_boxes ( boxes , window , name = None ) Args: boxes: nx4, xyxy window: [h, w] View Source @under_name_scope () def clip_boxes ( boxes , window , name = None ) : \"\"\" Args: boxes: nx4, xyxy window: [h, w] \"\"\" boxes = tf . maximum ( boxes , 0.0 ) m = tf . tile ( tf . reverse ( window , [ 0 ] ), [ 2 ] ) # ( 4 ,) boxes = tf . minimum ( boxes , tf . cast ( m , tf . float32 ), name = name ) return boxes","title":"clip_boxes"},{"location":"reference/mot/object_detection/modeling/model_box/#crop_and_resize","text":"def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size View Source @under_name_scope () def crop_and_resize ( image , boxes , box_ind , crop_size , pad_border = True ) : \"\"\" Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Args: image: NCHW boxes: nx4, x1y1x2y2 box_ind: (n,) crop_size (int): Returns: n,C,size,size \"\"\" assert isinstance ( crop_size , int ), crop_size boxes = tf . stop_gradient ( boxes ) # TF 's crop_and_resize produces zeros on border if pad_border: # this can be quite slow image = tf.pad(image, [[0, 0], [0, 0], [1, 1], [1, 1]], mode=' SYMMETRIC ' ) boxes = boxes + 1 @under_name_scope () def transform_fpcoor_for_tf ( boxes , image_shape , crop_shape ) : \"\"\" The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize Returns: y1x1y2x2 \"\"\" x0 , y0 , x1 , y1 = tf . split ( boxes , 4 , axis = 1 ) spacing_w = ( x1 - x0 ) / tf . cast ( crop_shape [ 1 ] , tf . float32 ) spacing_h = ( y1 - y0 ) / tf . cast ( crop_shape [ 0 ] , tf . float32 ) imshape = [ tf.cast(image_shape[0 ] - 1 , tf . float32 ), tf . cast ( image_shape [ 1 ] - 1 , tf . float32 ) ] nx0 = ( x0 + spacing_w / 2 - 0.5 ) / imshape [ 1 ] ny0 = ( y0 + spacing_h / 2 - 0.5 ) / imshape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , tf . float32 ) / imshape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , tf . float32 ) / imshape [ 0 ] return tf . concat ( [ ny0, nx0, ny0 + nh, nx0 + nw ] , axis = 1 ) image_shape = tf . shape ( image ) [ 2: ] boxes = transform_fpcoor_for_tf ( boxes , image_shape , [ crop_size, crop_size ] ) image = tf . transpose ( image , [ 0, 2, 3, 1 ] ) # nhwc ret = tf . image . crop_and_resize ( image , boxes , tf . cast ( box_ind , tf . int32 ), crop_size =[ crop_size, crop_size ] ) ret = tf . transpose ( ret , [ 0, 3, 1, 2 ] ) # ncss return ret","title":"crop_and_resize"},{"location":"reference/mot/object_detection/modeling/model_box/#decode_bbox_target","text":"def decode_bbox_target ( box_predictions , anchors ) Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. View Source @under_name_scope () def decode_bbox_target ( box_predictions , anchors ) : \"\"\" Args: box_predictions: (..., 4), logits anchors: (..., 4), floatbox. Must have the same shape Returns: box_decoded: (..., 4), float32. With the same shape. \"\"\" orig_shape = tf . shape ( anchors ) box_pred_txtytwth = tf . reshape ( box_predictions , ( - 1 , 2 , 2 )) box_pred_txty , box_pred_twth = tf . split ( box_pred_txtytwth , 2 , axis = 1 ) # each is (...) x1x2 anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 clip = np . log ( config . PREPROC . MAX_SIZE / 16. ) wbhb = tf . exp ( tf . minimum ( box_pred_twth , clip )) * waha xbyb = box_pred_txty * waha + xaya x1y1 = xbyb - wbhb * 0.5 x2y2 = xbyb + wbhb * 0.5 # (...) x1x2 out = tf . concat ( [ x1y1, x2y2 ] , axis =- 2 ) return tf . reshape ( out , orig_shape )","title":"decode_bbox_target"},{"location":"reference/mot/object_detection/modeling/model_box/#encode_bbox_target","text":"def encode_bbox_target ( boxes , anchors ) Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. View Source @under_name_scope () def encode_bbox_target ( boxes , anchors ) : \"\"\" Args: boxes: (..., 4), float32 anchors: (..., 4), float32 Returns: box_encoded: (..., 4), float32 with the same shape. \"\"\" anchors_x1y1x2y2 = tf . reshape ( anchors , ( - 1 , 2 , 2 )) anchors_x1y1 , anchors_x2y2 = tf . split ( anchors_x1y1x2y2 , 2 , axis = 1 ) waha = anchors_x2y2 - anchors_x1y1 xaya = ( anchors_x2y2 + anchors_x1y1 ) * 0.5 boxes_x1y1x2y2 = tf . reshape ( boxes , ( - 1 , 2 , 2 )) boxes_x1y1 , boxes_x2y2 = tf . split ( boxes_x1y1x2y2 , 2 , axis = 1 ) wbhb = boxes_x2y2 - boxes_x1y1 xbyb = ( boxes_x2y2 + boxes_x1y1 ) * 0.5 # Note that here not all boxes are valid . Some may be zero txty = ( xbyb - xaya ) / waha twth = tf . log ( wbhb / waha ) # may contain - inf for invalid boxes encoded = tf . concat ( [ txty, twth ] , axis = 1 ) # ( - 1 x2x2 ) return tf . reshape ( encoded , tf . shape ( boxes ))","title":"encode_bbox_target"},{"location":"reference/mot/object_detection/modeling/model_box/#roi_align","text":"def roi_align ( featuremap , boxes , resolution ) Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res View Source @under_name_scope () def roi_align ( featuremap , boxes , resolution ) : \"\"\" Args: featuremap: 1xCxHxW boxes: Nx4 floatbox resolution: output spatial resolution Returns: NxCx res x res \"\"\" # sample 4 locations per roi bin ret = crop_and_resize ( featuremap , boxes , tf . zeros ( [ tf.shape(boxes)[0 ] ] , dtype = tf . int32 ), resolution * 2 ) try : avgpool = tf . nn . avg_pool2d except AttributeError : avgpool = tf . nn . avg_pool ret = avgpool ( ret , [ 1, 1, 2, 2 ] , [ 1, 1, 2, 2 ] , padding = 'SAME' , data_format = 'NCHW' ) return ret","title":"roi_align"},{"location":"reference/mot/object_detection/modeling/model_box/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/modeling/model_box/#rpnanchors","text":"class RPNAnchors ( / , * args , ** kwargs ) boxes (FS x FS x NA x 4): The anchor boxes. gt_labels (FS x FS x NA): gt_boxes (FS x FS x NA x 4): Groundtruth boxes corresponding to each anchor.","title":"RPNAnchors"},{"location":"reference/mot/object_detection/modeling/model_box/#ancestors-in-mro","text":"model_box._RPNAnchors builtins.tuple","title":"Ancestors (in MRO)"},{"location":"reference/mot/object_detection/modeling/model_box/#instance-variables","text":"boxes Alias for field number 0 gt_boxes Alias for field number 2 gt_labels Alias for field number 1","title":"Instance variables"},{"location":"reference/mot/object_detection/modeling/model_box/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/model_box/#count","text":"def count ( ... ) T.count(value) -> integer -- return number of occurrences of value","title":"count"},{"location":"reference/mot/object_detection/modeling/model_box/#decode_logits","text":"def decode_logits ( self , logits ) View Source def decode_logits ( self , logits ): return decode_bbox_target ( logits , self . boxes )","title":"decode_logits"},{"location":"reference/mot/object_detection/modeling/model_box/#encoded_gt_boxes","text":"def encoded_gt_boxes ( self ) View Source def encoded_gt_boxes ( self ): return encode_bbox_target ( self . gt_boxes , self . boxes )","title":"encoded_gt_boxes"},{"location":"reference/mot/object_detection/modeling/model_box/#index","text":"def index ( ... ) T.index(value, [start, [stop]]) -> integer -- return first index of value. Raises ValueError if the value is not present.","title":"index"},{"location":"reference/mot/object_detection/modeling/model_box/#narrow_to","text":"def narrow_to ( self , featuremap ) Slice anchors to the spatial size of this featuremap. View Source @under_name_scope () def narrow_to ( self , featuremap ) : \"\"\" Slice anchors to the spatial size of this featuremap. \"\"\" shape2d = tf . shape ( featuremap ) [ 2: ] # h , w slice3d = tf . concat ( [ shape2d, [-1 ] ] , axis = 0 ) slice4d = tf . concat ( [ shape2d, [-1, -1 ] ] , axis = 0 ) boxes = tf . slice ( self . boxes , [ 0, 0, 0, 0 ] , slice4d ) gt_labels = tf . slice ( self . gt_labels , [ 0, 0, 0 ] , slice3d ) gt_boxes = tf . slice ( self . gt_boxes , [ 0, 0, 0, 0 ] , slice4d ) return RPNAnchors ( boxes , gt_labels , gt_boxes )","title":"narrow_to"},{"location":"reference/mot/object_detection/modeling/model_cascade/","text":"Module mot.object_detection.modeling.model_cascade View Source import tensorflow as tf from tensorpack.tfutils import get_current_tower_context from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import pairwise_iou , area as tf_area from mot.object_detection.modeling.model_box import clip_boxes from mot.object_detection.modeling.model_frcnn import BoxProposals , FastRCNNHead , fastrcnn_outputs class CascadeRCNNHead ( object ): def __init__ ( self , proposals , roi_func , fastrcnn_head_func , gt_targets , image_shape2d , num_categories ): \"\"\" Args: proposals: BoxProposals roi_func (boxes -> features): a function to crop features with rois fastrcnn_head_func (features -> features): the fastrcnn head to apply on the cropped features gt_targets (gt_boxes, gt_labels): \"\"\" for k , v in locals () . items (): if k != 'self' : setattr ( self , k , v ) self . gt_boxes , self . gt_labels = gt_targets del self . gt_targets self . num_cascade_stages = len ( cfg . CASCADE . IOUS ) self . training = get_current_tower_context () . is_training if self . training : @tf.custom_gradient def scale_gradient ( x ): return x , lambda dy : dy * ( 1.0 / self . num_cascade_stages ) self . scale_gradient = scale_gradient else : self . scale_gradient = tf . identity ious = cfg . CASCADE . IOUS # It's unclear how to do >3 stages, so it does not make sense to implement them assert self . num_cascade_stages == 3 , \"Only 3-stage cascade was implemented!\" with tf . variable_scope ( 'cascade_rcnn_stage1' ): H1 , B1 = self . run_head ( self . proposals , 0 ) with tf . variable_scope ( 'cascade_rcnn_stage2' ): B1_proposal = self . match_box_with_gt ( B1 , ious [ 1 ]) H2 , B2 = self . run_head ( B1_proposal , 1 ) with tf . variable_scope ( 'cascade_rcnn_stage3' ): B2_proposal = self . match_box_with_gt ( B2 , ious [ 2 ]) H3 , B3 = self . run_head ( B2_proposal , 2 ) self . _cascade_boxes = [ B1 , B2 , B3 ] self . _heads = [ H1 , H2 , H3 ] def run_head ( self , proposals , stage ): \"\"\" Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes \"\"\" reg_weights = tf . constant ( cfg . CASCADE . BBOX_REG_WEIGHTS [ stage ], dtype = tf . float32 ) pooled_feature = self . roi_func ( proposals . boxes ) # N,C,S,S pooled_feature = self . scale_gradient ( pooled_feature ) head_feature = self . fastrcnn_head_func ( 'head' , pooled_feature ) label_logits , box_logits = fastrcnn_outputs ( 'outputs' , head_feature , self . num_categories , class_agnostic_regression = True ) head = FastRCNNHead ( proposals , box_logits , label_logits , self . gt_boxes , reg_weights ) refined_boxes = head . decoded_output_boxes_class_agnostic () refined_boxes = clip_boxes ( refined_boxes , self . image_shape2d ) if self . training : refined_boxes = tf . boolean_mask ( refined_boxes , tf_area ( refined_boxes ) > 0 ) return head , tf . stop_gradient ( refined_boxes , name = 'output_boxes' ) def match_box_with_gt ( self , boxes , iou_threshold ): \"\"\" Args: boxes: Nx4 Returns: BoxProposals \"\"\" if self . training : with tf . name_scope ( 'match_box_with_gt_{}' . format ( iou_threshold )): iou = pairwise_iou ( boxes , self . gt_boxes ) # NxM max_iou_per_box = tf . reduce_max ( iou , axis = 1 ) # N best_iou_ind = tf . argmax ( iou , axis = 1 ) # N labels_per_box = tf . gather ( self . gt_labels , best_iou_ind ) fg_mask = max_iou_per_box >= iou_threshold fg_inds_wrt_gt = tf . boolean_mask ( best_iou_ind , fg_mask ) labels_per_box = tf . stop_gradient ( labels_per_box * tf . cast ( fg_mask , tf . int64 )) return BoxProposals ( boxes , labels_per_box , fg_inds_wrt_gt ) else : return BoxProposals ( boxes ) def losses ( self ): ret = [] for idx , head in enumerate ( self . _heads ): with tf . name_scope ( 'cascade_loss_stage{}' . format ( idx + 1 )): ret . extend ( head . losses ()) return ret def decoded_output_boxes ( self ): \"\"\" Returns: Nx#classx4 \"\"\" ret = self . _cascade_boxes [ - 1 ] ret = tf . expand_dims ( ret , 1 ) # class-agnostic return tf . tile ( ret , [ 1 , self . num_categories + 1 , 1 ]) def output_scores ( self , name = None ): \"\"\" Returns: Nx#class \"\"\" scores = [ head . output_scores ( 'cascade_scores_stage{}' . format ( idx + 1 )) for idx , head in enumerate ( self . _heads )] return tf . multiply ( tf . add_n ( scores ), ( 1.0 / self . num_cascade_stages ), name = name ) Classes CascadeRCNNHead class CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , gt_targets , image_shape2d , num_categories ) Methods decoded_output_boxes def decoded_output_boxes ( self ) Returns: Nx#classx4 View Source def decoded_output_boxes ( self ): \"\"\" Returns: Nx#classx4 \"\"\" ret = self . _cascade_boxes [ - 1 ] ret = tf . expand_dims ( ret , 1 ) # class - agnostic return tf . tile ( ret , [ 1 , self . num_categories + 1 , 1 ]) losses def losses ( self ) View Source def losses ( self ): ret = [] for idx , head in enumerate ( self . _heads ): with tf . name_scope ( 'cascade_loss_stage{}' . format ( idx + 1 )): ret . extend ( head . losses ()) return ret match_box_with_gt def match_box_with_gt ( self , boxes , iou_threshold ) Args: boxes: Nx4 Returns: BoxProposals View Source def match_box_with_gt ( self , boxes , iou_threshold ): \"\"\" Args: boxes: Nx4 Returns: BoxProposals \"\"\" if self . training : with tf . name_scope ( 'match_box_with_gt_{}' . format ( iou_threshold )): iou = pairwise_iou ( boxes , self . gt_boxes ) # NxM max_iou_per_box = tf . reduce_max ( iou , axis = 1 ) # N best_iou_ind = tf . argmax ( iou , axis = 1 ) # N labels_per_box = tf . gather ( self . gt_labels , best_iou_ind ) fg_mask = max_iou_per_box >= iou_threshold fg_inds_wrt_gt = tf . boolean_mask ( best_iou_ind , fg_mask ) labels_per_box = tf . stop_gradient ( labels_per_box * tf . cast ( fg_mask , tf . int64 )) return BoxProposals ( boxes , labels_per_box , fg_inds_wrt_gt ) else : return BoxProposals ( boxes ) output_scores def output_scores ( self , name = None ) Returns: Nx#class View Source def output_scores ( self , name = None ): \"\"\" Returns: Nx#class \"\"\" scores = [ head . output_scores ( 'cascade_scores_stage{}' . format ( idx + 1 )) for idx , head in enumerate ( self . _heads )] return tf . multiply ( tf . add_n ( scores ), ( 1.0 / self . num_cascade_stages ), name = name ) run_head def run_head ( self , proposals , stage ) Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes View Source def run_head ( self , proposals , stage ) : \"\"\" Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes \"\"\" reg_weights = tf . constant ( cfg . CASCADE . BBOX_REG_WEIGHTS [ stage ] , dtype = tf . float32 ) pooled_feature = self . roi_func ( proposals . boxes ) # N , C , S , S pooled_feature = self . scale_gradient ( pooled_feature ) head_feature = self . fastrcnn_head_func ( 'head' , pooled_feature ) label_logits , box_logits = fastrcnn_outputs ( 'outputs' , head_feature , self . num_categories , class_agnostic_regression = True ) head = FastRCNNHead ( proposals , box_logits , label_logits , self . gt_boxes , reg_weights ) refined_boxes = head . decoded_output_boxes_class_agnostic () refined_boxes = clip_boxes ( refined_boxes , self . image_shape2d ) if self . training : refined_boxes = tf . boolean_mask ( refined_boxes , tf_area ( refined_boxes ) > 0 ) return head , tf . stop_gradient ( refined_boxes , name = 'output_boxes' )","title":"Model Cascade"},{"location":"reference/mot/object_detection/modeling/model_cascade/#module-motobject_detectionmodelingmodel_cascade","text":"View Source import tensorflow as tf from tensorpack.tfutils import get_current_tower_context from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import pairwise_iou , area as tf_area from mot.object_detection.modeling.model_box import clip_boxes from mot.object_detection.modeling.model_frcnn import BoxProposals , FastRCNNHead , fastrcnn_outputs class CascadeRCNNHead ( object ): def __init__ ( self , proposals , roi_func , fastrcnn_head_func , gt_targets , image_shape2d , num_categories ): \"\"\" Args: proposals: BoxProposals roi_func (boxes -> features): a function to crop features with rois fastrcnn_head_func (features -> features): the fastrcnn head to apply on the cropped features gt_targets (gt_boxes, gt_labels): \"\"\" for k , v in locals () . items (): if k != 'self' : setattr ( self , k , v ) self . gt_boxes , self . gt_labels = gt_targets del self . gt_targets self . num_cascade_stages = len ( cfg . CASCADE . IOUS ) self . training = get_current_tower_context () . is_training if self . training : @tf.custom_gradient def scale_gradient ( x ): return x , lambda dy : dy * ( 1.0 / self . num_cascade_stages ) self . scale_gradient = scale_gradient else : self . scale_gradient = tf . identity ious = cfg . CASCADE . IOUS # It's unclear how to do >3 stages, so it does not make sense to implement them assert self . num_cascade_stages == 3 , \"Only 3-stage cascade was implemented!\" with tf . variable_scope ( 'cascade_rcnn_stage1' ): H1 , B1 = self . run_head ( self . proposals , 0 ) with tf . variable_scope ( 'cascade_rcnn_stage2' ): B1_proposal = self . match_box_with_gt ( B1 , ious [ 1 ]) H2 , B2 = self . run_head ( B1_proposal , 1 ) with tf . variable_scope ( 'cascade_rcnn_stage3' ): B2_proposal = self . match_box_with_gt ( B2 , ious [ 2 ]) H3 , B3 = self . run_head ( B2_proposal , 2 ) self . _cascade_boxes = [ B1 , B2 , B3 ] self . _heads = [ H1 , H2 , H3 ] def run_head ( self , proposals , stage ): \"\"\" Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes \"\"\" reg_weights = tf . constant ( cfg . CASCADE . BBOX_REG_WEIGHTS [ stage ], dtype = tf . float32 ) pooled_feature = self . roi_func ( proposals . boxes ) # N,C,S,S pooled_feature = self . scale_gradient ( pooled_feature ) head_feature = self . fastrcnn_head_func ( 'head' , pooled_feature ) label_logits , box_logits = fastrcnn_outputs ( 'outputs' , head_feature , self . num_categories , class_agnostic_regression = True ) head = FastRCNNHead ( proposals , box_logits , label_logits , self . gt_boxes , reg_weights ) refined_boxes = head . decoded_output_boxes_class_agnostic () refined_boxes = clip_boxes ( refined_boxes , self . image_shape2d ) if self . training : refined_boxes = tf . boolean_mask ( refined_boxes , tf_area ( refined_boxes ) > 0 ) return head , tf . stop_gradient ( refined_boxes , name = 'output_boxes' ) def match_box_with_gt ( self , boxes , iou_threshold ): \"\"\" Args: boxes: Nx4 Returns: BoxProposals \"\"\" if self . training : with tf . name_scope ( 'match_box_with_gt_{}' . format ( iou_threshold )): iou = pairwise_iou ( boxes , self . gt_boxes ) # NxM max_iou_per_box = tf . reduce_max ( iou , axis = 1 ) # N best_iou_ind = tf . argmax ( iou , axis = 1 ) # N labels_per_box = tf . gather ( self . gt_labels , best_iou_ind ) fg_mask = max_iou_per_box >= iou_threshold fg_inds_wrt_gt = tf . boolean_mask ( best_iou_ind , fg_mask ) labels_per_box = tf . stop_gradient ( labels_per_box * tf . cast ( fg_mask , tf . int64 )) return BoxProposals ( boxes , labels_per_box , fg_inds_wrt_gt ) else : return BoxProposals ( boxes ) def losses ( self ): ret = [] for idx , head in enumerate ( self . _heads ): with tf . name_scope ( 'cascade_loss_stage{}' . format ( idx + 1 )): ret . extend ( head . losses ()) return ret def decoded_output_boxes ( self ): \"\"\" Returns: Nx#classx4 \"\"\" ret = self . _cascade_boxes [ - 1 ] ret = tf . expand_dims ( ret , 1 ) # class-agnostic return tf . tile ( ret , [ 1 , self . num_categories + 1 , 1 ]) def output_scores ( self , name = None ): \"\"\" Returns: Nx#class \"\"\" scores = [ head . output_scores ( 'cascade_scores_stage{}' . format ( idx + 1 )) for idx , head in enumerate ( self . _heads )] return tf . multiply ( tf . add_n ( scores ), ( 1.0 / self . num_cascade_stages ), name = name )","title":"Module mot.object_detection.modeling.model_cascade"},{"location":"reference/mot/object_detection/modeling/model_cascade/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/modeling/model_cascade/#cascadercnnhead","text":"class CascadeRCNNHead ( proposals , roi_func , fastrcnn_head_func , gt_targets , image_shape2d , num_categories )","title":"CascadeRCNNHead"},{"location":"reference/mot/object_detection/modeling/model_cascade/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/model_cascade/#decoded_output_boxes","text":"def decoded_output_boxes ( self ) Returns: Nx#classx4 View Source def decoded_output_boxes ( self ): \"\"\" Returns: Nx#classx4 \"\"\" ret = self . _cascade_boxes [ - 1 ] ret = tf . expand_dims ( ret , 1 ) # class - agnostic return tf . tile ( ret , [ 1 , self . num_categories + 1 , 1 ])","title":"decoded_output_boxes"},{"location":"reference/mot/object_detection/modeling/model_cascade/#losses","text":"def losses ( self ) View Source def losses ( self ): ret = [] for idx , head in enumerate ( self . _heads ): with tf . name_scope ( 'cascade_loss_stage{}' . format ( idx + 1 )): ret . extend ( head . losses ()) return ret","title":"losses"},{"location":"reference/mot/object_detection/modeling/model_cascade/#match_box_with_gt","text":"def match_box_with_gt ( self , boxes , iou_threshold ) Args: boxes: Nx4 Returns: BoxProposals View Source def match_box_with_gt ( self , boxes , iou_threshold ): \"\"\" Args: boxes: Nx4 Returns: BoxProposals \"\"\" if self . training : with tf . name_scope ( 'match_box_with_gt_{}' . format ( iou_threshold )): iou = pairwise_iou ( boxes , self . gt_boxes ) # NxM max_iou_per_box = tf . reduce_max ( iou , axis = 1 ) # N best_iou_ind = tf . argmax ( iou , axis = 1 ) # N labels_per_box = tf . gather ( self . gt_labels , best_iou_ind ) fg_mask = max_iou_per_box >= iou_threshold fg_inds_wrt_gt = tf . boolean_mask ( best_iou_ind , fg_mask ) labels_per_box = tf . stop_gradient ( labels_per_box * tf . cast ( fg_mask , tf . int64 )) return BoxProposals ( boxes , labels_per_box , fg_inds_wrt_gt ) else : return BoxProposals ( boxes )","title":"match_box_with_gt"},{"location":"reference/mot/object_detection/modeling/model_cascade/#output_scores","text":"def output_scores ( self , name = None ) Returns: Nx#class View Source def output_scores ( self , name = None ): \"\"\" Returns: Nx#class \"\"\" scores = [ head . output_scores ( 'cascade_scores_stage{}' . format ( idx + 1 )) for idx , head in enumerate ( self . _heads )] return tf . multiply ( tf . add_n ( scores ), ( 1.0 / self . num_cascade_stages ), name = name )","title":"output_scores"},{"location":"reference/mot/object_detection/modeling/model_cascade/#run_head","text":"def run_head ( self , proposals , stage ) Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes View Source def run_head ( self , proposals , stage ) : \"\"\" Args: proposals: BoxProposals stage: 0, 1, 2 Returns: FastRCNNHead Nx4, updated boxes \"\"\" reg_weights = tf . constant ( cfg . CASCADE . BBOX_REG_WEIGHTS [ stage ] , dtype = tf . float32 ) pooled_feature = self . roi_func ( proposals . boxes ) # N , C , S , S pooled_feature = self . scale_gradient ( pooled_feature ) head_feature = self . fastrcnn_head_func ( 'head' , pooled_feature ) label_logits , box_logits = fastrcnn_outputs ( 'outputs' , head_feature , self . num_categories , class_agnostic_regression = True ) head = FastRCNNHead ( proposals , box_logits , label_logits , self . gt_boxes , reg_weights ) refined_boxes = head . decoded_output_boxes_class_agnostic () refined_boxes = clip_boxes ( refined_boxes , self . image_shape2d ) if self . training : refined_boxes = tf . boolean_mask ( refined_boxes , tf_area ( refined_boxes ) > 0 ) return head , tf . stop_gradient ( refined_boxes , name = 'output_boxes' )","title":"run_head"},{"location":"reference/mot/object_detection/modeling/model_fpn/","text":"Module mot.object_detection.modeling.model_fpn View Source # -*- coding: utf-8 -*- import itertools import numpy as np import tensorflow as tf from tensorpack.models import Conv2D , FixedUnPooling , MaxPooling , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.scope_utils import under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.tfutils.tower import get_current_tower_context from tensorpack.utils.argtools import memoized from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import area as tf_area from mot.object_detection.modeling.backbone import GroupNorm from mot.object_detection.modeling.model_box import roi_align from mot.object_detection.modeling.model_rpn import generate_rpn_proposals , rpn_losses , get_all_anchors @layer_register ( log_shape = True ) def fpn_model ( features ): \"\"\" Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 \"\"\" assert len ( features ) == 4 , features num_channel = cfg . FPN . NUM_CHANNEL use_gn = cfg . FPN . NORM == 'GN' def upsample2x ( name , x ): try : resize = tf . compat . v2 . image . resize_images with tf . name_scope ( name ): shp2d = tf . shape ( x )[ 2 :] x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ]) x = resize ( x , shp2d * 2 , 'nearest' ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ]) return x except AttributeError : return FixedUnPooling ( name , x , 2 , unpool_mat = np . ones (( 2 , 2 ), dtype = 'float32' ), data_format = 'channels_first' ) with argscope ( Conv2D , data_format = 'channels_first' , activation = tf . identity , use_bias = True , kernel_initializer = tf . variance_scaling_initializer ( scale = 1. )): lat_2345 = [ Conv2D ( 'lateral_1x1_c {} ' . format ( i + 2 ), c , num_channel , 1 ) for i , c in enumerate ( features )] if use_gn : lat_2345 = [ GroupNorm ( 'gn_c {} ' . format ( i + 2 ), c ) for i , c in enumerate ( lat_2345 )] lat_sum_5432 = [] for idx , lat in enumerate ( lat_2345 [:: - 1 ]): if idx == 0 : lat_sum_5432 . append ( lat ) else : lat = lat + upsample2x ( 'upsample_lat {} ' . format ( 6 - idx ), lat_sum_5432 [ - 1 ]) lat_sum_5432 . append ( lat ) p2345 = [ Conv2D ( 'posthoc_3x3_p {} ' . format ( i + 2 ), c , num_channel , 3 ) for i , c in enumerate ( lat_sum_5432 [:: - 1 ])] if use_gn : p2345 = [ GroupNorm ( 'gn_p {} ' . format ( i + 2 ), c ) for i , c in enumerate ( p2345 )] p6 = MaxPooling ( 'maxpool_p6' , p2345 [ - 1 ], pool_size = 1 , strides = 2 , data_format = 'channels_first' , padding = 'VALID' ) return p2345 + [ p6 ] @under_name_scope () def fpn_map_rois_to_levels ( boxes ): \"\"\" Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. \"\"\" sqrtarea = tf . sqrt ( tf_area ( boxes )) level = tf . cast ( tf . floor ( 4 + tf . log ( sqrtarea * ( 1. / 224 ) + 1e-6 ) * ( 1.0 / np . log ( 2 ))), tf . int32 ) # RoI levels range from 2~5 (not 6) level_ids = [ tf . where ( level <= 2 ), tf . where ( tf . equal ( level , 3 )), # == is not supported tf . where ( tf . equal ( level , 4 )), tf . where ( level >= 5 )] level_ids = [ tf . reshape ( x , [ - 1 ], name = 'roi_level {} _id' . format ( i + 2 )) for i , x in enumerate ( level_ids )] num_in_levels = [ tf . size ( x , name = 'num_roi_level {} ' . format ( i + 2 )) for i , x in enumerate ( level_ids )] add_moving_summary ( * num_in_levels ) level_boxes = [ tf . gather ( boxes , ids ) for ids in level_ids ] return level_ids , level_boxes @under_name_scope () def multilevel_roi_align ( features , rcnn_boxes , resolution ): \"\"\" Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res \"\"\" assert len ( features ) == 4 , features # Reassign rcnn_boxes to levels level_ids , level_boxes = fpn_map_rois_to_levels ( rcnn_boxes ) all_rois = [] # Crop patches from corresponding levels for i , boxes , featuremap in zip ( itertools . count (), level_boxes , features ): with tf . name_scope ( 'roi_level {} ' . format ( i + 2 )): boxes_on_featuremap = boxes * ( 1.0 / cfg . FPN . ANCHOR_STRIDES [ i ]) all_rois . append ( roi_align ( featuremap , boxes_on_featuremap , resolution )) # this can fail if using TF<=1.8 with MKL build all_rois = tf . concat ( all_rois , axis = 0 ) # NCHW # Unshuffle to the original order, to match the original samples level_id_perm = tf . concat ( level_ids , axis = 0 ) # A permutation of 1~N level_id_invert_perm = tf . invert_permutation ( level_id_perm ) all_rois = tf . gather ( all_rois , level_id_invert_perm , name = \"output\" ) return all_rois def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ): \"\"\" Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_anchors ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl assert len ( multilevel_box_logits ) == num_lvl losses = [] with tf . name_scope ( 'rpn_losses' ): for lvl in range ( num_lvl ): anchors = multilevel_anchors [ lvl ] label_loss , box_loss = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), multilevel_label_logits [ lvl ], multilevel_box_logits [ lvl ], name_scope = 'level {} ' . format ( lvl + 2 )) losses . extend ([ label_loss , box_loss ]) total_label_loss = tf . add_n ( losses [:: 2 ], name = 'label_loss' ) total_box_loss = tf . add_n ( losses [ 1 :: 2 ], name = 'box_loss' ) add_moving_summary ( total_label_loss , total_box_loss ) return [ total_label_loss , total_box_loss ] @under_name_scope () def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ): \"\"\" Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_pred_boxes ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl training = get_current_tower_context () . is_training all_boxes = [] all_scores = [] if cfg . FPN . PROPOSAL_MODE == 'Level' : fpn_nms_topk = cfg . RPN . TRAIN_PER_LEVEL_NMS_TOPK if training else cfg . RPN . TEST_PER_LEVEL_NMS_TOPK for lvl in range ( num_lvl ): with tf . name_scope ( 'Lvl {} ' . format ( lvl + 2 )): pred_boxes_decoded = multilevel_pred_boxes [ lvl ] proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( multilevel_label_logits [ lvl ], [ - 1 ]), image_shape2d , fpn_nms_topk ) all_boxes . append ( proposal_boxes ) all_scores . append ( proposal_scores ) proposal_boxes = tf . concat ( all_boxes , axis = 0 ) # nx4 proposal_scores = tf . concat ( all_scores , axis = 0 ) # n # Here we are different from Detectron. # Detectron picks top-k within the batch, rather than within an image. However we do not have a batch. proposal_topk = tf . minimum ( tf . size ( proposal_scores ), fpn_nms_topk ) proposal_scores , topk_indices = tf . nn . top_k ( proposal_scores , k = proposal_topk , sorted = False ) proposal_boxes = tf . gather ( proposal_boxes , topk_indices , name = \"all_proposals\" ) else : for lvl in range ( num_lvl ): with tf . name_scope ( 'Lvl {} ' . format ( lvl + 2 )): pred_boxes_decoded = multilevel_pred_boxes [ lvl ] all_boxes . append ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ])) all_scores . append ( tf . reshape ( multilevel_label_logits [ lvl ], [ - 1 ])) all_boxes = tf . concat ( all_boxes , axis = 0 ) all_scores = tf . concat ( all_scores , axis = 0 ) proposal_boxes , proposal_scores = generate_rpn_proposals ( all_boxes , all_scores , image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if training else cfg . RPN . TEST_POST_NMS_TOPK ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), \\ tf . stop_gradient ( proposal_scores , name = 'scores' ) @memoized def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ): \"\"\" Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. \"\"\" assert len ( strides ) == len ( sizes ) foas = [] for stride , size in zip ( strides , sizes ): foa = get_all_anchors ( stride = stride , sizes = ( size ,), ratios = ratios , max_size = max_size ) foas . append ( foa ) return foas Functions fpn_map_rois_to_levels def fpn_map_rois_to_levels ( boxes ) Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. View Source @ under_name_scope () def fpn_map_rois_to_levels ( boxes ): \"\"\" Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. \"\"\" sqrtarea = tf . sqrt ( tf_area ( boxes )) level = tf . cast ( tf . floor ( 4 + tf . log ( sqrtarea * ( 1. / 224 ) + 1e-6 ) * ( 1.0 / np . log ( 2 ))), tf . int32 ) # RoI levels range from 2~5 (not 6) level_ids = [ tf . where ( level <= 2 ), tf . where ( tf . equal ( level , 3 )), # == is not supported tf . where ( tf . equal ( level , 4 )), tf . where ( level >= 5 )] level_ids = [ tf . reshape ( x , [ - 1 ], name = 'roi_level{}_id' . format ( i + 2 )) for i , x in enumerate ( level_ids )] num_in_levels = [ tf . size ( x , name = 'num_roi_level{}' . format ( i + 2 )) for i , x in enumerate ( level_ids )] add_moving_summary ( * num_in_levels ) level_boxes = [ tf . gather ( boxes , ids ) for ids in level_ids ] return level_ids , level_boxes fpn_model def fpn_model ( features ) Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 View Source @ layer_register ( log_shape = True ) def fpn_model ( features ) : \"\"\" Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 \"\"\" assert len ( features ) == 4 , features num_channel = cfg . FPN . NUM_CHANNEL use_gn = cfg . FPN . NORM == 'GN' def upsample2x ( name , x ) : try : resize = tf . compat . v2 . image . resize_images with tf . name_scope ( name ) : shp2d = tf . shape ( x )[ 2 : ] x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ]) x = resize ( x , shp2d * 2 , 'nearest' ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ]) return x except AttributeError : return FixedUnPooling ( name , x , 2 , unpool_mat = np . ones (( 2 , 2 ), dtype='float32' ), data_format='channels_first' ) with argscope ( Conv2D , data_format='channels_first' , activation = tf . identity , use_bias = True , kernel_initializer = tf . variance_scaling_initializer ( scale = 1. )) : lat_2345 = [ Conv2D ( 'lateral_1x1_c{}' . format ( i + 2 ), c , num_channel , 1 ) for i , c in enumerate ( features )] if use_gn: lat_2345 = [ GroupNorm ( 'gn_c{}' . format ( i + 2 ), c ) for i , c in enumerate ( lat_2345 )] lat_sum_5432 = [] for idx , lat in enumerate ( lat_2345 [ ::- 1 ]) : if idx == 0 : lat_sum_5432 . append ( lat ) else : lat = lat + upsample2x ( 'upsample_lat{}' . format ( 6 - idx ), lat_sum_5432 [ - 1 ]) lat_sum_5432 . append ( lat ) p2345 = [ Conv2D ( 'posthoc_3x3_p{}' . format ( i + 2 ), c , num_channel , 3 ) for i , c in enumerate ( lat_sum_5432 [ ::- 1 ])] if use_gn: p2345 = [ GroupNorm ( 'gn_p{}' . format ( i + 2 ), c ) for i , c in enumerate ( p2345 )] p6 = MaxPooling ( 'maxpool_p6' , p2345 [ - 1 ], pool_size = 1 , strides = 2 , data_format='channels_first' , padding='VALID' ) return p2345 + [ p6 ] generate_fpn_proposals def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits View Source @under_name_scope () def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) : \"\"\" Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_pred_boxes ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl training = get_current_tower_context (). is_training all_boxes = [] all_scores = [] if cfg . FPN . PROPOSAL_MODE == 'Level' : fpn_nms_topk = cfg . RPN . TRAIN_PER_LEVEL_NMS_TOPK if training else cfg . RPN . TEST_PER_LEVEL_NMS_TOPK for lvl in range ( num_lvl ) : with tf . name_scope ( 'Lvl{}' . format ( lvl + 2 )) : pred_boxes_decoded = multilevel_pred_boxes [ lvl ] proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ -1, 4 ] ), tf . reshape ( multilevel_label_logits [ lvl ] , [ -1 ] ), image_shape2d , fpn_nms_topk ) all_boxes . append ( proposal_boxes ) all_scores . append ( proposal_scores ) proposal_boxes = tf . concat ( all_boxes , axis = 0 ) # nx4 proposal_scores = tf . concat ( all_scores , axis = 0 ) # n # Here we are different from Detectron . # Detectron picks top - k within the batch , rather than within an image . However we do not have a batch . proposal_topk = tf . minimum ( tf . size ( proposal_scores ), fpn_nms_topk ) proposal_scores , topk_indices = tf . nn . top_k ( proposal_scores , k = proposal_topk , sorted = False ) proposal_boxes = tf . gather ( proposal_boxes , topk_indices , name = \"all_proposals\" ) else : for lvl in range ( num_lvl ) : with tf . name_scope ( 'Lvl{}' . format ( lvl + 2 )) : pred_boxes_decoded = multilevel_pred_boxes [ lvl ] all_boxes . append ( tf . reshape ( pred_boxes_decoded , [ -1, 4 ] )) all_scores . append ( tf . reshape ( multilevel_label_logits [ lvl ] , [ -1 ] )) all_boxes = tf . concat ( all_boxes , axis = 0 ) all_scores = tf . concat ( all_scores , axis = 0 ) proposal_boxes , proposal_scores = generate_rpn_proposals ( all_boxes , all_scores , image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if training else cfg . RPN . TEST_POST_NMS_TOPK ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), \\ tf . stop_gradient ( proposal_scores , name = 'scores' ) get_all_anchors_fpn def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ) Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. View Source @memoized def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ) : \"\"\" Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. \"\"\" assert len ( strides ) == len ( sizes ) foas = [] for stride , size in zip ( strides , sizes ) : foa = get_all_anchors ( stride = stride , sizes = ( size ,), ratios = ratios , max_size = max_size ) foas . append ( foa ) return foas multilevel_roi_align def multilevel_roi_align ( features , rcnn_boxes , resolution ) Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res View Source @under_name_scope () def multilevel_roi_align ( features , rcnn_boxes , resolution ) : \"\"\" Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res \"\"\" assert len ( features ) == 4 , features # Reassign rcnn_boxes to levels level_ids , level_boxes = fpn_map_rois_to_levels ( rcnn_boxes ) all_rois = [] # Crop patches from corresponding levels for i , boxes , featuremap in zip ( itertools . count (), level_boxes , features ) : with tf . name_scope ( 'roi_level{}' . format ( i + 2 )) : boxes_on_featuremap = boxes * ( 1.0 / cfg . FPN . ANCHOR_STRIDES [ i ] ) all_rois . append ( roi_align ( featuremap , boxes_on_featuremap , resolution )) # this can fail if using TF <= 1.8 with MKL build all_rois = tf . concat ( all_rois , axis = 0 ) # NCHW # Unshuffle to the original order , to match the original samples level_id_perm = tf . concat ( level_ids , axis = 0 ) # A permutation of 1 ~ N level_id_invert_perm = tf . invert_permutation ( level_id_perm ) all_rois = tf . gather ( all_rois , level_id_invert_perm , name = \"output\" ) return all_rois multilevel_rpn_losses def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss View Source def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) : \"\"\" Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_anchors ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl assert len ( multilevel_box_logits ) == num_lvl losses = [] with tf . name_scope ( 'rpn_losses' ) : for lvl in range ( num_lvl ) : anchors = multilevel_anchors [ lvl ] label_loss , box_loss = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), multilevel_label_logits [ lvl ] , multilevel_box_logits [ lvl ] , name_scope = 'level{}' . format ( lvl + 2 )) losses . extend ( [ label_loss, box_loss ] ) total_label_loss = tf . add_n ( losses [ ::2 ] , name = 'label_loss' ) total_box_loss = tf . add_n ( losses [ 1::2 ] , name = 'box_loss' ) add_moving_summary ( total_label_loss , total_box_loss ) return [ total_label_loss, total_box_loss ]","title":"Model Fpn"},{"location":"reference/mot/object_detection/modeling/model_fpn/#module-motobject_detectionmodelingmodel_fpn","text":"View Source # -*- coding: utf-8 -*- import itertools import numpy as np import tensorflow as tf from tensorpack.models import Conv2D , FixedUnPooling , MaxPooling , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.scope_utils import under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.tfutils.tower import get_current_tower_context from tensorpack.utils.argtools import memoized from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import area as tf_area from mot.object_detection.modeling.backbone import GroupNorm from mot.object_detection.modeling.model_box import roi_align from mot.object_detection.modeling.model_rpn import generate_rpn_proposals , rpn_losses , get_all_anchors @layer_register ( log_shape = True ) def fpn_model ( features ): \"\"\" Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 \"\"\" assert len ( features ) == 4 , features num_channel = cfg . FPN . NUM_CHANNEL use_gn = cfg . FPN . NORM == 'GN' def upsample2x ( name , x ): try : resize = tf . compat . v2 . image . resize_images with tf . name_scope ( name ): shp2d = tf . shape ( x )[ 2 :] x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ]) x = resize ( x , shp2d * 2 , 'nearest' ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ]) return x except AttributeError : return FixedUnPooling ( name , x , 2 , unpool_mat = np . ones (( 2 , 2 ), dtype = 'float32' ), data_format = 'channels_first' ) with argscope ( Conv2D , data_format = 'channels_first' , activation = tf . identity , use_bias = True , kernel_initializer = tf . variance_scaling_initializer ( scale = 1. )): lat_2345 = [ Conv2D ( 'lateral_1x1_c {} ' . format ( i + 2 ), c , num_channel , 1 ) for i , c in enumerate ( features )] if use_gn : lat_2345 = [ GroupNorm ( 'gn_c {} ' . format ( i + 2 ), c ) for i , c in enumerate ( lat_2345 )] lat_sum_5432 = [] for idx , lat in enumerate ( lat_2345 [:: - 1 ]): if idx == 0 : lat_sum_5432 . append ( lat ) else : lat = lat + upsample2x ( 'upsample_lat {} ' . format ( 6 - idx ), lat_sum_5432 [ - 1 ]) lat_sum_5432 . append ( lat ) p2345 = [ Conv2D ( 'posthoc_3x3_p {} ' . format ( i + 2 ), c , num_channel , 3 ) for i , c in enumerate ( lat_sum_5432 [:: - 1 ])] if use_gn : p2345 = [ GroupNorm ( 'gn_p {} ' . format ( i + 2 ), c ) for i , c in enumerate ( p2345 )] p6 = MaxPooling ( 'maxpool_p6' , p2345 [ - 1 ], pool_size = 1 , strides = 2 , data_format = 'channels_first' , padding = 'VALID' ) return p2345 + [ p6 ] @under_name_scope () def fpn_map_rois_to_levels ( boxes ): \"\"\" Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. \"\"\" sqrtarea = tf . sqrt ( tf_area ( boxes )) level = tf . cast ( tf . floor ( 4 + tf . log ( sqrtarea * ( 1. / 224 ) + 1e-6 ) * ( 1.0 / np . log ( 2 ))), tf . int32 ) # RoI levels range from 2~5 (not 6) level_ids = [ tf . where ( level <= 2 ), tf . where ( tf . equal ( level , 3 )), # == is not supported tf . where ( tf . equal ( level , 4 )), tf . where ( level >= 5 )] level_ids = [ tf . reshape ( x , [ - 1 ], name = 'roi_level {} _id' . format ( i + 2 )) for i , x in enumerate ( level_ids )] num_in_levels = [ tf . size ( x , name = 'num_roi_level {} ' . format ( i + 2 )) for i , x in enumerate ( level_ids )] add_moving_summary ( * num_in_levels ) level_boxes = [ tf . gather ( boxes , ids ) for ids in level_ids ] return level_ids , level_boxes @under_name_scope () def multilevel_roi_align ( features , rcnn_boxes , resolution ): \"\"\" Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res \"\"\" assert len ( features ) == 4 , features # Reassign rcnn_boxes to levels level_ids , level_boxes = fpn_map_rois_to_levels ( rcnn_boxes ) all_rois = [] # Crop patches from corresponding levels for i , boxes , featuremap in zip ( itertools . count (), level_boxes , features ): with tf . name_scope ( 'roi_level {} ' . format ( i + 2 )): boxes_on_featuremap = boxes * ( 1.0 / cfg . FPN . ANCHOR_STRIDES [ i ]) all_rois . append ( roi_align ( featuremap , boxes_on_featuremap , resolution )) # this can fail if using TF<=1.8 with MKL build all_rois = tf . concat ( all_rois , axis = 0 ) # NCHW # Unshuffle to the original order, to match the original samples level_id_perm = tf . concat ( level_ids , axis = 0 ) # A permutation of 1~N level_id_invert_perm = tf . invert_permutation ( level_id_perm ) all_rois = tf . gather ( all_rois , level_id_invert_perm , name = \"output\" ) return all_rois def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ): \"\"\" Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_anchors ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl assert len ( multilevel_box_logits ) == num_lvl losses = [] with tf . name_scope ( 'rpn_losses' ): for lvl in range ( num_lvl ): anchors = multilevel_anchors [ lvl ] label_loss , box_loss = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), multilevel_label_logits [ lvl ], multilevel_box_logits [ lvl ], name_scope = 'level {} ' . format ( lvl + 2 )) losses . extend ([ label_loss , box_loss ]) total_label_loss = tf . add_n ( losses [:: 2 ], name = 'label_loss' ) total_box_loss = tf . add_n ( losses [ 1 :: 2 ], name = 'box_loss' ) add_moving_summary ( total_label_loss , total_box_loss ) return [ total_label_loss , total_box_loss ] @under_name_scope () def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ): \"\"\" Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_pred_boxes ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl training = get_current_tower_context () . is_training all_boxes = [] all_scores = [] if cfg . FPN . PROPOSAL_MODE == 'Level' : fpn_nms_topk = cfg . RPN . TRAIN_PER_LEVEL_NMS_TOPK if training else cfg . RPN . TEST_PER_LEVEL_NMS_TOPK for lvl in range ( num_lvl ): with tf . name_scope ( 'Lvl {} ' . format ( lvl + 2 )): pred_boxes_decoded = multilevel_pred_boxes [ lvl ] proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ]), tf . reshape ( multilevel_label_logits [ lvl ], [ - 1 ]), image_shape2d , fpn_nms_topk ) all_boxes . append ( proposal_boxes ) all_scores . append ( proposal_scores ) proposal_boxes = tf . concat ( all_boxes , axis = 0 ) # nx4 proposal_scores = tf . concat ( all_scores , axis = 0 ) # n # Here we are different from Detectron. # Detectron picks top-k within the batch, rather than within an image. However we do not have a batch. proposal_topk = tf . minimum ( tf . size ( proposal_scores ), fpn_nms_topk ) proposal_scores , topk_indices = tf . nn . top_k ( proposal_scores , k = proposal_topk , sorted = False ) proposal_boxes = tf . gather ( proposal_boxes , topk_indices , name = \"all_proposals\" ) else : for lvl in range ( num_lvl ): with tf . name_scope ( 'Lvl {} ' . format ( lvl + 2 )): pred_boxes_decoded = multilevel_pred_boxes [ lvl ] all_boxes . append ( tf . reshape ( pred_boxes_decoded , [ - 1 , 4 ])) all_scores . append ( tf . reshape ( multilevel_label_logits [ lvl ], [ - 1 ])) all_boxes = tf . concat ( all_boxes , axis = 0 ) all_scores = tf . concat ( all_scores , axis = 0 ) proposal_boxes , proposal_scores = generate_rpn_proposals ( all_boxes , all_scores , image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if training else cfg . RPN . TEST_POST_NMS_TOPK ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), \\ tf . stop_gradient ( proposal_scores , name = 'scores' ) @memoized def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ): \"\"\" Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. \"\"\" assert len ( strides ) == len ( sizes ) foas = [] for stride , size in zip ( strides , sizes ): foa = get_all_anchors ( stride = stride , sizes = ( size ,), ratios = ratios , max_size = max_size ) foas . append ( foa ) return foas","title":"Module mot.object_detection.modeling.model_fpn"},{"location":"reference/mot/object_detection/modeling/model_fpn/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/model_fpn/#fpn_map_rois_to_levels","text":"def fpn_map_rois_to_levels ( boxes ) Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. View Source @ under_name_scope () def fpn_map_rois_to_levels ( boxes ): \"\"\" Assign boxes to level 2~5. Args: boxes (nx4): Returns: [tf.Tensor]: 4 tensors for level 2-5. Each tensor is a vector of indices of boxes in its level. [tf.Tensor]: 4 tensors, the gathered boxes in each level. Be careful that the returned tensor could be empty. \"\"\" sqrtarea = tf . sqrt ( tf_area ( boxes )) level = tf . cast ( tf . floor ( 4 + tf . log ( sqrtarea * ( 1. / 224 ) + 1e-6 ) * ( 1.0 / np . log ( 2 ))), tf . int32 ) # RoI levels range from 2~5 (not 6) level_ids = [ tf . where ( level <= 2 ), tf . where ( tf . equal ( level , 3 )), # == is not supported tf . where ( tf . equal ( level , 4 )), tf . where ( level >= 5 )] level_ids = [ tf . reshape ( x , [ - 1 ], name = 'roi_level{}_id' . format ( i + 2 )) for i , x in enumerate ( level_ids )] num_in_levels = [ tf . size ( x , name = 'num_roi_level{}' . format ( i + 2 )) for i , x in enumerate ( level_ids )] add_moving_summary ( * num_in_levels ) level_boxes = [ tf . gather ( boxes , ids ) for ids in level_ids ] return level_ids , level_boxes","title":"fpn_map_rois_to_levels"},{"location":"reference/mot/object_detection/modeling/model_fpn/#fpn_model","text":"def fpn_model ( features ) Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 View Source @ layer_register ( log_shape = True ) def fpn_model ( features ) : \"\"\" Args: features ([tf.Tensor]): ResNet features c2-c5 Returns: [tf.Tensor]: FPN features p2-p6 \"\"\" assert len ( features ) == 4 , features num_channel = cfg . FPN . NUM_CHANNEL use_gn = cfg . FPN . NORM == 'GN' def upsample2x ( name , x ) : try : resize = tf . compat . v2 . image . resize_images with tf . name_scope ( name ) : shp2d = tf . shape ( x )[ 2 : ] x = tf . transpose ( x , [ 0 , 2 , 3 , 1 ]) x = resize ( x , shp2d * 2 , 'nearest' ) x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ]) return x except AttributeError : return FixedUnPooling ( name , x , 2 , unpool_mat = np . ones (( 2 , 2 ), dtype='float32' ), data_format='channels_first' ) with argscope ( Conv2D , data_format='channels_first' , activation = tf . identity , use_bias = True , kernel_initializer = tf . variance_scaling_initializer ( scale = 1. )) : lat_2345 = [ Conv2D ( 'lateral_1x1_c{}' . format ( i + 2 ), c , num_channel , 1 ) for i , c in enumerate ( features )] if use_gn: lat_2345 = [ GroupNorm ( 'gn_c{}' . format ( i + 2 ), c ) for i , c in enumerate ( lat_2345 )] lat_sum_5432 = [] for idx , lat in enumerate ( lat_2345 [ ::- 1 ]) : if idx == 0 : lat_sum_5432 . append ( lat ) else : lat = lat + upsample2x ( 'upsample_lat{}' . format ( 6 - idx ), lat_sum_5432 [ - 1 ]) lat_sum_5432 . append ( lat ) p2345 = [ Conv2D ( 'posthoc_3x3_p{}' . format ( i + 2 ), c , num_channel , 3 ) for i , c in enumerate ( lat_sum_5432 [ ::- 1 ])] if use_gn: p2345 = [ GroupNorm ( 'gn_p{}' . format ( i + 2 ), c ) for i , c in enumerate ( p2345 )] p6 = MaxPooling ( 'maxpool_p6' , p2345 [ - 1 ], pool_size = 1 , strides = 2 , data_format='channels_first' , padding='VALID' ) return p2345 + [ p6 ]","title":"fpn_model"},{"location":"reference/mot/object_detection/modeling/model_fpn/#generate_fpn_proposals","text":"def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits View Source @under_name_scope () def generate_fpn_proposals ( multilevel_pred_boxes , multilevel_label_logits , image_shape2d ) : \"\"\" Args: multilevel_pred_boxes: #lvl HxWxAx4 boxes multilevel_label_logits: #lvl tensors of shape HxWxA Returns: boxes: kx4 float scores: k logits \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_pred_boxes ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl training = get_current_tower_context (). is_training all_boxes = [] all_scores = [] if cfg . FPN . PROPOSAL_MODE == 'Level' : fpn_nms_topk = cfg . RPN . TRAIN_PER_LEVEL_NMS_TOPK if training else cfg . RPN . TEST_PER_LEVEL_NMS_TOPK for lvl in range ( num_lvl ) : with tf . name_scope ( 'Lvl{}' . format ( lvl + 2 )) : pred_boxes_decoded = multilevel_pred_boxes [ lvl ] proposal_boxes , proposal_scores = generate_rpn_proposals ( tf . reshape ( pred_boxes_decoded , [ -1, 4 ] ), tf . reshape ( multilevel_label_logits [ lvl ] , [ -1 ] ), image_shape2d , fpn_nms_topk ) all_boxes . append ( proposal_boxes ) all_scores . append ( proposal_scores ) proposal_boxes = tf . concat ( all_boxes , axis = 0 ) # nx4 proposal_scores = tf . concat ( all_scores , axis = 0 ) # n # Here we are different from Detectron . # Detectron picks top - k within the batch , rather than within an image . However we do not have a batch . proposal_topk = tf . minimum ( tf . size ( proposal_scores ), fpn_nms_topk ) proposal_scores , topk_indices = tf . nn . top_k ( proposal_scores , k = proposal_topk , sorted = False ) proposal_boxes = tf . gather ( proposal_boxes , topk_indices , name = \"all_proposals\" ) else : for lvl in range ( num_lvl ) : with tf . name_scope ( 'Lvl{}' . format ( lvl + 2 )) : pred_boxes_decoded = multilevel_pred_boxes [ lvl ] all_boxes . append ( tf . reshape ( pred_boxes_decoded , [ -1, 4 ] )) all_scores . append ( tf . reshape ( multilevel_label_logits [ lvl ] , [ -1 ] )) all_boxes = tf . concat ( all_boxes , axis = 0 ) all_scores = tf . concat ( all_scores , axis = 0 ) proposal_boxes , proposal_scores = generate_rpn_proposals ( all_boxes , all_scores , image_shape2d , cfg . RPN . TRAIN_PRE_NMS_TOPK if training else cfg . RPN . TEST_PRE_NMS_TOPK , cfg . RPN . TRAIN_POST_NMS_TOPK if training else cfg . RPN . TEST_POST_NMS_TOPK ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), \\ tf . stop_gradient ( proposal_scores , name = 'scores' )","title":"generate_fpn_proposals"},{"location":"reference/mot/object_detection/modeling/model_fpn/#get_all_anchors_fpn","text":"def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ) Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. View Source @memoized def get_all_anchors_fpn ( * , strides , sizes , ratios , max_size ) : \"\"\" Returns: [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array. \"\"\" assert len ( strides ) == len ( sizes ) foas = [] for stride , size in zip ( strides , sizes ) : foa = get_all_anchors ( stride = stride , sizes = ( size ,), ratios = ratios , max_size = max_size ) foas . append ( foa ) return foas","title":"get_all_anchors_fpn"},{"location":"reference/mot/object_detection/modeling/model_fpn/#multilevel_roi_align","text":"def multilevel_roi_align ( features , rcnn_boxes , resolution ) Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res View Source @under_name_scope () def multilevel_roi_align ( features , rcnn_boxes , resolution ) : \"\"\" Args: features ([tf.Tensor]): 4 FPN feature level 2-5 rcnn_boxes (tf.Tensor): nx4 boxes resolution (int): output spatial resolution Returns: NxC x res x res \"\"\" assert len ( features ) == 4 , features # Reassign rcnn_boxes to levels level_ids , level_boxes = fpn_map_rois_to_levels ( rcnn_boxes ) all_rois = [] # Crop patches from corresponding levels for i , boxes , featuremap in zip ( itertools . count (), level_boxes , features ) : with tf . name_scope ( 'roi_level{}' . format ( i + 2 )) : boxes_on_featuremap = boxes * ( 1.0 / cfg . FPN . ANCHOR_STRIDES [ i ] ) all_rois . append ( roi_align ( featuremap , boxes_on_featuremap , resolution )) # this can fail if using TF <= 1.8 with MKL build all_rois = tf . concat ( all_rois , axis = 0 ) # NCHW # Unshuffle to the original order , to match the original samples level_id_perm = tf . concat ( level_ids , axis = 0 ) # A permutation of 1 ~ N level_id_invert_perm = tf . invert_permutation ( level_id_perm ) all_rois = tf . gather ( all_rois , level_id_invert_perm , name = \"output\" ) return all_rois","title":"multilevel_roi_align"},{"location":"reference/mot/object_detection/modeling/model_fpn/#multilevel_rpn_losses","text":"def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss View Source def multilevel_rpn_losses ( multilevel_anchors , multilevel_label_logits , multilevel_box_logits ) : \"\"\" Args: multilevel_anchors: #lvl RPNAnchors multilevel_label_logits: #lvl tensors of shape HxWxA multilevel_box_logits: #lvl tensors of shape HxWxAx4 Returns: label_loss, box_loss \"\"\" num_lvl = len ( cfg . FPN . ANCHOR_STRIDES ) assert len ( multilevel_anchors ) == num_lvl assert len ( multilevel_label_logits ) == num_lvl assert len ( multilevel_box_logits ) == num_lvl losses = [] with tf . name_scope ( 'rpn_losses' ) : for lvl in range ( num_lvl ) : anchors = multilevel_anchors [ lvl ] label_loss , box_loss = rpn_losses ( anchors . gt_labels , anchors . encoded_gt_boxes (), multilevel_label_logits [ lvl ] , multilevel_box_logits [ lvl ] , name_scope = 'level{}' . format ( lvl + 2 )) losses . extend ( [ label_loss, box_loss ] ) total_label_loss = tf . add_n ( losses [ ::2 ] , name = 'label_loss' ) total_box_loss = tf . add_n ( losses [ 1::2 ] , name = 'box_loss' ) add_moving_summary ( total_label_loss , total_box_loss ) return [ total_label_loss, total_box_loss ]","title":"multilevel_rpn_losses"},{"location":"reference/mot/object_detection/modeling/model_frcnn/","text":"Module mot.object_detection.modeling.model_frcnn View Source # -*- coding: utf-8 -*- # File: model_frcnn.py import tensorflow as tf from tensorpack.models import Conv2D , FullyConnected , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.common import get_tf_version_tuple from tensorpack.tfutils.scope_utils import under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.utils.argtools import memoized_method from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import pairwise_iou from mot.object_detection.modeling.model_box import decode_bbox_target , encode_bbox_target from mot.object_detection.modeling.backbone import GroupNorm @under_name_scope () def proposal_metrics ( iou ): \"\"\" Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt \"\"\" # find best roi for each gt, for summary only best_iou = tf . reduce_max ( iou , axis = 0 ) mean_best_iou = tf . reduce_mean ( best_iou , name = 'best_iou_per_gt' ) summaries = [ mean_best_iou ] with tf . device ( '/cpu:0' ): for th in [ 0.3 , 0.5 ]: recall = tf . truediv ( tf . count_nonzero ( best_iou >= th ), tf . size ( best_iou , out_type = tf . int64 ), name = 'recall_iou{}' . format ( th )) summaries . append ( recall ) add_moving_summary ( * summaries ) @under_name_scope () def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ): \"\"\" Sample some boxes from all proposals for training. #fg is guaranteed to be > 0, because ground truth boxes will be added as proposals. Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. \"\"\" iou = pairwise_iou ( boxes , gt_boxes ) # nxm proposal_metrics ( iou ) # add ground truth as proposals as well boxes = tf . concat ([ boxes , gt_boxes ], axis = 0 ) # (n+m) x 4 iou = tf . concat ([ iou , tf . eye ( tf . shape ( gt_boxes )[ 0 ])], axis = 0 ) # (n+m) x m # #proposal=n+m from now on def sample_fg_bg ( iou ): fg_mask = tf . cond ( tf . shape ( iou )[ 1 ] > 0 , lambda : tf . reduce_max ( iou , axis = 1 ) >= cfg . FRCNN . FG_THRESH , lambda : tf . zeros ([ tf . shape ( iou )[ 0 ]], dtype = tf . bool )) fg_inds = tf . reshape ( tf . where ( fg_mask ), [ - 1 ]) num_fg = tf . minimum ( int ( cfg . FRCNN . BATCH_PER_IM * cfg . FRCNN . FG_RATIO ), tf . size ( fg_inds ), name = 'num_fg' ) fg_inds = tf . random_shuffle ( fg_inds )[: num_fg ] bg_inds = tf . reshape ( tf . where ( tf . logical_not ( fg_mask )), [ - 1 ]) num_bg = tf . minimum ( cfg . FRCNN . BATCH_PER_IM - num_fg , tf . size ( bg_inds ), name = 'num_bg' ) bg_inds = tf . random_shuffle ( bg_inds )[: num_bg ] add_moving_summary ( num_fg , num_bg ) return fg_inds , bg_inds fg_inds , bg_inds = sample_fg_bg ( iou ) # fg,bg indices w.r.t proposals best_iou_ind = tf . cond ( tf . shape ( iou )[ 1 ] > 0 , lambda : tf . argmax ( iou , axis = 1 ), # #proposal, each in 0~m-1 lambda : tf . zeros ([ tf . shape ( iou )[ 0 ]], dtype = tf . int64 )) fg_inds_wrt_gt = tf . gather ( best_iou_ind , fg_inds ) # num_fg all_indices = tf . concat ([ fg_inds , bg_inds ], axis = 0 ) # indices w.r.t all n+m proposal boxes ret_boxes = tf . gather ( boxes , all_indices ) ret_labels = tf . concat ( [ tf . gather ( gt_labels , fg_inds_wrt_gt ), tf . zeros_like ( bg_inds , dtype = tf . int64 )], axis = 0 ) # stop the gradient -- they are meant to be training targets return BoxProposals ( tf . stop_gradient ( ret_boxes , name = 'sampled_proposal_boxes' ), tf . stop_gradient ( ret_labels , name = 'sampled_labels' ), tf . stop_gradient ( fg_inds_wrt_gt )) @layer_register ( log_shape = True ) def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ): \"\"\" Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic \"\"\" num_classes = num_categories + 1 classification = FullyConnected ( 'class' , feature , num_classes , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )) num_classes_for_box = 1 if class_agnostic_regression else num_classes box_regression = FullyConnected ( 'box' , feature , num_classes_for_box * 4 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) box_regression = tf . reshape ( box_regression , ( - 1 , num_classes_for_box , 4 ), name = 'output_box' ) return classification , box_regression @under_name_scope () def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ): \"\"\" Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss \"\"\" label_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = labels , logits = label_logits ) label_loss = tf . reduce_mean ( label_loss , name = 'label_loss' ) fg_inds = tf . where ( labels > 0 )[:, 0 ] fg_labels = tf . gather ( labels , fg_inds ) num_fg = tf . size ( fg_inds , out_type = tf . int64 ) empty_fg = tf . equal ( num_fg , 0 ) if int ( fg_box_logits . shape [ 1 ]) > 1 : if get_tf_version_tuple () >= ( 1 , 14 ): fg_labels = tf . expand_dims ( fg_labels , axis = 1 ) # nfg x 1 fg_box_logits = tf . gather ( fg_box_logits , fg_labels , batch_dims = 1 ) else : indices = tf . stack ([ tf . range ( num_fg ), fg_labels ], axis = 1 ) # nfgx2 fg_box_logits = tf . gather_nd ( fg_box_logits , indices ) fg_box_logits = tf . reshape ( fg_box_logits , [ - 1 , 4 ]) # nfg x 4 with tf . name_scope ( 'label_metrics' ), tf . device ( '/cpu:0' ): prediction = tf . argmax ( label_logits , axis = 1 , name = 'label_prediction' ) correct = tf . cast ( tf . equal ( prediction , labels ), tf . float32 ) # boolean/integer gather is unavailable on GPU accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) fg_label_pred = tf . argmax ( tf . gather ( label_logits , fg_inds ), axis = 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int64 ), name = 'num_zero' ) false_negative = tf . where ( empty_fg , 0. , tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 ), name = 'false_negative' ) fg_accuracy = tf . where ( empty_fg , 0. , tf . reduce_mean ( tf . gather ( correct , fg_inds )), name = 'fg_accuracy' ) box_loss = tf . reduce_sum ( tf . abs ( fg_boxes - fg_box_logits )) box_loss = tf . truediv ( box_loss , tf . cast ( tf . shape ( labels )[ 0 ], tf . float32 ), name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , accuracy , fg_accuracy , false_negative , tf . cast ( num_fg , tf . float32 , name = 'num_fg_label' )) return [ label_loss , box_loss ] @under_name_scope () def fastrcnn_predictions ( boxes , scores ): \"\"\" Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K \"\"\" assert boxes . shape [ 1 ] == scores . shape [ 1 ] boxes = tf . transpose ( boxes , [ 1 , 0 , 2 ])[ 1 :, :, :] # #catxnx4 scores = tf . transpose ( scores [:, 1 :], [ 1 , 0 ]) # #catxn max_coord = tf . reduce_max ( boxes ) filtered_ids = tf . where ( scores > cfg . TEST . RESULT_SCORE_THRESH ) # Fx2 filtered_boxes = tf . gather_nd ( boxes , filtered_ids ) # Fx4 filtered_scores = tf . gather_nd ( scores , filtered_ids ) # F, cls_per_box = tf . slice ( filtered_ids , [ 0 , 0 ], [ - 1 , 1 ]) offsets = tf . cast ( cls_per_box , tf . float32 ) * ( max_coord + 1 ) # F,1 nms_boxes = filtered_boxes + offsets selection = tf . image . non_max_suppression ( nms_boxes , filtered_scores , cfg . TEST . RESULTS_PER_IM , cfg . TEST . FRCNN_NMS_THRESH ) # The next lines are really dirty: it's a trick to return the scores for all classes final_scores = tf . gather ( tf . gather ( scores , filtered_ids [:, 1 ], axis = 1 ), selection , axis = 1 ) final_scores = tf . transpose ( final_scores , name = \"scores\" ) final_labels = tf . add ( tf . gather ( cls_per_box [:, 0 ], selection ), 1 , name = 'labels' ) final_boxes = tf . gather ( filtered_boxes , selection , name = 'boxes' ) return final_boxes , final_scores , final_labels \"\"\" FastRCNN heads for FPN: \"\"\" @layer_register ( log_shape = True ) def fastrcnn_2fc_head ( feature ): \"\"\" Args: feature (any shape): Returns: 2D head feature \"\"\" dim = cfg . FPN . FRCNN_FC_HEAD_DIM init = tf . variance_scaling_initializer () hidden = FullyConnected ( 'fc6' , feature , dim , kernel_initializer = init , activation = tf . nn . relu ) hidden = FullyConnected ( 'fc7' , hidden , dim , kernel_initializer = init , activation = tf . nn . relu ) return hidden @layer_register ( log_shape = True ) def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ): \"\"\" Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): for k in range ( num_convs ): l = Conv2D ( 'conv{}' . format ( k ), l , cfg . FPN . FRCNN_CONV_HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = FullyConnected ( 'fc' , l , cfg . FPN . FRCNN_FC_HEAD_DIM , kernel_initializer = tf . variance_scaling_initializer (), activation = tf . nn . relu ) return l def fastrcnn_4conv1fc_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , ** kwargs ) def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs ) class BoxProposals ( object ): \"\"\" A structure to manage box proposals and their relations with ground truth. \"\"\" def __init__ ( self , boxes , labels = None , fg_inds_wrt_gt = None ): \"\"\" Args: boxes: Nx4 labels: N, each in [0, #class), the true label for each input box fg_inds_wrt_gt: #fg, each in [0, M) The last four arguments could be None when not training. \"\"\" for k , v in locals () . items (): if k != 'self' and v is not None : setattr ( self , k , v ) @memoized_method def fg_inds ( self ): \"\"\" Returns: #fg indices in [0, N-1] \"\"\" return tf . reshape ( tf . where ( self . labels > 0 ), [ - 1 ], name = 'fg_inds' ) @memoized_method def fg_boxes ( self ): \"\"\" Returns: #fg x4\"\"\" return tf . gather ( self . boxes , self . fg_inds (), name = 'fg_boxes' ) @memoized_method def fg_labels ( self ): \"\"\" Returns: #fg\"\"\" return tf . gather ( self . labels , self . fg_inds (), name = 'fg_labels' ) class FastRCNNHead ( object ): \"\"\" A class to process & decode inputs/outputs of a fastrcnn classification+regression head. \"\"\" def __init__ ( self , proposals , box_logits , label_logits , gt_boxes , bbox_regression_weights ): \"\"\" Args: proposals: BoxProposals box_logits: Nx#classx4 or Nx1x4, the output of the head label_logits: Nx#class, the output of the head gt_boxes: Mx4 bbox_regression_weights: a 4 element tensor \"\"\" for k , v in locals () . items (): if k != 'self' and v is not None : setattr ( self , k , v ) self . _bbox_class_agnostic = int ( box_logits . shape [ 1 ]) == 1 self . _num_classes = box_logits . shape [ 1 ] @memoized_method def fg_box_logits ( self ): \"\"\" Returns: #fg x ? x 4 \"\"\" return tf . gather ( self . box_logits , self . proposals . fg_inds (), name = 'fg_box_logits' ) @memoized_method def losses ( self ): encoded_fg_gt_boxes = encode_bbox_target ( tf . gather ( self . gt_boxes , self . proposals . fg_inds_wrt_gt ), self . proposals . fg_boxes ()) * self . bbox_regression_weights return fastrcnn_losses ( self . proposals . labels , self . label_logits , encoded_fg_gt_boxes , self . fg_box_logits () ) @memoized_method def decoded_output_boxes ( self ): \"\"\" Returns: N x #class x 4 \"\"\" anchors = tf . tile ( tf . expand_dims ( self . proposals . boxes , 1 ), [ 1 , self . _num_classes , 1 ]) # N x #class x 4 decoded_boxes = decode_bbox_target ( self . box_logits / self . bbox_regression_weights , anchors ) return decoded_boxes @memoized_method def decoded_output_boxes_for_true_label ( self ): \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . proposals . labels ) @memoized_method def decoded_output_boxes_for_predicted_label ( self ): \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . predicted_labels ()) @memoized_method def decoded_output_boxes_for_label ( self , labels ): assert not self . _bbox_class_agnostic indices = tf . stack ([ tf . range ( tf . size ( labels , out_type = tf . int64 )), labels ]) needed_logits = tf . gather_nd ( self . box_logits , indices ) decoded = decode_bbox_target ( needed_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded @memoized_method def decoded_output_boxes_class_agnostic ( self ): \"\"\" Returns: Nx4 \"\"\" assert self . _bbox_class_agnostic box_logits = tf . reshape ( self . box_logits , [ - 1 , 4 ]) decoded = decode_bbox_target ( box_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded @memoized_method def output_scores ( self , name = None ): \"\"\" Returns: N x #class scores, summed to one for each box.\"\"\" return tf . nn . softmax ( self . label_logits , name = name ) @memoized_method def predicted_labels ( self ): \"\"\" Returns: N ints \"\"\" return tf . argmax ( self . label_logits , axis = 1 , name = 'predicted_labels' ) Functions fastrcnn_2fc_head def fastrcnn_2fc_head ( feature ) Args: feature (any shape): Returns: 2D head feature View Source @ layer_register ( log_shape = True ) def fastrcnn_2fc_head ( feature ): \"\"\" Args: feature (any shape): Returns: 2D head feature \"\"\" dim = cfg . FPN . FRCNN_FC_HEAD_DIM init = tf . variance_scaling_initializer () hidden = FullyConnected ( 'fc6' , feature , dim , kernel_initializer = init , activation = tf . nn . relu ) hidden = FullyConnected ( 'fc7' , hidden , dim , kernel_initializer = init , activation = tf . nn . relu ) return hidden fastrcnn_4conv1fc_gn_head def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ) View Source def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs ) fastrcnn_4conv1fc_head def fastrcnn_4conv1fc_head ( * args , ** kwargs ) View Source def fastrcnn_4conv1fc_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , ** kwargs ) fastrcnn_Xconv1fc_head def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ) Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature View Source @ layer_register ( log_shape = True ) def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ): \"\"\" Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): for k in range ( num_convs ): l = Conv2D ( 'conv{}' . format ( k ), l , cfg . FPN . FRCNN_CONV_HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = FullyConnected ( 'fc' , l , cfg . FPN . FRCNN_FC_HEAD_DIM , kernel_initializer = tf . variance_scaling_initializer (), activation = tf . nn . relu ) return l fastrcnn_losses def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ) Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss View Source @under_name_scope () def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ) : \"\"\" Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss \"\"\" label_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = labels , logits = label_logits ) label_loss = tf . reduce_mean ( label_loss , name = 'label_loss' ) fg_inds = tf . where ( labels > 0 ) [ :, 0 ] fg_labels = tf . gather ( labels , fg_inds ) num_fg = tf . size ( fg_inds , out_type = tf . int64 ) empty_fg = tf . equal ( num_fg , 0 ) if int ( fg_box_logits . shape [ 1 ] ) > 1 : if get_tf_version_tuple () >= ( 1 , 14 ) : fg_labels = tf . expand_dims ( fg_labels , axis = 1 ) # nfg x 1 fg_box_logits = tf . gather ( fg_box_logits , fg_labels , batch_dims = 1 ) else : indices = tf . stack ( [ tf.range(num_fg), fg_labels ] , axis = 1 ) # nfgx2 fg_box_logits = tf . gather_nd ( fg_box_logits , indices ) fg_box_logits = tf . reshape ( fg_box_logits , [ -1, 4 ] ) # nfg x 4 with tf . name_scope ( 'label_metrics' ), tf . device ( '/cpu:0' ) : prediction = tf . argmax ( label_logits , axis = 1 , name = 'label_prediction' ) correct = tf . cast ( tf . equal ( prediction , labels ), tf . float32 ) # boolean / integer gather is unavailable on GPU accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) fg_label_pred = tf . argmax ( tf . gather ( label_logits , fg_inds ), axis = 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int64 ), name = 'num_zero' ) false_negative = tf . where ( empty_fg , 0. , tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 ), name = 'false_negative' ) fg_accuracy = tf . where ( empty_fg , 0. , tf . reduce_mean ( tf . gather ( correct , fg_inds )), name = 'fg_accuracy' ) box_loss = tf . reduce_sum ( tf . abs ( fg_boxes - fg_box_logits )) box_loss = tf . truediv ( box_loss , tf . cast ( tf . shape ( labels ) [ 0 ] , tf . float32 ), name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , accuracy , fg_accuracy , false_negative , tf . cast ( num_fg , tf . float32 , name = 'num_fg_label' )) return [ label_loss, box_loss ] fastrcnn_outputs def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ) Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic View Source @layer_register ( log_shape = True ) def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ) : \"\"\" Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic \"\"\" num_classes = num_categories + 1 classification = FullyConnected ( 'class' , feature , num_classes , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )) num_classes_for_box = 1 if class_agnostic_regression else num_classes box_regression = FullyConnected ( 'box' , feature , num_classes_for_box * 4 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) box_regression = tf . reshape ( box_regression , ( - 1 , num_classes_for_box , 4 ), name = 'output_box' ) return classification , box_regression fastrcnn_predictions def fastrcnn_predictions ( boxes , scores ) Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K View Source @under_name_scope () def fastrcnn_predictions ( boxes , scores ) : \"\"\" Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K \"\"\" assert boxes . shape [ 1 ] == scores . shape [ 1 ] boxes = tf . transpose ( boxes , [ 1, 0, 2 ] ) [ 1:, :, : ] # #catxnx4 scores = tf . transpose ( scores [ :, 1: ] , [ 1, 0 ] ) # #catxn max_coord = tf . reduce_max ( boxes ) filtered_ids = tf . where ( scores > cfg . TEST . RESULT_SCORE_THRESH ) # Fx2 filtered_boxes = tf . gather_nd ( boxes , filtered_ids ) # Fx4 filtered_scores = tf . gather_nd ( scores , filtered_ids ) # F , cls_per_box = tf . slice ( filtered_ids , [ 0, 0 ] , [ -1, 1 ] ) offsets = tf . cast ( cls_per_box , tf . float32 ) * ( max_coord + 1 ) # F , 1 nms_boxes = filtered_boxes + offsets selection = tf . image . non_max_suppression ( nms_boxes , filtered_scores , cfg . TEST . RESULTS_PER_IM , cfg . TEST . FRCNN_NMS_THRESH ) # The next lines are really dirty : it 's a trick to return the scores for all classes final_scores = tf.gather(tf.gather(scores, filtered_ids[:, 1], axis=1), selection, axis=1) final_scores = tf.transpose(final_scores, name=\"scores\") final_labels = tf.add(tf.gather(cls_per_box[:, 0], selection), 1, name=' labels ') final_boxes = tf.gather(filtered_boxes, selection, name=' boxes ' ) return final_boxes , final_scores , final_labels proposal_metrics def proposal_metrics ( iou ) Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt View Source @under_name_scope () def proposal_metrics ( iou ) : \"\"\" Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt \"\"\" # find best roi for each gt , for summary only best_iou = tf . reduce_max ( iou , axis = 0 ) mean_best_iou = tf . reduce_mean ( best_iou , name = 'best_iou_per_gt' ) summaries = [ mean_best_iou ] with tf . device ( '/cpu:0' ) : for th in [ 0.3, 0.5 ] : recall = tf . truediv ( tf . count_nonzero ( best_iou >= th ), tf . size ( best_iou , out_type = tf . int64 ), name = 'recall_iou{}' . format ( th )) summaries . append ( recall ) add_moving_summary ( * summaries ) sample_fast_rcnn_targets def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ) Sample some boxes from all proposals for training. fg is guaranteed to be > 0, because ground truth boxes will be added as proposals. Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. View Source @under_name_scope () def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ) : \"\"\" Sample some boxes from all proposals for training. #fg is guaranteed to be > 0, because ground truth boxes will be added as proposals. Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. \"\"\" iou = pairwise_iou ( boxes , gt_boxes ) # nxm proposal_metrics ( iou ) # add ground truth as proposals as well boxes = tf . concat ( [ boxes, gt_boxes ] , axis = 0 ) # ( n + m ) x 4 iou = tf . concat ( [ iou, tf.eye(tf.shape(gt_boxes)[0 ] ) ] , axis = 0 ) # ( n + m ) x m # #proposal = n + m from now on def sample_fg_bg ( iou ) : fg_mask = tf . cond ( tf . shape ( iou ) [ 1 ] > 0 , lambda : tf . reduce_max ( iou , axis = 1 ) >= cfg . FRCNN . FG_THRESH , lambda : tf . zeros ( [ tf.shape(iou)[0 ] ] , dtype = tf . bool )) fg_inds = tf . reshape ( tf . where ( fg_mask ), [ -1 ] ) num_fg = tf . minimum ( int ( cfg . FRCNN . BATCH_PER_IM * cfg . FRCNN . FG_RATIO ), tf . size ( fg_inds ), name = 'num_fg' ) fg_inds = tf . random_shuffle ( fg_inds ) [ :num_fg ] bg_inds = tf . reshape ( tf . where ( tf . logical_not ( fg_mask )), [ -1 ] ) num_bg = tf . minimum ( cfg . FRCNN . BATCH_PER_IM - num_fg , tf . size ( bg_inds ), name = 'num_bg' ) bg_inds = tf . random_shuffle ( bg_inds ) [ :num_bg ] add_moving_summary ( num_fg , num_bg ) return fg_inds , bg_inds fg_inds , bg_inds = sample_fg_bg ( iou ) # fg , bg indices w . r . t proposals best_iou_ind = tf . cond ( tf . shape ( iou ) [ 1 ] > 0 , lambda : tf . argmax ( iou , axis = 1 ), # #proposal , each in 0 ~ m - 1 lambda : tf . zeros ( [ tf.shape(iou)[0 ] ] , dtype = tf . int64 )) fg_inds_wrt_gt = tf . gather ( best_iou_ind , fg_inds ) # num_fg all_indices = tf . concat ( [ fg_inds, bg_inds ] , axis = 0 ) # indices w . r . t all n + m proposal boxes ret_boxes = tf . gather ( boxes , all_indices ) ret_labels = tf . concat ( [ tf.gather(gt_labels, fg_inds_wrt_gt), tf.zeros_like(bg_inds, dtype=tf.int64) ] , axis = 0 ) # stop the gradient -- they are meant to be training targets return BoxProposals ( tf . stop_gradient ( ret_boxes , name = 'sampled_proposal_boxes' ), tf . stop_gradient ( ret_labels , name = 'sampled_labels' ), tf . stop_gradient ( fg_inds_wrt_gt )) Classes BoxProposals class BoxProposals ( boxes , labels = None , fg_inds_wrt_gt = None ) A structure to manage box proposals and their relations with ground truth. Methods fg_boxes def fg_boxes ( self ) Returns: #fg x4 View Source @memoized_method def fg_boxes ( self ) : \"\"\" Returns: #fg x4\"\"\" return tf . gather ( self . boxes , self . fg_inds (), name = 'fg_boxes' ) fg_inds def fg_inds ( self ) Returns: #fg indices in [0, N-1] View Source @memoized_method def fg_inds ( self ) : \"\"\" Returns: #fg indices in [0, N-1] \"\"\" return tf . reshape ( tf . where ( self . labels > 0 ), [ -1 ] , name = 'fg_inds' ) fg_labels def fg_labels ( self ) Returns: #fg View Source @memoized_method def fg_labels ( self ) : \"\"\" Returns: #fg\"\"\" return tf . gather ( self . labels , self . fg_inds (), name = 'fg_labels' ) FastRCNNHead class FastRCNNHead ( proposals , box_logits , label_logits , gt_boxes , bbox_regression_weights ) A class to process & decode inputs/outputs of a fastrcnn classification+regression head. Methods decoded_output_boxes def decoded_output_boxes ( self ) Returns: N x #class x 4 View Source @memoized_method def decoded_output_boxes ( self ) : \"\"\" Returns: N x #class x 4 \"\"\" anchors = tf . tile ( tf . expand_dims ( self . proposals . boxes , 1 ), [ 1, self._num_classes, 1 ] ) # N x #class x 4 decoded_boxes = decode_bbox_target ( self . box_logits / self . bbox_regression_weights , anchors ) return decoded_boxes decoded_output_boxes_class_agnostic def decoded_output_boxes_class_agnostic ( self ) Returns: Nx4 View Source @memoized_method def decoded_output_boxes_class_agnostic ( self ) : \"\"\" Returns: Nx4 \"\"\" assert self . _bbox_class_agnostic box_logits = tf . reshape ( self . box_logits , [ -1, 4 ] ) decoded = decode_bbox_target ( box_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded decoded_output_boxes_for_label def decoded_output_boxes_for_label ( self , labels ) View Source @memoized_method def decoded_output_boxes_for_label ( self , labels ) : assert not self . _bbox_class_agnostic indices = tf . stack ( [ tf.range(tf.size(labels, out_type=tf.int64)), labels ] ) needed_logits = tf . gather_nd ( self . box_logits , indices ) decoded = decode_bbox_target ( needed_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded decoded_output_boxes_for_predicted_label def decoded_output_boxes_for_predicted_label ( self ) Returns: Nx4 decoded boxes View Source @memoized_method def decoded_output_boxes_for_predicted_label ( self ) : \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . predicted_labels ()) decoded_output_boxes_for_true_label def decoded_output_boxes_for_true_label ( self ) Returns: Nx4 decoded boxes View Source @memoized_method def decoded_output_boxes_for_true_label ( self ) : \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . proposals . labels ) fg_box_logits def fg_box_logits ( self ) Returns: #fg x ? x 4 View Source @memoized_method def fg_box_logits ( self ) : \"\"\" Returns: #fg x ? x 4 \"\"\" return tf . gather ( self . box_logits , self . proposals . fg_inds (), name = 'fg_box_logits' ) losses def losses ( self ) View Source @memoized_method def losses ( self ) : encoded_fg_gt_boxes = encode_bbox_target ( tf . gather ( self . gt_boxes , self . proposals . fg_inds_wrt_gt ), self . proposals . fg_boxes ()) * self . bbox_regression_weights return fastrcnn_losses ( self . proposals . labels , self . label_logits , encoded_fg_gt_boxes , self . fg_box_logits () ) output_scores def output_scores ( self , name = None ) Returns: N x #class scores, summed to one for each box. View Source @memoized_method def output_scores ( self , name = None ) : \"\"\" Returns: N x #class scores, summed to one for each box.\"\"\" return tf . nn . softmax ( self . label_logits , name = name ) predicted_labels def predicted_labels ( self ) Returns: N ints View Source @memoized_method def predicted_labels ( self ) : \"\"\" Returns: N ints \"\"\" return tf . argmax ( self . label_logits , axis = 1 , name = 'predicted_labels' )","title":"Model Frcnn"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#module-motobject_detectionmodelingmodel_frcnn","text":"View Source # -*- coding: utf-8 -*- # File: model_frcnn.py import tensorflow as tf from tensorpack.models import Conv2D , FullyConnected , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.common import get_tf_version_tuple from tensorpack.tfutils.scope_utils import under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.utils.argtools import memoized_method from mot.object_detection.config import config as cfg from mot.object_detection.utils.box_ops import pairwise_iou from mot.object_detection.modeling.model_box import decode_bbox_target , encode_bbox_target from mot.object_detection.modeling.backbone import GroupNorm @under_name_scope () def proposal_metrics ( iou ): \"\"\" Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt \"\"\" # find best roi for each gt, for summary only best_iou = tf . reduce_max ( iou , axis = 0 ) mean_best_iou = tf . reduce_mean ( best_iou , name = 'best_iou_per_gt' ) summaries = [ mean_best_iou ] with tf . device ( '/cpu:0' ): for th in [ 0.3 , 0.5 ]: recall = tf . truediv ( tf . count_nonzero ( best_iou >= th ), tf . size ( best_iou , out_type = tf . int64 ), name = 'recall_iou{}' . format ( th )) summaries . append ( recall ) add_moving_summary ( * summaries ) @under_name_scope () def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ): \"\"\" Sample some boxes from all proposals for training. #fg is guaranteed to be > 0, because ground truth boxes will be added as proposals. Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. \"\"\" iou = pairwise_iou ( boxes , gt_boxes ) # nxm proposal_metrics ( iou ) # add ground truth as proposals as well boxes = tf . concat ([ boxes , gt_boxes ], axis = 0 ) # (n+m) x 4 iou = tf . concat ([ iou , tf . eye ( tf . shape ( gt_boxes )[ 0 ])], axis = 0 ) # (n+m) x m # #proposal=n+m from now on def sample_fg_bg ( iou ): fg_mask = tf . cond ( tf . shape ( iou )[ 1 ] > 0 , lambda : tf . reduce_max ( iou , axis = 1 ) >= cfg . FRCNN . FG_THRESH , lambda : tf . zeros ([ tf . shape ( iou )[ 0 ]], dtype = tf . bool )) fg_inds = tf . reshape ( tf . where ( fg_mask ), [ - 1 ]) num_fg = tf . minimum ( int ( cfg . FRCNN . BATCH_PER_IM * cfg . FRCNN . FG_RATIO ), tf . size ( fg_inds ), name = 'num_fg' ) fg_inds = tf . random_shuffle ( fg_inds )[: num_fg ] bg_inds = tf . reshape ( tf . where ( tf . logical_not ( fg_mask )), [ - 1 ]) num_bg = tf . minimum ( cfg . FRCNN . BATCH_PER_IM - num_fg , tf . size ( bg_inds ), name = 'num_bg' ) bg_inds = tf . random_shuffle ( bg_inds )[: num_bg ] add_moving_summary ( num_fg , num_bg ) return fg_inds , bg_inds fg_inds , bg_inds = sample_fg_bg ( iou ) # fg,bg indices w.r.t proposals best_iou_ind = tf . cond ( tf . shape ( iou )[ 1 ] > 0 , lambda : tf . argmax ( iou , axis = 1 ), # #proposal, each in 0~m-1 lambda : tf . zeros ([ tf . shape ( iou )[ 0 ]], dtype = tf . int64 )) fg_inds_wrt_gt = tf . gather ( best_iou_ind , fg_inds ) # num_fg all_indices = tf . concat ([ fg_inds , bg_inds ], axis = 0 ) # indices w.r.t all n+m proposal boxes ret_boxes = tf . gather ( boxes , all_indices ) ret_labels = tf . concat ( [ tf . gather ( gt_labels , fg_inds_wrt_gt ), tf . zeros_like ( bg_inds , dtype = tf . int64 )], axis = 0 ) # stop the gradient -- they are meant to be training targets return BoxProposals ( tf . stop_gradient ( ret_boxes , name = 'sampled_proposal_boxes' ), tf . stop_gradient ( ret_labels , name = 'sampled_labels' ), tf . stop_gradient ( fg_inds_wrt_gt )) @layer_register ( log_shape = True ) def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ): \"\"\" Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic \"\"\" num_classes = num_categories + 1 classification = FullyConnected ( 'class' , feature , num_classes , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )) num_classes_for_box = 1 if class_agnostic_regression else num_classes box_regression = FullyConnected ( 'box' , feature , num_classes_for_box * 4 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) box_regression = tf . reshape ( box_regression , ( - 1 , num_classes_for_box , 4 ), name = 'output_box' ) return classification , box_regression @under_name_scope () def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ): \"\"\" Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss \"\"\" label_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = labels , logits = label_logits ) label_loss = tf . reduce_mean ( label_loss , name = 'label_loss' ) fg_inds = tf . where ( labels > 0 )[:, 0 ] fg_labels = tf . gather ( labels , fg_inds ) num_fg = tf . size ( fg_inds , out_type = tf . int64 ) empty_fg = tf . equal ( num_fg , 0 ) if int ( fg_box_logits . shape [ 1 ]) > 1 : if get_tf_version_tuple () >= ( 1 , 14 ): fg_labels = tf . expand_dims ( fg_labels , axis = 1 ) # nfg x 1 fg_box_logits = tf . gather ( fg_box_logits , fg_labels , batch_dims = 1 ) else : indices = tf . stack ([ tf . range ( num_fg ), fg_labels ], axis = 1 ) # nfgx2 fg_box_logits = tf . gather_nd ( fg_box_logits , indices ) fg_box_logits = tf . reshape ( fg_box_logits , [ - 1 , 4 ]) # nfg x 4 with tf . name_scope ( 'label_metrics' ), tf . device ( '/cpu:0' ): prediction = tf . argmax ( label_logits , axis = 1 , name = 'label_prediction' ) correct = tf . cast ( tf . equal ( prediction , labels ), tf . float32 ) # boolean/integer gather is unavailable on GPU accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) fg_label_pred = tf . argmax ( tf . gather ( label_logits , fg_inds ), axis = 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int64 ), name = 'num_zero' ) false_negative = tf . where ( empty_fg , 0. , tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 ), name = 'false_negative' ) fg_accuracy = tf . where ( empty_fg , 0. , tf . reduce_mean ( tf . gather ( correct , fg_inds )), name = 'fg_accuracy' ) box_loss = tf . reduce_sum ( tf . abs ( fg_boxes - fg_box_logits )) box_loss = tf . truediv ( box_loss , tf . cast ( tf . shape ( labels )[ 0 ], tf . float32 ), name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , accuracy , fg_accuracy , false_negative , tf . cast ( num_fg , tf . float32 , name = 'num_fg_label' )) return [ label_loss , box_loss ] @under_name_scope () def fastrcnn_predictions ( boxes , scores ): \"\"\" Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K \"\"\" assert boxes . shape [ 1 ] == scores . shape [ 1 ] boxes = tf . transpose ( boxes , [ 1 , 0 , 2 ])[ 1 :, :, :] # #catxnx4 scores = tf . transpose ( scores [:, 1 :], [ 1 , 0 ]) # #catxn max_coord = tf . reduce_max ( boxes ) filtered_ids = tf . where ( scores > cfg . TEST . RESULT_SCORE_THRESH ) # Fx2 filtered_boxes = tf . gather_nd ( boxes , filtered_ids ) # Fx4 filtered_scores = tf . gather_nd ( scores , filtered_ids ) # F, cls_per_box = tf . slice ( filtered_ids , [ 0 , 0 ], [ - 1 , 1 ]) offsets = tf . cast ( cls_per_box , tf . float32 ) * ( max_coord + 1 ) # F,1 nms_boxes = filtered_boxes + offsets selection = tf . image . non_max_suppression ( nms_boxes , filtered_scores , cfg . TEST . RESULTS_PER_IM , cfg . TEST . FRCNN_NMS_THRESH ) # The next lines are really dirty: it's a trick to return the scores for all classes final_scores = tf . gather ( tf . gather ( scores , filtered_ids [:, 1 ], axis = 1 ), selection , axis = 1 ) final_scores = tf . transpose ( final_scores , name = \"scores\" ) final_labels = tf . add ( tf . gather ( cls_per_box [:, 0 ], selection ), 1 , name = 'labels' ) final_boxes = tf . gather ( filtered_boxes , selection , name = 'boxes' ) return final_boxes , final_scores , final_labels \"\"\" FastRCNN heads for FPN: \"\"\" @layer_register ( log_shape = True ) def fastrcnn_2fc_head ( feature ): \"\"\" Args: feature (any shape): Returns: 2D head feature \"\"\" dim = cfg . FPN . FRCNN_FC_HEAD_DIM init = tf . variance_scaling_initializer () hidden = FullyConnected ( 'fc6' , feature , dim , kernel_initializer = init , activation = tf . nn . relu ) hidden = FullyConnected ( 'fc7' , hidden , dim , kernel_initializer = init , activation = tf . nn . relu ) return hidden @layer_register ( log_shape = True ) def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ): \"\"\" Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): for k in range ( num_convs ): l = Conv2D ( 'conv{}' . format ( k ), l , cfg . FPN . FRCNN_CONV_HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = FullyConnected ( 'fc' , l , cfg . FPN . FRCNN_FC_HEAD_DIM , kernel_initializer = tf . variance_scaling_initializer (), activation = tf . nn . relu ) return l def fastrcnn_4conv1fc_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , ** kwargs ) def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs ) class BoxProposals ( object ): \"\"\" A structure to manage box proposals and their relations with ground truth. \"\"\" def __init__ ( self , boxes , labels = None , fg_inds_wrt_gt = None ): \"\"\" Args: boxes: Nx4 labels: N, each in [0, #class), the true label for each input box fg_inds_wrt_gt: #fg, each in [0, M) The last four arguments could be None when not training. \"\"\" for k , v in locals () . items (): if k != 'self' and v is not None : setattr ( self , k , v ) @memoized_method def fg_inds ( self ): \"\"\" Returns: #fg indices in [0, N-1] \"\"\" return tf . reshape ( tf . where ( self . labels > 0 ), [ - 1 ], name = 'fg_inds' ) @memoized_method def fg_boxes ( self ): \"\"\" Returns: #fg x4\"\"\" return tf . gather ( self . boxes , self . fg_inds (), name = 'fg_boxes' ) @memoized_method def fg_labels ( self ): \"\"\" Returns: #fg\"\"\" return tf . gather ( self . labels , self . fg_inds (), name = 'fg_labels' ) class FastRCNNHead ( object ): \"\"\" A class to process & decode inputs/outputs of a fastrcnn classification+regression head. \"\"\" def __init__ ( self , proposals , box_logits , label_logits , gt_boxes , bbox_regression_weights ): \"\"\" Args: proposals: BoxProposals box_logits: Nx#classx4 or Nx1x4, the output of the head label_logits: Nx#class, the output of the head gt_boxes: Mx4 bbox_regression_weights: a 4 element tensor \"\"\" for k , v in locals () . items (): if k != 'self' and v is not None : setattr ( self , k , v ) self . _bbox_class_agnostic = int ( box_logits . shape [ 1 ]) == 1 self . _num_classes = box_logits . shape [ 1 ] @memoized_method def fg_box_logits ( self ): \"\"\" Returns: #fg x ? x 4 \"\"\" return tf . gather ( self . box_logits , self . proposals . fg_inds (), name = 'fg_box_logits' ) @memoized_method def losses ( self ): encoded_fg_gt_boxes = encode_bbox_target ( tf . gather ( self . gt_boxes , self . proposals . fg_inds_wrt_gt ), self . proposals . fg_boxes ()) * self . bbox_regression_weights return fastrcnn_losses ( self . proposals . labels , self . label_logits , encoded_fg_gt_boxes , self . fg_box_logits () ) @memoized_method def decoded_output_boxes ( self ): \"\"\" Returns: N x #class x 4 \"\"\" anchors = tf . tile ( tf . expand_dims ( self . proposals . boxes , 1 ), [ 1 , self . _num_classes , 1 ]) # N x #class x 4 decoded_boxes = decode_bbox_target ( self . box_logits / self . bbox_regression_weights , anchors ) return decoded_boxes @memoized_method def decoded_output_boxes_for_true_label ( self ): \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . proposals . labels ) @memoized_method def decoded_output_boxes_for_predicted_label ( self ): \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . predicted_labels ()) @memoized_method def decoded_output_boxes_for_label ( self , labels ): assert not self . _bbox_class_agnostic indices = tf . stack ([ tf . range ( tf . size ( labels , out_type = tf . int64 )), labels ]) needed_logits = tf . gather_nd ( self . box_logits , indices ) decoded = decode_bbox_target ( needed_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded @memoized_method def decoded_output_boxes_class_agnostic ( self ): \"\"\" Returns: Nx4 \"\"\" assert self . _bbox_class_agnostic box_logits = tf . reshape ( self . box_logits , [ - 1 , 4 ]) decoded = decode_bbox_target ( box_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded @memoized_method def output_scores ( self , name = None ): \"\"\" Returns: N x #class scores, summed to one for each box.\"\"\" return tf . nn . softmax ( self . label_logits , name = name ) @memoized_method def predicted_labels ( self ): \"\"\" Returns: N ints \"\"\" return tf . argmax ( self . label_logits , axis = 1 , name = 'predicted_labels' )","title":"Module mot.object_detection.modeling.model_frcnn"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_2fc_head","text":"def fastrcnn_2fc_head ( feature ) Args: feature (any shape): Returns: 2D head feature View Source @ layer_register ( log_shape = True ) def fastrcnn_2fc_head ( feature ): \"\"\" Args: feature (any shape): Returns: 2D head feature \"\"\" dim = cfg . FPN . FRCNN_FC_HEAD_DIM init = tf . variance_scaling_initializer () hidden = FullyConnected ( 'fc6' , feature , dim , kernel_initializer = init , activation = tf . nn . relu ) hidden = FullyConnected ( 'fc7' , hidden , dim , kernel_initializer = init , activation = tf . nn . relu ) return hidden","title":"fastrcnn_2fc_head"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_4conv1fc_gn_head","text":"def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ) View Source def fastrcnn_4conv1fc_gn_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs )","title":"fastrcnn_4conv1fc_gn_head"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_4conv1fc_head","text":"def fastrcnn_4conv1fc_head ( * args , ** kwargs ) View Source def fastrcnn_4conv1fc_head ( * args , ** kwargs ): return fastrcnn_Xconv1fc_head ( * args , num_convs = 4 , ** kwargs )","title":"fastrcnn_4conv1fc_head"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_xconv1fc_head","text":"def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ) Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature View Source @ layer_register ( log_shape = True ) def fastrcnn_Xconv1fc_head ( feature , num_convs , norm = None ): \"\"\" Args: feature (NCHW): num_classes(int): num_category + 1 num_convs (int): number of conv layers norm (str or None): either None or 'GN' Returns: 2D head feature \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): for k in range ( num_convs ): l = Conv2D ( 'conv{}' . format ( k ), l , cfg . FPN . FRCNN_CONV_HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = FullyConnected ( 'fc' , l , cfg . FPN . FRCNN_FC_HEAD_DIM , kernel_initializer = tf . variance_scaling_initializer (), activation = tf . nn . relu ) return l","title":"fastrcnn_Xconv1fc_head"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_losses","text":"def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ) Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss View Source @under_name_scope () def fastrcnn_losses ( labels , label_logits , fg_boxes , fg_box_logits ) : \"\"\" Args: labels: n, label_logits: nxC fg_boxes: nfgx4, encoded fg_box_logits: nfgxCx4 or nfgx1x4 if class agnostic Returns: label_loss, box_loss \"\"\" label_loss = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = labels , logits = label_logits ) label_loss = tf . reduce_mean ( label_loss , name = 'label_loss' ) fg_inds = tf . where ( labels > 0 ) [ :, 0 ] fg_labels = tf . gather ( labels , fg_inds ) num_fg = tf . size ( fg_inds , out_type = tf . int64 ) empty_fg = tf . equal ( num_fg , 0 ) if int ( fg_box_logits . shape [ 1 ] ) > 1 : if get_tf_version_tuple () >= ( 1 , 14 ) : fg_labels = tf . expand_dims ( fg_labels , axis = 1 ) # nfg x 1 fg_box_logits = tf . gather ( fg_box_logits , fg_labels , batch_dims = 1 ) else : indices = tf . stack ( [ tf.range(num_fg), fg_labels ] , axis = 1 ) # nfgx2 fg_box_logits = tf . gather_nd ( fg_box_logits , indices ) fg_box_logits = tf . reshape ( fg_box_logits , [ -1, 4 ] ) # nfg x 4 with tf . name_scope ( 'label_metrics' ), tf . device ( '/cpu:0' ) : prediction = tf . argmax ( label_logits , axis = 1 , name = 'label_prediction' ) correct = tf . cast ( tf . equal ( prediction , labels ), tf . float32 ) # boolean / integer gather is unavailable on GPU accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) fg_label_pred = tf . argmax ( tf . gather ( label_logits , fg_inds ), axis = 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int64 ), name = 'num_zero' ) false_negative = tf . where ( empty_fg , 0. , tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 ), name = 'false_negative' ) fg_accuracy = tf . where ( empty_fg , 0. , tf . reduce_mean ( tf . gather ( correct , fg_inds )), name = 'fg_accuracy' ) box_loss = tf . reduce_sum ( tf . abs ( fg_boxes - fg_box_logits )) box_loss = tf . truediv ( box_loss , tf . cast ( tf . shape ( labels ) [ 0 ] , tf . float32 ), name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , accuracy , fg_accuracy , false_negative , tf . cast ( num_fg , tf . float32 , name = 'num_fg_label' )) return [ label_loss, box_loss ]","title":"fastrcnn_losses"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_outputs","text":"def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ) Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic View Source @layer_register ( log_shape = True ) def fastrcnn_outputs ( feature , num_categories , class_agnostic_regression = False ) : \"\"\" Args: feature (any shape): num_categories (int): class_agnostic_regression (bool): if True, regression to N x 1 x 4 Returns: cls_logits: N x num_class classification logits reg_logits: N x num_classx4 or Nx1x4 if class agnostic \"\"\" num_classes = num_categories + 1 classification = FullyConnected ( 'class' , feature , num_classes , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )) num_classes_for_box = 1 if class_agnostic_regression else num_classes box_regression = FullyConnected ( 'box' , feature , num_classes_for_box * 4 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) box_regression = tf . reshape ( box_regression , ( - 1 , num_classes_for_box , 4 ), name = 'output_box' ) return classification , box_regression","title":"fastrcnn_outputs"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnn_predictions","text":"def fastrcnn_predictions ( boxes , scores ) Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K View Source @under_name_scope () def fastrcnn_predictions ( boxes , scores ) : \"\"\" Generate final results from predictions of all proposals. Args: boxes: n#classx4 floatbox in float32 scores: nx#class Returns: boxes: Kx4 scores: K labels: K \"\"\" assert boxes . shape [ 1 ] == scores . shape [ 1 ] boxes = tf . transpose ( boxes , [ 1, 0, 2 ] ) [ 1:, :, : ] # #catxnx4 scores = tf . transpose ( scores [ :, 1: ] , [ 1, 0 ] ) # #catxn max_coord = tf . reduce_max ( boxes ) filtered_ids = tf . where ( scores > cfg . TEST . RESULT_SCORE_THRESH ) # Fx2 filtered_boxes = tf . gather_nd ( boxes , filtered_ids ) # Fx4 filtered_scores = tf . gather_nd ( scores , filtered_ids ) # F , cls_per_box = tf . slice ( filtered_ids , [ 0, 0 ] , [ -1, 1 ] ) offsets = tf . cast ( cls_per_box , tf . float32 ) * ( max_coord + 1 ) # F , 1 nms_boxes = filtered_boxes + offsets selection = tf . image . non_max_suppression ( nms_boxes , filtered_scores , cfg . TEST . RESULTS_PER_IM , cfg . TEST . FRCNN_NMS_THRESH ) # The next lines are really dirty : it 's a trick to return the scores for all classes final_scores = tf.gather(tf.gather(scores, filtered_ids[:, 1], axis=1), selection, axis=1) final_scores = tf.transpose(final_scores, name=\"scores\") final_labels = tf.add(tf.gather(cls_per_box[:, 0], selection), 1, name=' labels ') final_boxes = tf.gather(filtered_boxes, selection, name=' boxes ' ) return final_boxes , final_scores , final_labels","title":"fastrcnn_predictions"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#proposal_metrics","text":"def proposal_metrics ( iou ) Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt View Source @under_name_scope () def proposal_metrics ( iou ) : \"\"\" Add summaries for RPN proposals. Args: iou: nxm, #proposal x #gt \"\"\" # find best roi for each gt , for summary only best_iou = tf . reduce_max ( iou , axis = 0 ) mean_best_iou = tf . reduce_mean ( best_iou , name = 'best_iou_per_gt' ) summaries = [ mean_best_iou ] with tf . device ( '/cpu:0' ) : for th in [ 0.3, 0.5 ] : recall = tf . truediv ( tf . count_nonzero ( best_iou >= th ), tf . size ( best_iou , out_type = tf . int64 ), name = 'recall_iou{}' . format ( th )) summaries . append ( recall ) add_moving_summary ( * summaries )","title":"proposal_metrics"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#sample_fast_rcnn_targets","text":"def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ) Sample some boxes from all proposals for training.","title":"sample_fast_rcnn_targets"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fg-is-guaranteed-to-be-0-because-ground-truth-boxes-will-be-added-as-proposals","text":"Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. View Source @under_name_scope () def sample_fast_rcnn_targets ( boxes , gt_boxes , gt_labels ) : \"\"\" Sample some boxes from all proposals for training. #fg is guaranteed to be > 0, because ground truth boxes will be added as proposals. Args: boxes: nx4 region proposals, floatbox gt_boxes: mx4, floatbox gt_labels: m, int32 Returns: A BoxProposals instance, with: sampled_boxes: tx4 floatbox, the rois sampled_labels: t int64 labels, in [0, #class). Positive means foreground. fg_inds_wrt_gt: #fg indices, each in range [0, m-1]. It contains the matching GT of each foreground roi. \"\"\" iou = pairwise_iou ( boxes , gt_boxes ) # nxm proposal_metrics ( iou ) # add ground truth as proposals as well boxes = tf . concat ( [ boxes, gt_boxes ] , axis = 0 ) # ( n + m ) x 4 iou = tf . concat ( [ iou, tf.eye(tf.shape(gt_boxes)[0 ] ) ] , axis = 0 ) # ( n + m ) x m # #proposal = n + m from now on def sample_fg_bg ( iou ) : fg_mask = tf . cond ( tf . shape ( iou ) [ 1 ] > 0 , lambda : tf . reduce_max ( iou , axis = 1 ) >= cfg . FRCNN . FG_THRESH , lambda : tf . zeros ( [ tf.shape(iou)[0 ] ] , dtype = tf . bool )) fg_inds = tf . reshape ( tf . where ( fg_mask ), [ -1 ] ) num_fg = tf . minimum ( int ( cfg . FRCNN . BATCH_PER_IM * cfg . FRCNN . FG_RATIO ), tf . size ( fg_inds ), name = 'num_fg' ) fg_inds = tf . random_shuffle ( fg_inds ) [ :num_fg ] bg_inds = tf . reshape ( tf . where ( tf . logical_not ( fg_mask )), [ -1 ] ) num_bg = tf . minimum ( cfg . FRCNN . BATCH_PER_IM - num_fg , tf . size ( bg_inds ), name = 'num_bg' ) bg_inds = tf . random_shuffle ( bg_inds ) [ :num_bg ] add_moving_summary ( num_fg , num_bg ) return fg_inds , bg_inds fg_inds , bg_inds = sample_fg_bg ( iou ) # fg , bg indices w . r . t proposals best_iou_ind = tf . cond ( tf . shape ( iou ) [ 1 ] > 0 , lambda : tf . argmax ( iou , axis = 1 ), # #proposal , each in 0 ~ m - 1 lambda : tf . zeros ( [ tf.shape(iou)[0 ] ] , dtype = tf . int64 )) fg_inds_wrt_gt = tf . gather ( best_iou_ind , fg_inds ) # num_fg all_indices = tf . concat ( [ fg_inds, bg_inds ] , axis = 0 ) # indices w . r . t all n + m proposal boxes ret_boxes = tf . gather ( boxes , all_indices ) ret_labels = tf . concat ( [ tf.gather(gt_labels, fg_inds_wrt_gt), tf.zeros_like(bg_inds, dtype=tf.int64) ] , axis = 0 ) # stop the gradient -- they are meant to be training targets return BoxProposals ( tf . stop_gradient ( ret_boxes , name = 'sampled_proposal_boxes' ), tf . stop_gradient ( ret_labels , name = 'sampled_labels' ), tf . stop_gradient ( fg_inds_wrt_gt ))","title":"fg is guaranteed to be &gt; 0, because ground truth boxes will be added as proposals."},{"location":"reference/mot/object_detection/modeling/model_frcnn/#classes","text":"","title":"Classes"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#boxproposals","text":"class BoxProposals ( boxes , labels = None , fg_inds_wrt_gt = None ) A structure to manage box proposals and their relations with ground truth.","title":"BoxProposals"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#methods","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fg_boxes","text":"def fg_boxes ( self ) Returns: #fg x4 View Source @memoized_method def fg_boxes ( self ) : \"\"\" Returns: #fg x4\"\"\" return tf . gather ( self . boxes , self . fg_inds (), name = 'fg_boxes' )","title":"fg_boxes"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fg_inds","text":"def fg_inds ( self ) Returns: #fg indices in [0, N-1] View Source @memoized_method def fg_inds ( self ) : \"\"\" Returns: #fg indices in [0, N-1] \"\"\" return tf . reshape ( tf . where ( self . labels > 0 ), [ -1 ] , name = 'fg_inds' )","title":"fg_inds"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fg_labels","text":"def fg_labels ( self ) Returns: #fg View Source @memoized_method def fg_labels ( self ) : \"\"\" Returns: #fg\"\"\" return tf . gather ( self . labels , self . fg_inds (), name = 'fg_labels' )","title":"fg_labels"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fastrcnnhead","text":"class FastRCNNHead ( proposals , box_logits , label_logits , gt_boxes , bbox_regression_weights ) A class to process & decode inputs/outputs of a fastrcnn classification+regression head.","title":"FastRCNNHead"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#decoded_output_boxes","text":"def decoded_output_boxes ( self ) Returns: N x #class x 4 View Source @memoized_method def decoded_output_boxes ( self ) : \"\"\" Returns: N x #class x 4 \"\"\" anchors = tf . tile ( tf . expand_dims ( self . proposals . boxes , 1 ), [ 1, self._num_classes, 1 ] ) # N x #class x 4 decoded_boxes = decode_bbox_target ( self . box_logits / self . bbox_regression_weights , anchors ) return decoded_boxes","title":"decoded_output_boxes"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#decoded_output_boxes_class_agnostic","text":"def decoded_output_boxes_class_agnostic ( self ) Returns: Nx4 View Source @memoized_method def decoded_output_boxes_class_agnostic ( self ) : \"\"\" Returns: Nx4 \"\"\" assert self . _bbox_class_agnostic box_logits = tf . reshape ( self . box_logits , [ -1, 4 ] ) decoded = decode_bbox_target ( box_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded","title":"decoded_output_boxes_class_agnostic"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#decoded_output_boxes_for_label","text":"def decoded_output_boxes_for_label ( self , labels ) View Source @memoized_method def decoded_output_boxes_for_label ( self , labels ) : assert not self . _bbox_class_agnostic indices = tf . stack ( [ tf.range(tf.size(labels, out_type=tf.int64)), labels ] ) needed_logits = tf . gather_nd ( self . box_logits , indices ) decoded = decode_bbox_target ( needed_logits / self . bbox_regression_weights , self . proposals . boxes ) return decoded","title":"decoded_output_boxes_for_label"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#decoded_output_boxes_for_predicted_label","text":"def decoded_output_boxes_for_predicted_label ( self ) Returns: Nx4 decoded boxes View Source @memoized_method def decoded_output_boxes_for_predicted_label ( self ) : \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . predicted_labels ())","title":"decoded_output_boxes_for_predicted_label"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#decoded_output_boxes_for_true_label","text":"def decoded_output_boxes_for_true_label ( self ) Returns: Nx4 decoded boxes View Source @memoized_method def decoded_output_boxes_for_true_label ( self ) : \"\"\" Returns: Nx4 decoded boxes \"\"\" return self . _decoded_output_boxes_for_label ( self . proposals . labels )","title":"decoded_output_boxes_for_true_label"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#fg_box_logits","text":"def fg_box_logits ( self ) Returns: #fg x ? x 4 View Source @memoized_method def fg_box_logits ( self ) : \"\"\" Returns: #fg x ? x 4 \"\"\" return tf . gather ( self . box_logits , self . proposals . fg_inds (), name = 'fg_box_logits' )","title":"fg_box_logits"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#losses","text":"def losses ( self ) View Source @memoized_method def losses ( self ) : encoded_fg_gt_boxes = encode_bbox_target ( tf . gather ( self . gt_boxes , self . proposals . fg_inds_wrt_gt ), self . proposals . fg_boxes ()) * self . bbox_regression_weights return fastrcnn_losses ( self . proposals . labels , self . label_logits , encoded_fg_gt_boxes , self . fg_box_logits () )","title":"losses"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#output_scores","text":"def output_scores ( self , name = None ) Returns: N x #class scores, summed to one for each box. View Source @memoized_method def output_scores ( self , name = None ) : \"\"\" Returns: N x #class scores, summed to one for each box.\"\"\" return tf . nn . softmax ( self . label_logits , name = name )","title":"output_scores"},{"location":"reference/mot/object_detection/modeling/model_frcnn/#predicted_labels","text":"def predicted_labels ( self ) Returns: N ints View Source @memoized_method def predicted_labels ( self ) : \"\"\" Returns: N ints \"\"\" return tf . argmax ( self . label_logits , axis = 1 , name = 'predicted_labels' )","title":"predicted_labels"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/","text":"Module mot.object_detection.modeling.model_mrcnn View Source # -*- coding : utf - 8 -*- import tensorflow as tf from tensorpack . models import Conv2D , Conv2DTranspose , layer_register from tensorpack . tfutils . argscope import argscope from tensorpack . tfutils . common import get_tf_version_tuple from tensorpack . tfutils . scope_utils import under_name_scope from tensorpack . tfutils . summary import add_moving_summary from mot . object_detection . modeling . backbone import GroupNorm from mot . object_detection . config import config as cfg @ under_name_scope () def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) : \"\"\" Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 \"\"\" if get_tf_version_tuple () >= ( 1 , 14 ) : mask_logits = tf . gather ( mask_logits , tf . reshape ( fg_labels - 1 , [ - 1 , 1 ]), batch_dims = 1 ) else : indices = tf . stack ([ tf . range ( tf . size ( fg_labels , out_type = tf . int64 )), fg_labels - 1 ], axis = 1 ) # #fgx2 mask_logits = tf . gather_nd ( mask_logits , indices ) # #fg x h x w mask_logits = tf . squeeze ( mask_logits , axis = 1 ) mask_probs = tf . sigmoid ( mask_logits ) # add some training visualizations to tensorboard with tf . name_scope ( 'mask_viz' ) : viz = tf . concat ([ fg_target_masks , mask_probs ], axis = 1 ) viz = tf . expand_dims ( viz , 3 ) viz = tf . cast ( viz * 255 , tf . uint8 , name='viz' ) tf . summary . image ( 'mask_truth|pred' , viz , max_outputs = 10 ) loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = fg_target_masks , logits = mask_logits ) loss = tf . reduce_mean ( loss , name='maskrcnn_loss' ) pred_label = mask_probs > 0.5 truth_label = fg_target_masks > 0.5 accuracy = tf . reduce_mean ( tf . cast ( tf . equal ( pred_label , truth_label ), tf . float32 ), name='accuracy' ) pos_accuracy = tf . logical_and ( tf . equal ( pred_label , truth_label ), tf . equal ( truth_label , True )) pos_accuracy = tf . reduce_mean ( tf . cast ( pos_accuracy , tf . float32 ), name='pos_accuracy' ) fg_pixel_ratio = tf . reduce_mean ( tf . cast ( truth_label , tf . float32 ), name='fg_pixel_ratio' ) add_moving_summary ( loss , accuracy , fg_pixel_ratio , pos_accuracy ) return loss @ layer_register ( log_shape = True ) def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ) : \"\"\" Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ([ Conv2D , Conv2DTranspose ], data_format='channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode='fan_out' , distribution='untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )) : # c2's MSRAFill is fan_out for k in range(num_convs): l = Conv2D('fcn {} '.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu) if norm is not None: l = GroupNorm('gn {} '.format(k), l) l = Conv2DTranspose('deconv', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu) l = Conv2D('conv', l, num_category, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.001)) return l def maskrcnn_up4conv_head(*args, **kwargs): return maskrcnn_upXconv_head(*args, num_convs=4, **kwargs) def maskrcnn_up4conv_gn_head(*args, **kwargs): return maskrcnn_upXconv_head(*args, num_convs=4, norm=' GN ' , **kwargs ) def unpackbits_masks ( masks ) : \"\"\" Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of `np.packbits` \"\"\" assert masks . dtype == tf . uint8 , masks bits = tf . constant (( 128 , 64 , 32 , 16 , 8 , 4 , 2 , 1 ), dtype = tf . uint8 ) unpacked = tf . bitwise . bitwise_and ( tf . expand_dims ( masks , - 1 ), bits ) > 0 unpacked = tf . reshape ( unpacked , tf . concat ([ tf . shape ( masks )[:- 1 ], [ 8 * tf . shape ( masks )[ - 1 ]]], axis = 0 )) return unpacked Functions maskrcnn_loss def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 View Source @under_name_scope () def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) : \"\"\" Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 \"\"\" if get_tf_version_tuple () >= ( 1 , 14 ) : mask_logits = tf . gather ( mask_logits , tf . reshape ( fg_labels - 1 , [ -1, 1 ] ), batch_dims = 1 ) else : indices = tf . stack ( [ tf.range(tf.size(fg_labels, out_type=tf.int64)), fg_labels - 1 ] , axis = 1 ) # #fgx2 mask_logits = tf . gather_nd ( mask_logits , indices ) # #fg x h x w mask_logits = tf . squeeze ( mask_logits , axis = 1 ) mask_probs = tf . sigmoid ( mask_logits ) # add some training visualizations to tensorboard with tf . name_scope ( 'mask_viz' ) : viz = tf . concat ( [ fg_target_masks, mask_probs ] , axis = 1 ) viz = tf . expand_dims ( viz , 3 ) viz = tf . cast ( viz * 255 , tf . uint8 , name = 'viz' ) tf . summary . image ( 'mask_truth|pred' , viz , max_outputs = 10 ) loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = fg_target_masks , logits = mask_logits ) loss = tf . reduce_mean ( loss , name = 'maskrcnn_loss' ) pred_label = mask_probs > 0.5 truth_label = fg_target_masks > 0.5 accuracy = tf . reduce_mean ( tf . cast ( tf . equal ( pred_label , truth_label ), tf . float32 ), name = 'accuracy' ) pos_accuracy = tf . logical_and ( tf . equal ( pred_label , truth_label ), tf . equal ( truth_label , True )) pos_accuracy = tf . reduce_mean ( tf . cast ( pos_accuracy , tf . float32 ), name = 'pos_accuracy' ) fg_pixel_ratio = tf . reduce_mean ( tf . cast ( truth_label , tf . float32 ), name = 'fg_pixel_ratio' ) add_moving_summary ( loss , accuracy , fg_pixel_ratio , pos_accuracy ) return loss maskrcnn_up4conv_gn_head def maskrcnn_up4conv_gn_head ( * args , ** kwargs ) View Source def maskrcnn_up4conv_gn_head ( * args , ** kwargs ): return maskrcnn_upXconv_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs ) maskrcnn_up4conv_head def maskrcnn_up4conv_head ( * args , ** kwargs ) View Source def maskrcnn_up4conv_head ( * args , ** kwargs ): return maskrcnn_upXconv_head ( * args , num_convs = 4 , ** kwargs ) maskrcnn_upXconv_head def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ) Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): View Source @ layer_register ( log_shape = True ) def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ): \"\"\" Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ([ Conv2D , Conv2DTranspose ], data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): # c2's MSRAFill is fan_out for k in range ( num_convs ): l = Conv2D ( 'fcn{}' . format ( k ), l , cfg . MRCNN . HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = Conv2DTranspose ( 'deconv' , l , cfg . MRCNN . HEAD_DIM , 2 , strides = 2 , activation = tf . nn . relu ) l = Conv2D ( 'conv' , l , num_category , 1 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) return l unpackbits_masks def unpackbits_masks ( masks ) Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of np.packbits View Source def unpackbits_masks ( masks ) : \"\"\" Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of `np.packbits` \"\"\" assert masks . dtype == tf . uint8 , masks bits = tf . constant (( 128 , 64 , 32 , 16 , 8 , 4 , 2 , 1 ), dtype = tf . uint8 ) unpacked = tf . bitwise . bitwise_and ( tf . expand_dims ( masks , - 1 ), bits ) > 0 unpacked = tf . reshape ( unpacked , tf . concat ([ tf . shape ( masks )[:- 1 ], [ 8 * tf . shape ( masks )[ - 1 ]]], axis = 0 )) return unpacked","title":"Model Mrcnn"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#module-motobject_detectionmodelingmodel_mrcnn","text":"View Source # -*- coding : utf - 8 -*- import tensorflow as tf from tensorpack . models import Conv2D , Conv2DTranspose , layer_register from tensorpack . tfutils . argscope import argscope from tensorpack . tfutils . common import get_tf_version_tuple from tensorpack . tfutils . scope_utils import under_name_scope from tensorpack . tfutils . summary import add_moving_summary from mot . object_detection . modeling . backbone import GroupNorm from mot . object_detection . config import config as cfg @ under_name_scope () def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) : \"\"\" Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 \"\"\" if get_tf_version_tuple () >= ( 1 , 14 ) : mask_logits = tf . gather ( mask_logits , tf . reshape ( fg_labels - 1 , [ - 1 , 1 ]), batch_dims = 1 ) else : indices = tf . stack ([ tf . range ( tf . size ( fg_labels , out_type = tf . int64 )), fg_labels - 1 ], axis = 1 ) # #fgx2 mask_logits = tf . gather_nd ( mask_logits , indices ) # #fg x h x w mask_logits = tf . squeeze ( mask_logits , axis = 1 ) mask_probs = tf . sigmoid ( mask_logits ) # add some training visualizations to tensorboard with tf . name_scope ( 'mask_viz' ) : viz = tf . concat ([ fg_target_masks , mask_probs ], axis = 1 ) viz = tf . expand_dims ( viz , 3 ) viz = tf . cast ( viz * 255 , tf . uint8 , name='viz' ) tf . summary . image ( 'mask_truth|pred' , viz , max_outputs = 10 ) loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = fg_target_masks , logits = mask_logits ) loss = tf . reduce_mean ( loss , name='maskrcnn_loss' ) pred_label = mask_probs > 0.5 truth_label = fg_target_masks > 0.5 accuracy = tf . reduce_mean ( tf . cast ( tf . equal ( pred_label , truth_label ), tf . float32 ), name='accuracy' ) pos_accuracy = tf . logical_and ( tf . equal ( pred_label , truth_label ), tf . equal ( truth_label , True )) pos_accuracy = tf . reduce_mean ( tf . cast ( pos_accuracy , tf . float32 ), name='pos_accuracy' ) fg_pixel_ratio = tf . reduce_mean ( tf . cast ( truth_label , tf . float32 ), name='fg_pixel_ratio' ) add_moving_summary ( loss , accuracy , fg_pixel_ratio , pos_accuracy ) return loss @ layer_register ( log_shape = True ) def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ) : \"\"\" Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ([ Conv2D , Conv2DTranspose ], data_format='channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode='fan_out' , distribution='untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )) : # c2's MSRAFill is fan_out for k in range(num_convs): l = Conv2D('fcn {} '.format(k), l, cfg.MRCNN.HEAD_DIM, 3, activation=tf.nn.relu) if norm is not None: l = GroupNorm('gn {} '.format(k), l) l = Conv2DTranspose('deconv', l, cfg.MRCNN.HEAD_DIM, 2, strides=2, activation=tf.nn.relu) l = Conv2D('conv', l, num_category, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.001)) return l def maskrcnn_up4conv_head(*args, **kwargs): return maskrcnn_upXconv_head(*args, num_convs=4, **kwargs) def maskrcnn_up4conv_gn_head(*args, **kwargs): return maskrcnn_upXconv_head(*args, num_convs=4, norm=' GN ' , **kwargs ) def unpackbits_masks ( masks ) : \"\"\" Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of `np.packbits` \"\"\" assert masks . dtype == tf . uint8 , masks bits = tf . constant (( 128 , 64 , 32 , 16 , 8 , 4 , 2 , 1 ), dtype = tf . uint8 ) unpacked = tf . bitwise . bitwise_and ( tf . expand_dims ( masks , - 1 ), bits ) > 0 unpacked = tf . reshape ( unpacked , tf . concat ([ tf . shape ( masks )[:- 1 ], [ 8 * tf . shape ( masks )[ - 1 ]]], axis = 0 )) return unpacked","title":"Module mot.object_detection.modeling.model_mrcnn"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#maskrcnn_loss","text":"def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 View Source @under_name_scope () def maskrcnn_loss ( mask_logits , fg_labels , fg_target_masks ) : \"\"\" Args: mask_logits: #fg x #category xhxw fg_labels: #fg, in 1~#class, int64 fg_target_masks: #fgxhxw, float32 \"\"\" if get_tf_version_tuple () >= ( 1 , 14 ) : mask_logits = tf . gather ( mask_logits , tf . reshape ( fg_labels - 1 , [ -1, 1 ] ), batch_dims = 1 ) else : indices = tf . stack ( [ tf.range(tf.size(fg_labels, out_type=tf.int64)), fg_labels - 1 ] , axis = 1 ) # #fgx2 mask_logits = tf . gather_nd ( mask_logits , indices ) # #fg x h x w mask_logits = tf . squeeze ( mask_logits , axis = 1 ) mask_probs = tf . sigmoid ( mask_logits ) # add some training visualizations to tensorboard with tf . name_scope ( 'mask_viz' ) : viz = tf . concat ( [ fg_target_masks, mask_probs ] , axis = 1 ) viz = tf . expand_dims ( viz , 3 ) viz = tf . cast ( viz * 255 , tf . uint8 , name = 'viz' ) tf . summary . image ( 'mask_truth|pred' , viz , max_outputs = 10 ) loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = fg_target_masks , logits = mask_logits ) loss = tf . reduce_mean ( loss , name = 'maskrcnn_loss' ) pred_label = mask_probs > 0.5 truth_label = fg_target_masks > 0.5 accuracy = tf . reduce_mean ( tf . cast ( tf . equal ( pred_label , truth_label ), tf . float32 ), name = 'accuracy' ) pos_accuracy = tf . logical_and ( tf . equal ( pred_label , truth_label ), tf . equal ( truth_label , True )) pos_accuracy = tf . reduce_mean ( tf . cast ( pos_accuracy , tf . float32 ), name = 'pos_accuracy' ) fg_pixel_ratio = tf . reduce_mean ( tf . cast ( truth_label , tf . float32 ), name = 'fg_pixel_ratio' ) add_moving_summary ( loss , accuracy , fg_pixel_ratio , pos_accuracy ) return loss","title":"maskrcnn_loss"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#maskrcnn_up4conv_gn_head","text":"def maskrcnn_up4conv_gn_head ( * args , ** kwargs ) View Source def maskrcnn_up4conv_gn_head ( * args , ** kwargs ): return maskrcnn_upXconv_head ( * args , num_convs = 4 , norm = 'GN' , ** kwargs )","title":"maskrcnn_up4conv_gn_head"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#maskrcnn_up4conv_head","text":"def maskrcnn_up4conv_head ( * args , ** kwargs ) View Source def maskrcnn_up4conv_head ( * args , ** kwargs ): return maskrcnn_upXconv_head ( * args , num_convs = 4 , ** kwargs )","title":"maskrcnn_up4conv_head"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#maskrcnn_upxconv_head","text":"def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ) Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): View Source @ layer_register ( log_shape = True ) def maskrcnn_upXconv_head ( feature , num_category , num_convs , norm = None ): \"\"\" Args: feature (NxCx s x s): size is 7 in C4 models and 14 in FPN models. num_category(int): num_convs (int): number of convolution layers norm (str or None): either None or 'GN' Returns: mask_logits (N x num_category x 2s x 2s): \"\"\" assert norm in [ None , 'GN' ], norm l = feature with argscope ([ Conv2D , Conv2DTranspose ], data_format = 'channels_first' , kernel_initializer = tf . variance_scaling_initializer ( scale = 2.0 , mode = 'fan_out' , distribution = 'untruncated_normal' if get_tf_version_tuple () >= ( 1 , 12 ) else 'normal' )): # c2's MSRAFill is fan_out for k in range ( num_convs ): l = Conv2D ( 'fcn{}' . format ( k ), l , cfg . MRCNN . HEAD_DIM , 3 , activation = tf . nn . relu ) if norm is not None : l = GroupNorm ( 'gn{}' . format ( k ), l ) l = Conv2DTranspose ( 'deconv' , l , cfg . MRCNN . HEAD_DIM , 2 , strides = 2 , activation = tf . nn . relu ) l = Conv2D ( 'conv' , l , num_category , 1 , kernel_initializer = tf . random_normal_initializer ( stddev = 0.001 )) return l","title":"maskrcnn_upXconv_head"},{"location":"reference/mot/object_detection/modeling/model_mrcnn/#unpackbits_masks","text":"def unpackbits_masks ( masks ) Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of np.packbits View Source def unpackbits_masks ( masks ) : \"\"\" Args: masks (Tensor): uint8 Tensor of shape N, H, W. The last dimension is packed bits. Returns: masks (Tensor): bool Tensor of shape N, H, 8*W. This is a reverse operation of `np.packbits` \"\"\" assert masks . dtype == tf . uint8 , masks bits = tf . constant (( 128 , 64 , 32 , 16 , 8 , 4 , 2 , 1 ), dtype = tf . uint8 ) unpacked = tf . bitwise . bitwise_and ( tf . expand_dims ( masks , - 1 ), bits ) > 0 unpacked = tf . reshape ( unpacked , tf . concat ([ tf . shape ( masks )[:- 1 ], [ 8 * tf . shape ( masks )[ - 1 ]]], axis = 0 )) return unpacked","title":"unpackbits_masks"},{"location":"reference/mot/object_detection/modeling/model_rpn/","text":"Module mot.object_detection.modeling.model_rpn View Source # -*- coding: utf-8 -*- import tensorflow as tf import numpy as np from tensorpack.models import Conv2D , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope , under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.utils.argtools import memoized from mot.object_detection.config import config as cfg from mot.object_detection.modeling.model_box import clip_boxes @layer_register ( log_shape = True ) @auto_reuse_variable_scope def rpn_head ( featuremap , channel , num_anchors ): \"\"\" Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 \"\"\" with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )): hidden = Conv2D ( 'conv0' , featuremap , channel , 3 , activation = tf . nn . relu ) label_logits = Conv2D ( 'class' , hidden , num_anchors , 1 ) box_logits = Conv2D ( 'box' , hidden , 4 * num_anchors , 1 ) # 1, NA(*4), im/16, im/16 (NCHW) label_logits = tf . transpose ( label_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWxNA label_logits = tf . squeeze ( label_logits , 0 ) # fHxfWxNA shp = tf . shape ( box_logits ) # 1x(NAx4)xfHxfW box_logits = tf . transpose ( box_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWx(NAx4) box_logits = tf . reshape ( box_logits , tf . stack ([ shp [ 2 ], shp [ 3 ], num_anchors , 4 ])) # fHxfWxNAx4 return label_logits , box_logits @under_name_scope () def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ): \"\"\" Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss \"\"\" with tf . device ( '/cpu:0' ): valid_mask = tf . stop_gradient ( tf . not_equal ( anchor_labels , - 1 )) pos_mask = tf . stop_gradient ( tf . equal ( anchor_labels , 1 )) nr_valid = tf . stop_gradient ( tf . count_nonzero ( valid_mask , dtype = tf . int32 ), name = 'num_valid_anchor' ) nr_pos = tf . identity ( tf . count_nonzero ( pos_mask , dtype = tf . int32 ), name = 'num_pos_anchor' ) # nr_pos is guaranteed >0 in C4. But in FPN. even nr_valid could be 0. valid_anchor_labels = tf . boolean_mask ( anchor_labels , valid_mask ) valid_label_logits = tf . boolean_mask ( label_logits , valid_mask ) with tf . name_scope ( 'label_metrics' ): valid_label_prob = tf . nn . sigmoid ( valid_label_logits ) summaries = [] with tf . device ( '/cpu:0' ): for th in [ 0.5 , 0.2 , 0.1 ]: valid_prediction = tf . cast ( valid_label_prob > th , tf . int32 ) nr_pos_prediction = tf . reduce_sum ( valid_prediction , name = 'num_pos_prediction' ) pos_prediction_corr = tf . count_nonzero ( tf . logical_and ( valid_label_prob > th , tf . equal ( valid_prediction , valid_anchor_labels )), dtype = tf . int32 ) placeholder = 0.5 # A small value will make summaries appear lower. recall = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos ), tf . float32 ) recall = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , recall , name = 'recall_th {} ' . format ( th )) precision = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos_prediction ), tf . float32 ) precision = tf . where ( tf . equal ( nr_pos_prediction , 0 ), placeholder , precision , name = 'precision_th {} ' . format ( th )) summaries . extend ([ precision , recall ]) add_moving_summary ( * summaries ) # Per-level loss summaries in FPN may appear lower due to the use of a small placeholder. # But the total RPN loss will be fine. TODO make the summary op smarter placeholder = 0. label_loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = tf . cast ( valid_anchor_labels , tf . float32 ), logits = valid_label_logits ) label_loss = tf . reduce_sum ( label_loss ) * ( 1. / cfg . RPN . BATCH_PER_IM ) label_loss = tf . where ( tf . equal ( nr_valid , 0 ), placeholder , label_loss , name = 'label_loss' ) pos_anchor_boxes = tf . boolean_mask ( anchor_boxes , pos_mask ) pos_box_logits = tf . boolean_mask ( box_logits , pos_mask ) delta = 1.0 / 9 box_loss = tf . losses . huber_loss ( pos_anchor_boxes , pos_box_logits , delta = delta , reduction = tf . losses . Reduction . SUM ) / delta box_loss = box_loss * ( 1. / cfg . RPN . BATCH_PER_IM ) box_loss = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , box_loss , name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , nr_valid , nr_pos ) return [ label_loss , box_loss ] @under_name_scope () def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ): \"\"\" Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits \"\"\" assert boxes . shape . ndims == 2 , boxes . shape if post_nms_topk is None : post_nms_topk = pre_nms_topk topk = tf . minimum ( pre_nms_topk , tf . size ( scores )) topk_scores , topk_indices = tf . nn . top_k ( scores , k = topk , sorted = False ) topk_boxes = tf . gather ( boxes , topk_indices ) topk_boxes = clip_boxes ( topk_boxes , img_shape ) if cfg . RPN . MIN_SIZE > 0 : topk_boxes_x1y1x2y2 = tf . reshape ( topk_boxes , ( - 1 , 2 , 2 )) topk_boxes_x1y1 , topk_boxes_x2y2 = tf . split ( topk_boxes_x1y1x2y2 , 2 , axis = 1 ) # nx1x2 each wbhb = tf . squeeze ( topk_boxes_x2y2 - topk_boxes_x1y1 , axis = 1 ) valid = tf . reduce_all ( wbhb > cfg . RPN . MIN_SIZE , axis = 1 ) # n, topk_valid_boxes = tf . boolean_mask ( topk_boxes , valid ) topk_valid_scores = tf . boolean_mask ( topk_scores , valid ) else : topk_valid_boxes = topk_boxes topk_valid_scores = topk_scores nms_indices = tf . image . non_max_suppression ( topk_valid_boxes , topk_valid_scores , max_output_size = post_nms_topk , iou_threshold = cfg . RPN . PROPOSAL_NMS_THRESH ) proposal_boxes = tf . gather ( topk_valid_boxes , nms_indices ) proposal_scores = tf . gather ( topk_valid_scores , nms_indices ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), tf . stop_gradient ( proposal_scores , name = 'scores' ) @memoized def get_all_anchors ( * , stride , sizes , ratios , max_size ): \"\"\" Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. \"\"\" # Generates a NAx4 matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors # are centered on 0, have sqrt areas equal to the specified sizes, and aspect ratios as given. anchors = [] for sz in sizes : for ratio in ratios : w = np . sqrt ( sz * sz / ratio ) h = ratio * w anchors . append ([ - w , - h , w , h ]) cell_anchors = np . asarray ( anchors ) * 0.5 field_size = int ( np . ceil ( max_size / stride )) shifts = ( np . arange ( 0 , field_size ) * stride ) . astype ( \"float32\" ) shift_x , shift_y = np . meshgrid ( shifts , shifts ) shift_x = shift_x . flatten () shift_y = shift_y . flatten () shifts = np . vstack (( shift_x , shift_y , shift_x , shift_y )) . transpose () # Kx4, K = field_size * field_size K = shifts . shape [ 0 ] A = cell_anchors . shape [ 0 ] field_of_anchors = cell_anchors . reshape (( 1 , A , 4 )) + shifts . reshape (( 1 , K , 4 )) . transpose (( 1 , 0 , 2 )) field_of_anchors = field_of_anchors . reshape (( field_size , field_size , A , 4 )) # FSxFSxAx4 # Many rounding happens inside the anchor code anyway # assert np.all(field_of_anchors == field_of_anchors.astype('int32')) field_of_anchors = field_of_anchors . astype ( \"float32\" ) return field_of_anchors Functions generate_rpn_proposals def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ) Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits View Source @under_name_scope () def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ) : \"\"\" Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits \"\"\" assert boxes . shape . ndims == 2 , boxes . shape if post_nms_topk is None : post_nms_topk = pre_nms_topk topk = tf . minimum ( pre_nms_topk , tf . size ( scores )) topk_scores , topk_indices = tf . nn . top_k ( scores , k = topk , sorted = False ) topk_boxes = tf . gather ( boxes , topk_indices ) topk_boxes = clip_boxes ( topk_boxes , img_shape ) if cfg . RPN . MIN_SIZE > 0 : topk_boxes_x1y1x2y2 = tf . reshape ( topk_boxes , ( - 1 , 2 , 2 )) topk_boxes_x1y1 , topk_boxes_x2y2 = tf . split ( topk_boxes_x1y1x2y2 , 2 , axis = 1 ) # nx1x2 each wbhb = tf . squeeze ( topk_boxes_x2y2 - topk_boxes_x1y1 , axis = 1 ) valid = tf . reduce_all ( wbhb > cfg . RPN . MIN_SIZE , axis = 1 ) # n , topk_valid_boxes = tf . boolean_mask ( topk_boxes , valid ) topk_valid_scores = tf . boolean_mask ( topk_scores , valid ) else : topk_valid_boxes = topk_boxes topk_valid_scores = topk_scores nms_indices = tf . image . non_max_suppression ( topk_valid_boxes , topk_valid_scores , max_output_size = post_nms_topk , iou_threshold = cfg . RPN . PROPOSAL_NMS_THRESH ) proposal_boxes = tf . gather ( topk_valid_boxes , nms_indices ) proposal_scores = tf . gather ( topk_valid_scores , nms_indices ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), tf . stop_gradient ( proposal_scores , name = 'scores' ) get_all_anchors def get_all_anchors ( * , stride , sizes , ratios , max_size ) Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. View Source @memoized def get_all_anchors ( * , stride , sizes , ratios , max_size ) : \"\"\" Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. \"\"\" # Generates a NAx4 matrix of anchor boxes in ( x1 , y1 , x2 , y2 ) format . Anchors # are centered on 0 , have sqrt areas equal to the specified sizes , and aspect ratios as given . anchors = [] for sz in sizes : for ratio in ratios : w = np . sqrt ( sz * sz / ratio ) h = ratio * w anchors . append ( [ -w, -h, w, h ] ) cell_anchors = np . asarray ( anchors ) * 0.5 field_size = int ( np . ceil ( max_size / stride )) shifts = ( np . arange ( 0 , field_size ) * stride ). astype ( \"float32\" ) shift_x , shift_y = np . meshgrid ( shifts , shifts ) shift_x = shift_x . flatten () shift_y = shift_y . flatten () shifts = np . vstack (( shift_x , shift_y , shift_x , shift_y )). transpose () # Kx4 , K = field_size * field_size K = shifts . shape [ 0 ] A = cell_anchors . shape [ 0 ] field_of_anchors = cell_anchors . reshape (( 1 , A , 4 )) + shifts . reshape (( 1 , K , 4 )). transpose (( 1 , 0 , 2 )) field_of_anchors = field_of_anchors . reshape (( field_size , field_size , A , 4 )) # FSxFSxAx4 # Many rounding happens inside the anchor code anyway # assert np . all ( field_of_anchors == field_of_anchors . astype ( 'int32' )) field_of_anchors = field_of_anchors . astype ( \"float32\" ) return field_of_anchors rpn_head def rpn_head ( featuremap , channel , num_anchors ) Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 View Source @ layer_register ( log_shape = True ) @ auto_reuse_variable_scope def rpn_head ( featuremap , channel , num_anchors ): \"\"\" Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 \"\"\" with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )): hidden = Conv2D ( 'conv0' , featuremap , channel , 3 , activation = tf . nn . relu ) label_logits = Conv2D ( 'class' , hidden , num_anchors , 1 ) box_logits = Conv2D ( 'box' , hidden , 4 * num_anchors , 1 ) # 1, NA(*4), im/16, im/16 (NCHW) label_logits = tf . transpose ( label_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWxNA label_logits = tf . squeeze ( label_logits , 0 ) # fHxfWxNA shp = tf . shape ( box_logits ) # 1x(NAx4)xfHxfW box_logits = tf . transpose ( box_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWx(NAx4) box_logits = tf . reshape ( box_logits , tf . stack ([ shp [ 2 ], shp [ 3 ], num_anchors , 4 ])) # fHxfWxNAx4 return label_logits , box_logits rpn_losses def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ) Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss View Source @under_name_scope () def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ) : \"\"\" Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss \"\"\" with tf . device ( '/cpu:0' ) : valid_mask = tf . stop_gradient ( tf . not_equal ( anchor_labels , - 1 )) pos_mask = tf . stop_gradient ( tf . equal ( anchor_labels , 1 )) nr_valid = tf . stop_gradient ( tf . count_nonzero ( valid_mask , dtype = tf . int32 ), name = 'num_valid_anchor' ) nr_pos = tf . identity ( tf . count_nonzero ( pos_mask , dtype = tf . int32 ), name = 'num_pos_anchor' ) # nr_pos is guaranteed > 0 in C4 . But in FPN . even nr_valid could be 0. valid_anchor_labels = tf . boolean_mask ( anchor_labels , valid_mask ) valid_label_logits = tf . boolean_mask ( label_logits , valid_mask ) with tf . name_scope ( 'label_metrics' ) : valid_label_prob = tf . nn . sigmoid ( valid_label_logits ) summaries = [] with tf . device ( '/cpu:0' ) : for th in [ 0.5, 0.2, 0.1 ] : valid_prediction = tf . cast ( valid_label_prob > th , tf . int32 ) nr_pos_prediction = tf . reduce_sum ( valid_prediction , name = 'num_pos_prediction' ) pos_prediction_corr = tf . count_nonzero ( tf . logical_and ( valid_label_prob > th , tf . equal ( valid_prediction , valid_anchor_labels )), dtype = tf . int32 ) placeholder = 0.5 # A small value will make summaries appear lower . recall = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos ), tf . float32 ) recall = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , recall , name = 'recall_th{}' . format ( th )) precision = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos_prediction ), tf . float32 ) precision = tf . where ( tf . equal ( nr_pos_prediction , 0 ), placeholder , precision , name = 'precision_th{}' . format ( th )) summaries . extend ( [ precision, recall ] ) add_moving_summary ( * summaries ) # Per - level loss summaries in FPN may appear lower due to the use of a small placeholder . # But the total RPN loss will be fine . TODO make the summary op smarter placeholder = 0. label_loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = tf . cast ( valid_anchor_labels , tf . float32 ), logits = valid_label_logits ) label_loss = tf . reduce_sum ( label_loss ) * ( 1. / cfg . RPN . BATCH_PER_IM ) label_loss = tf . where ( tf . equal ( nr_valid , 0 ), placeholder , label_loss , name = 'label_loss' ) pos_anchor_boxes = tf . boolean_mask ( anchor_boxes , pos_mask ) pos_box_logits = tf . boolean_mask ( box_logits , pos_mask ) delta = 1.0 / 9 box_loss = tf . losses . huber_loss ( pos_anchor_boxes , pos_box_logits , delta = delta , reduction = tf . losses . Reduction . SUM ) / delta box_loss = box_loss * ( 1. / cfg . RPN . BATCH_PER_IM ) box_loss = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , box_loss , name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , nr_valid , nr_pos ) return [ label_loss, box_loss ]","title":"Model Rpn"},{"location":"reference/mot/object_detection/modeling/model_rpn/#module-motobject_detectionmodelingmodel_rpn","text":"View Source # -*- coding: utf-8 -*- import tensorflow as tf import numpy as np from tensorpack.models import Conv2D , layer_register from tensorpack.tfutils.argscope import argscope from tensorpack.tfutils.scope_utils import auto_reuse_variable_scope , under_name_scope from tensorpack.tfutils.summary import add_moving_summary from tensorpack.utils.argtools import memoized from mot.object_detection.config import config as cfg from mot.object_detection.modeling.model_box import clip_boxes @layer_register ( log_shape = True ) @auto_reuse_variable_scope def rpn_head ( featuremap , channel , num_anchors ): \"\"\" Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 \"\"\" with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )): hidden = Conv2D ( 'conv0' , featuremap , channel , 3 , activation = tf . nn . relu ) label_logits = Conv2D ( 'class' , hidden , num_anchors , 1 ) box_logits = Conv2D ( 'box' , hidden , 4 * num_anchors , 1 ) # 1, NA(*4), im/16, im/16 (NCHW) label_logits = tf . transpose ( label_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWxNA label_logits = tf . squeeze ( label_logits , 0 ) # fHxfWxNA shp = tf . shape ( box_logits ) # 1x(NAx4)xfHxfW box_logits = tf . transpose ( box_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWx(NAx4) box_logits = tf . reshape ( box_logits , tf . stack ([ shp [ 2 ], shp [ 3 ], num_anchors , 4 ])) # fHxfWxNAx4 return label_logits , box_logits @under_name_scope () def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ): \"\"\" Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss \"\"\" with tf . device ( '/cpu:0' ): valid_mask = tf . stop_gradient ( tf . not_equal ( anchor_labels , - 1 )) pos_mask = tf . stop_gradient ( tf . equal ( anchor_labels , 1 )) nr_valid = tf . stop_gradient ( tf . count_nonzero ( valid_mask , dtype = tf . int32 ), name = 'num_valid_anchor' ) nr_pos = tf . identity ( tf . count_nonzero ( pos_mask , dtype = tf . int32 ), name = 'num_pos_anchor' ) # nr_pos is guaranteed >0 in C4. But in FPN. even nr_valid could be 0. valid_anchor_labels = tf . boolean_mask ( anchor_labels , valid_mask ) valid_label_logits = tf . boolean_mask ( label_logits , valid_mask ) with tf . name_scope ( 'label_metrics' ): valid_label_prob = tf . nn . sigmoid ( valid_label_logits ) summaries = [] with tf . device ( '/cpu:0' ): for th in [ 0.5 , 0.2 , 0.1 ]: valid_prediction = tf . cast ( valid_label_prob > th , tf . int32 ) nr_pos_prediction = tf . reduce_sum ( valid_prediction , name = 'num_pos_prediction' ) pos_prediction_corr = tf . count_nonzero ( tf . logical_and ( valid_label_prob > th , tf . equal ( valid_prediction , valid_anchor_labels )), dtype = tf . int32 ) placeholder = 0.5 # A small value will make summaries appear lower. recall = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos ), tf . float32 ) recall = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , recall , name = 'recall_th {} ' . format ( th )) precision = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos_prediction ), tf . float32 ) precision = tf . where ( tf . equal ( nr_pos_prediction , 0 ), placeholder , precision , name = 'precision_th {} ' . format ( th )) summaries . extend ([ precision , recall ]) add_moving_summary ( * summaries ) # Per-level loss summaries in FPN may appear lower due to the use of a small placeholder. # But the total RPN loss will be fine. TODO make the summary op smarter placeholder = 0. label_loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = tf . cast ( valid_anchor_labels , tf . float32 ), logits = valid_label_logits ) label_loss = tf . reduce_sum ( label_loss ) * ( 1. / cfg . RPN . BATCH_PER_IM ) label_loss = tf . where ( tf . equal ( nr_valid , 0 ), placeholder , label_loss , name = 'label_loss' ) pos_anchor_boxes = tf . boolean_mask ( anchor_boxes , pos_mask ) pos_box_logits = tf . boolean_mask ( box_logits , pos_mask ) delta = 1.0 / 9 box_loss = tf . losses . huber_loss ( pos_anchor_boxes , pos_box_logits , delta = delta , reduction = tf . losses . Reduction . SUM ) / delta box_loss = box_loss * ( 1. / cfg . RPN . BATCH_PER_IM ) box_loss = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , box_loss , name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , nr_valid , nr_pos ) return [ label_loss , box_loss ] @under_name_scope () def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ): \"\"\" Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits \"\"\" assert boxes . shape . ndims == 2 , boxes . shape if post_nms_topk is None : post_nms_topk = pre_nms_topk topk = tf . minimum ( pre_nms_topk , tf . size ( scores )) topk_scores , topk_indices = tf . nn . top_k ( scores , k = topk , sorted = False ) topk_boxes = tf . gather ( boxes , topk_indices ) topk_boxes = clip_boxes ( topk_boxes , img_shape ) if cfg . RPN . MIN_SIZE > 0 : topk_boxes_x1y1x2y2 = tf . reshape ( topk_boxes , ( - 1 , 2 , 2 )) topk_boxes_x1y1 , topk_boxes_x2y2 = tf . split ( topk_boxes_x1y1x2y2 , 2 , axis = 1 ) # nx1x2 each wbhb = tf . squeeze ( topk_boxes_x2y2 - topk_boxes_x1y1 , axis = 1 ) valid = tf . reduce_all ( wbhb > cfg . RPN . MIN_SIZE , axis = 1 ) # n, topk_valid_boxes = tf . boolean_mask ( topk_boxes , valid ) topk_valid_scores = tf . boolean_mask ( topk_scores , valid ) else : topk_valid_boxes = topk_boxes topk_valid_scores = topk_scores nms_indices = tf . image . non_max_suppression ( topk_valid_boxes , topk_valid_scores , max_output_size = post_nms_topk , iou_threshold = cfg . RPN . PROPOSAL_NMS_THRESH ) proposal_boxes = tf . gather ( topk_valid_boxes , nms_indices ) proposal_scores = tf . gather ( topk_valid_scores , nms_indices ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), tf . stop_gradient ( proposal_scores , name = 'scores' ) @memoized def get_all_anchors ( * , stride , sizes , ratios , max_size ): \"\"\" Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. \"\"\" # Generates a NAx4 matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors # are centered on 0, have sqrt areas equal to the specified sizes, and aspect ratios as given. anchors = [] for sz in sizes : for ratio in ratios : w = np . sqrt ( sz * sz / ratio ) h = ratio * w anchors . append ([ - w , - h , w , h ]) cell_anchors = np . asarray ( anchors ) * 0.5 field_size = int ( np . ceil ( max_size / stride )) shifts = ( np . arange ( 0 , field_size ) * stride ) . astype ( \"float32\" ) shift_x , shift_y = np . meshgrid ( shifts , shifts ) shift_x = shift_x . flatten () shift_y = shift_y . flatten () shifts = np . vstack (( shift_x , shift_y , shift_x , shift_y )) . transpose () # Kx4, K = field_size * field_size K = shifts . shape [ 0 ] A = cell_anchors . shape [ 0 ] field_of_anchors = cell_anchors . reshape (( 1 , A , 4 )) + shifts . reshape (( 1 , K , 4 )) . transpose (( 1 , 0 , 2 )) field_of_anchors = field_of_anchors . reshape (( field_size , field_size , A , 4 )) # FSxFSxAx4 # Many rounding happens inside the anchor code anyway # assert np.all(field_of_anchors == field_of_anchors.astype('int32')) field_of_anchors = field_of_anchors . astype ( \"float32\" ) return field_of_anchors","title":"Module mot.object_detection.modeling.model_rpn"},{"location":"reference/mot/object_detection/modeling/model_rpn/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/modeling/model_rpn/#generate_rpn_proposals","text":"def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ) Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits View Source @under_name_scope () def generate_rpn_proposals ( boxes , scores , img_shape , pre_nms_topk , post_nms_topk = None ) : \"\"\" Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Args: boxes: nx4 float dtype, the proposal boxes. Decoded to floatbox already scores: n float, the logits img_shape: [h, w] pre_nms_topk, post_nms_topk (int): See above. Returns: boxes: kx4 float scores: k logits \"\"\" assert boxes . shape . ndims == 2 , boxes . shape if post_nms_topk is None : post_nms_topk = pre_nms_topk topk = tf . minimum ( pre_nms_topk , tf . size ( scores )) topk_scores , topk_indices = tf . nn . top_k ( scores , k = topk , sorted = False ) topk_boxes = tf . gather ( boxes , topk_indices ) topk_boxes = clip_boxes ( topk_boxes , img_shape ) if cfg . RPN . MIN_SIZE > 0 : topk_boxes_x1y1x2y2 = tf . reshape ( topk_boxes , ( - 1 , 2 , 2 )) topk_boxes_x1y1 , topk_boxes_x2y2 = tf . split ( topk_boxes_x1y1x2y2 , 2 , axis = 1 ) # nx1x2 each wbhb = tf . squeeze ( topk_boxes_x2y2 - topk_boxes_x1y1 , axis = 1 ) valid = tf . reduce_all ( wbhb > cfg . RPN . MIN_SIZE , axis = 1 ) # n , topk_valid_boxes = tf . boolean_mask ( topk_boxes , valid ) topk_valid_scores = tf . boolean_mask ( topk_scores , valid ) else : topk_valid_boxes = topk_boxes topk_valid_scores = topk_scores nms_indices = tf . image . non_max_suppression ( topk_valid_boxes , topk_valid_scores , max_output_size = post_nms_topk , iou_threshold = cfg . RPN . PROPOSAL_NMS_THRESH ) proposal_boxes = tf . gather ( topk_valid_boxes , nms_indices ) proposal_scores = tf . gather ( topk_valid_scores , nms_indices ) tf . sigmoid ( proposal_scores , name = 'probs' ) # for visualization return tf . stop_gradient ( proposal_boxes , name = 'boxes' ), tf . stop_gradient ( proposal_scores , name = 'scores' )","title":"generate_rpn_proposals"},{"location":"reference/mot/object_detection/modeling/model_rpn/#get_all_anchors","text":"def get_all_anchors ( * , stride , sizes , ratios , max_size ) Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. View Source @memoized def get_all_anchors ( * , stride , sizes , ratios , max_size ) : \"\"\" Get all anchors in the largest possible image, shifted, floatbox Args: stride (int): the stride of anchors. sizes (tuple[int]): the sizes (sqrt area) of anchors ratios (tuple[int]): the aspect ratios of anchors max_size (int): maximum size of input image Returns: anchors: SxSxNUM_ANCHORx4, where S == ceil(MAX_SIZE/STRIDE), floatbox The layout in the NUM_ANCHOR dim is NUM_RATIO x NUM_SIZE. \"\"\" # Generates a NAx4 matrix of anchor boxes in ( x1 , y1 , x2 , y2 ) format . Anchors # are centered on 0 , have sqrt areas equal to the specified sizes , and aspect ratios as given . anchors = [] for sz in sizes : for ratio in ratios : w = np . sqrt ( sz * sz / ratio ) h = ratio * w anchors . append ( [ -w, -h, w, h ] ) cell_anchors = np . asarray ( anchors ) * 0.5 field_size = int ( np . ceil ( max_size / stride )) shifts = ( np . arange ( 0 , field_size ) * stride ). astype ( \"float32\" ) shift_x , shift_y = np . meshgrid ( shifts , shifts ) shift_x = shift_x . flatten () shift_y = shift_y . flatten () shifts = np . vstack (( shift_x , shift_y , shift_x , shift_y )). transpose () # Kx4 , K = field_size * field_size K = shifts . shape [ 0 ] A = cell_anchors . shape [ 0 ] field_of_anchors = cell_anchors . reshape (( 1 , A , 4 )) + shifts . reshape (( 1 , K , 4 )). transpose (( 1 , 0 , 2 )) field_of_anchors = field_of_anchors . reshape (( field_size , field_size , A , 4 )) # FSxFSxAx4 # Many rounding happens inside the anchor code anyway # assert np . all ( field_of_anchors == field_of_anchors . astype ( 'int32' )) field_of_anchors = field_of_anchors . astype ( \"float32\" ) return field_of_anchors","title":"get_all_anchors"},{"location":"reference/mot/object_detection/modeling/model_rpn/#rpn_head","text":"def rpn_head ( featuremap , channel , num_anchors ) Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 View Source @ layer_register ( log_shape = True ) @ auto_reuse_variable_scope def rpn_head ( featuremap , channel , num_anchors ): \"\"\" Returns: label_logits: fHxfWxNA box_logits: fHxfWxNAx4 \"\"\" with argscope ( Conv2D , data_format = 'channels_first' , kernel_initializer = tf . random_normal_initializer ( stddev = 0.01 )): hidden = Conv2D ( 'conv0' , featuremap , channel , 3 , activation = tf . nn . relu ) label_logits = Conv2D ( 'class' , hidden , num_anchors , 1 ) box_logits = Conv2D ( 'box' , hidden , 4 * num_anchors , 1 ) # 1, NA(*4), im/16, im/16 (NCHW) label_logits = tf . transpose ( label_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWxNA label_logits = tf . squeeze ( label_logits , 0 ) # fHxfWxNA shp = tf . shape ( box_logits ) # 1x(NAx4)xfHxfW box_logits = tf . transpose ( box_logits , [ 0 , 2 , 3 , 1 ]) # 1xfHxfWx(NAx4) box_logits = tf . reshape ( box_logits , tf . stack ([ shp [ 2 ], shp [ 3 ], num_anchors , 4 ])) # fHxfWxNAx4 return label_logits , box_logits","title":"rpn_head"},{"location":"reference/mot/object_detection/modeling/model_rpn/#rpn_losses","text":"def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ) Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss View Source @under_name_scope () def rpn_losses ( anchor_labels , anchor_boxes , label_logits , box_logits ) : \"\"\" Args: anchor_labels: fHxfWxNA anchor_boxes: fHxfWxNAx4, encoded label_logits: fHxfWxNA box_logits: fHxfWxNAx4 Returns: label_loss, box_loss \"\"\" with tf . device ( '/cpu:0' ) : valid_mask = tf . stop_gradient ( tf . not_equal ( anchor_labels , - 1 )) pos_mask = tf . stop_gradient ( tf . equal ( anchor_labels , 1 )) nr_valid = tf . stop_gradient ( tf . count_nonzero ( valid_mask , dtype = tf . int32 ), name = 'num_valid_anchor' ) nr_pos = tf . identity ( tf . count_nonzero ( pos_mask , dtype = tf . int32 ), name = 'num_pos_anchor' ) # nr_pos is guaranteed > 0 in C4 . But in FPN . even nr_valid could be 0. valid_anchor_labels = tf . boolean_mask ( anchor_labels , valid_mask ) valid_label_logits = tf . boolean_mask ( label_logits , valid_mask ) with tf . name_scope ( 'label_metrics' ) : valid_label_prob = tf . nn . sigmoid ( valid_label_logits ) summaries = [] with tf . device ( '/cpu:0' ) : for th in [ 0.5, 0.2, 0.1 ] : valid_prediction = tf . cast ( valid_label_prob > th , tf . int32 ) nr_pos_prediction = tf . reduce_sum ( valid_prediction , name = 'num_pos_prediction' ) pos_prediction_corr = tf . count_nonzero ( tf . logical_and ( valid_label_prob > th , tf . equal ( valid_prediction , valid_anchor_labels )), dtype = tf . int32 ) placeholder = 0.5 # A small value will make summaries appear lower . recall = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos ), tf . float32 ) recall = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , recall , name = 'recall_th{}' . format ( th )) precision = tf . cast ( tf . truediv ( pos_prediction_corr , nr_pos_prediction ), tf . float32 ) precision = tf . where ( tf . equal ( nr_pos_prediction , 0 ), placeholder , precision , name = 'precision_th{}' . format ( th )) summaries . extend ( [ precision, recall ] ) add_moving_summary ( * summaries ) # Per - level loss summaries in FPN may appear lower due to the use of a small placeholder . # But the total RPN loss will be fine . TODO make the summary op smarter placeholder = 0. label_loss = tf . nn . sigmoid_cross_entropy_with_logits ( labels = tf . cast ( valid_anchor_labels , tf . float32 ), logits = valid_label_logits ) label_loss = tf . reduce_sum ( label_loss ) * ( 1. / cfg . RPN . BATCH_PER_IM ) label_loss = tf . where ( tf . equal ( nr_valid , 0 ), placeholder , label_loss , name = 'label_loss' ) pos_anchor_boxes = tf . boolean_mask ( anchor_boxes , pos_mask ) pos_box_logits = tf . boolean_mask ( box_logits , pos_mask ) delta = 1.0 / 9 box_loss = tf . losses . huber_loss ( pos_anchor_boxes , pos_box_logits , delta = delta , reduction = tf . losses . Reduction . SUM ) / delta box_loss = box_loss * ( 1. / cfg . RPN . BATCH_PER_IM ) box_loss = tf . where ( tf . equal ( nr_pos , 0 ), placeholder , box_loss , name = 'box_loss' ) add_moving_summary ( label_loss , box_loss , nr_valid , nr_pos ) return [ label_loss, box_loss ]","title":"rpn_losses"},{"location":"reference/mot/object_detection/utils/","text":"Module mot.object_detection.utils Sub-modules mot.object_detection.utils.box_ops mot.object_detection.utils.np_box_ops","title":"Index"},{"location":"reference/mot/object_detection/utils/#module-motobject_detectionutils","text":"","title":"Module mot.object_detection.utils"},{"location":"reference/mot/object_detection/utils/#sub-modules","text":"mot.object_detection.utils.box_ops mot.object_detection.utils.np_box_ops","title":"Sub-modules"},{"location":"reference/mot/object_detection/utils/box_ops/","text":"Module mot.object_detection.utils.box_ops View Source # -*- coding: utf-8 -*- # File: box_ops.py import tensorflow as tf from tensorpack.tfutils.scope_utils import under_name_scope \"\"\" This file is modified from https://github.com/tensorflow/models/blob/master/object_detection/core/box_list_ops.py \"\"\" @under_name_scope () def area ( boxes ): \"\"\" Args: boxes: nx4 floatbox Returns: n \"\"\" x_min , y_min , x_max , y_max = tf . split ( boxes , 4 , axis = 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), [ 1 ]) @under_name_scope () def pairwise_intersection ( boxlist1 , boxlist2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections \"\"\" x_min1 , y_min1 , x_max1 , y_max1 = tf . split ( boxlist1 , 4 , axis = 1 ) x_min2 , y_min2 , x_max2 , y_max2 = tf . split ( boxlist2 , 4 , axis = 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 )) intersect_heights = tf . maximum ( 0.0 , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 )) intersect_widths = tf . maximum ( 0.0 , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths @under_name_scope () def pairwise_iou ( boxlist1 , boxlist2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. \"\"\" intersections = pairwise_intersection ( boxlist1 , boxlist2 ) areas1 = area ( boxlist1 ) areas2 = area ( boxlist2 ) unions = ( tf . expand_dims ( areas1 , 1 ) + tf . expand_dims ( areas2 , 0 ) - intersections ) return tf . where ( tf . equal ( intersections , 0.0 ), tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) Functions area def area ( boxes ) Args: boxes: nx4 floatbox Returns: n View Source @under_name_scope () def area ( boxes ) : \"\"\" Args: boxes: nx4 floatbox Returns: n \"\"\" x_min , y_min , x_max , y_max = tf . split ( boxes , 4 , axis = 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), [ 1 ] ) pairwise_intersection def pairwise_intersection ( boxlist1 , boxlist2 ) Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections View Source @under_name_scope () def pairwise_intersection ( boxlist1 , boxlist2 ) : \"\"\"Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections \"\"\" x_min1 , y_min1 , x_max1 , y_max1 = tf . split ( boxlist1 , 4 , axis = 1 ) x_min2 , y_min2 , x_max2 , y_max2 = tf . split ( boxlist2 , 4 , axis = 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 )) intersect_heights = tf . maximum ( 0.0 , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 )) intersect_widths = tf . maximum ( 0.0 , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths pairwise_iou def pairwise_iou ( boxlist1 , boxlist2 ) Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. View Source @under_name_scope () def pairwise_iou ( boxlist1 , boxlist2 ) : \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. \"\"\" intersections = pairwise_intersection ( boxlist1 , boxlist2 ) areas1 = area ( boxlist1 ) areas2 = area ( boxlist2 ) unions = ( tf . expand_dims ( areas1 , 1 ) + tf . expand_dims ( areas2 , 0 ) - intersections ) return tf . where ( tf . equal ( intersections , 0.0 ), tf . zeros_like ( intersections ), tf . truediv ( intersections , unions ))","title":"Box Ops"},{"location":"reference/mot/object_detection/utils/box_ops/#module-motobject_detectionutilsbox_ops","text":"View Source # -*- coding: utf-8 -*- # File: box_ops.py import tensorflow as tf from tensorpack.tfutils.scope_utils import under_name_scope \"\"\" This file is modified from https://github.com/tensorflow/models/blob/master/object_detection/core/box_list_ops.py \"\"\" @under_name_scope () def area ( boxes ): \"\"\" Args: boxes: nx4 floatbox Returns: n \"\"\" x_min , y_min , x_max , y_max = tf . split ( boxes , 4 , axis = 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), [ 1 ]) @under_name_scope () def pairwise_intersection ( boxlist1 , boxlist2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections \"\"\" x_min1 , y_min1 , x_max1 , y_max1 = tf . split ( boxlist1 , 4 , axis = 1 ) x_min2 , y_min2 , x_max2 , y_max2 = tf . split ( boxlist2 , 4 , axis = 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 )) intersect_heights = tf . maximum ( 0.0 , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 )) intersect_widths = tf . maximum ( 0.0 , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths @under_name_scope () def pairwise_iou ( boxlist1 , boxlist2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. \"\"\" intersections = pairwise_intersection ( boxlist1 , boxlist2 ) areas1 = area ( boxlist1 ) areas2 = area ( boxlist2 ) unions = ( tf . expand_dims ( areas1 , 1 ) + tf . expand_dims ( areas2 , 0 ) - intersections ) return tf . where ( tf . equal ( intersections , 0.0 ), tf . zeros_like ( intersections ), tf . truediv ( intersections , unions ))","title":"Module mot.object_detection.utils.box_ops"},{"location":"reference/mot/object_detection/utils/box_ops/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/utils/box_ops/#area","text":"def area ( boxes ) Args: boxes: nx4 floatbox Returns: n View Source @under_name_scope () def area ( boxes ) : \"\"\" Args: boxes: nx4 floatbox Returns: n \"\"\" x_min , y_min , x_max , y_max = tf . split ( boxes , 4 , axis = 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), [ 1 ] )","title":"area"},{"location":"reference/mot/object_detection/utils/box_ops/#pairwise_intersection","text":"def pairwise_intersection ( boxlist1 , boxlist2 ) Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections View Source @under_name_scope () def pairwise_intersection ( boxlist1 , boxlist2 ) : \"\"\"Compute pairwise intersection areas between boxes. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise intersections \"\"\" x_min1 , y_min1 , x_max1 , y_max1 = tf . split ( boxlist1 , 4 , axis = 1 ) x_min2 , y_min2 , x_max2 , y_max2 = tf . split ( boxlist2 , 4 , axis = 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 )) intersect_heights = tf . maximum ( 0.0 , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 )) intersect_widths = tf . maximum ( 0.0 , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths","title":"pairwise_intersection"},{"location":"reference/mot/object_detection/utils/box_ops/#pairwise_iou","text":"def pairwise_iou ( boxlist1 , boxlist2 ) Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. View Source @under_name_scope () def pairwise_iou ( boxlist1 , boxlist2 ) : \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxlist1: Nx4 floatbox boxlist2: Mx4 Returns: a tensor with shape [N, M] representing pairwise iou scores. \"\"\" intersections = pairwise_intersection ( boxlist1 , boxlist2 ) areas1 = area ( boxlist1 ) areas2 = area ( boxlist2 ) unions = ( tf . expand_dims ( areas1 , 1 ) + tf . expand_dims ( areas2 , 0 ) - intersections ) return tf . where ( tf . equal ( intersections , 0.0 ), tf . zeros_like ( intersections ), tf . truediv ( intersections , unions ))","title":"pairwise_iou"},{"location":"reference/mot/object_detection/utils/np_box_ops/","text":"Module mot.object_detection.utils.np_box_ops Operations for [N, 4] numpy arrays representing bounding boxes. Example box operations that are supported: * Areas: compute bounding box areas * IOU: pairwise intersection-over-union scores View Source # Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"Operations for [N, 4] numpy arrays representing bounding boxes. Example box operations that are supported: * Areas: compute bounding box areas * IOU: pairwise intersection-over-union scores \"\"\" import numpy as np def area ( boxes ): \"\"\"Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas \"\"\" return ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ]) def intersection ( boxes1 , boxes2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area \"\"\" [ y_min1 , x_min1 , y_max1 , x_max1 ] = np . split ( boxes1 , 4 , axis = 1 ) [ y_min2 , x_min2 , y_max2 , x_max2 ] = np . split ( boxes2 , 4 , axis = 1 ) all_pairs_min_ymax = np . minimum ( y_max1 , np . transpose ( y_max2 )) all_pairs_max_ymin = np . maximum ( y_min1 , np . transpose ( y_min2 )) intersect_heights = np . maximum ( np . zeros ( all_pairs_max_ymin . shape , dtype = 'f4' ), all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = np . minimum ( x_max1 , np . transpose ( x_max2 )) all_pairs_max_xmin = np . maximum ( x_min1 , np . transpose ( x_min2 )) intersect_widths = np . maximum ( np . zeros ( all_pairs_max_xmin . shape , dtype = 'f4' ), all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def iou ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) area1 = area ( boxes1 ) area2 = area ( boxes2 ) union = np . expand_dims ( area1 , axis = 1 ) + np . expand_dims ( area2 , axis = 0 ) - intersect return intersect / union def ioa ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) inv_areas = np . expand_dims ( 1.0 / area ( boxes2 ), axis = 0 ) return intersect * inv_areas Functions area def area ( boxes ) Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas View Source def area ( boxes ): \"\"\"Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas \"\"\" return ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ]) intersection def intersection ( boxes1 , boxes2 ) Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area View Source def intersection ( boxes1 , boxes2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area \"\"\" [ y_min1 , x_min1 , y_max1 , x_max1 ] = np . split ( boxes1 , 4 , axis = 1 ) [ y_min2 , x_min2 , y_max2 , x_max2 ] = np . split ( boxes2 , 4 , axis = 1 ) all_pairs_min_ymax = np . minimum ( y_max1 , np . transpose ( y_max2 )) all_pairs_max_ymin = np . maximum ( y_min1 , np . transpose ( y_min2 )) intersect_heights = np . maximum ( np . zeros ( all_pairs_max_ymin . shape , dtype = 'f4' ), all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = np . minimum ( x_max1 , np . transpose ( x_max2 )) all_pairs_max_xmin = np . maximum ( x_min1 , np . transpose ( x_min2 )) intersect_widths = np . maximum ( np . zeros ( all_pairs_max_xmin . shape , dtype = 'f4' ), all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths ioa def ioa ( boxes1 , boxes2 ) Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. View Source def ioa ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) inv_areas = np . expand_dims ( 1 . 0 / area ( boxes2 ), axis = 0 ) return intersect * inv_areas iou def iou ( boxes1 , boxes2 ) Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. View Source def iou ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) area1 = area ( boxes1 ) area2 = area ( boxes2 ) union = np . expand_dims ( area1 , axis = 1 ) + np . expand_dims ( area2 , axis = 0 ) - intersect return intersect / union","title":"Np Box Ops"},{"location":"reference/mot/object_detection/utils/np_box_ops/#module-motobject_detectionutilsnp_box_ops","text":"Operations for [N, 4] numpy arrays representing bounding boxes. Example box operations that are supported: * Areas: compute bounding box areas * IOU: pairwise intersection-over-union scores View Source # Copyright 2017 The TensorFlow Authors. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"Operations for [N, 4] numpy arrays representing bounding boxes. Example box operations that are supported: * Areas: compute bounding box areas * IOU: pairwise intersection-over-union scores \"\"\" import numpy as np def area ( boxes ): \"\"\"Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas \"\"\" return ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ]) def intersection ( boxes1 , boxes2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area \"\"\" [ y_min1 , x_min1 , y_max1 , x_max1 ] = np . split ( boxes1 , 4 , axis = 1 ) [ y_min2 , x_min2 , y_max2 , x_max2 ] = np . split ( boxes2 , 4 , axis = 1 ) all_pairs_min_ymax = np . minimum ( y_max1 , np . transpose ( y_max2 )) all_pairs_max_ymin = np . maximum ( y_min1 , np . transpose ( y_min2 )) intersect_heights = np . maximum ( np . zeros ( all_pairs_max_ymin . shape , dtype = 'f4' ), all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = np . minimum ( x_max1 , np . transpose ( x_max2 )) all_pairs_max_xmin = np . maximum ( x_min1 , np . transpose ( x_min2 )) intersect_widths = np . maximum ( np . zeros ( all_pairs_max_xmin . shape , dtype = 'f4' ), all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def iou ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) area1 = area ( boxes1 ) area2 = area ( boxes2 ) union = np . expand_dims ( area1 , axis = 1 ) + np . expand_dims ( area2 , axis = 0 ) - intersect return intersect / union def ioa ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) inv_areas = np . expand_dims ( 1.0 / area ( boxes2 ), axis = 0 ) return intersect * inv_areas","title":"Module mot.object_detection.utils.np_box_ops"},{"location":"reference/mot/object_detection/utils/np_box_ops/#functions","text":"","title":"Functions"},{"location":"reference/mot/object_detection/utils/np_box_ops/#area","text":"def area ( boxes ) Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas View Source def area ( boxes ): \"\"\"Computes area of boxes. Args: boxes: Numpy array with shape [N, 4] holding N boxes Returns: a numpy array with shape [N*1] representing box areas \"\"\" return ( boxes [:, 2 ] - boxes [:, 0 ]) * ( boxes [:, 3 ] - boxes [:, 1 ])","title":"area"},{"location":"reference/mot/object_detection/utils/np_box_ops/#intersection","text":"def intersection ( boxes1 , boxes2 ) Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area View Source def intersection ( boxes1 , boxes2 ): \"\"\"Compute pairwise intersection areas between boxes. Args: boxes1: a numpy array with shape [N, 4] holding N boxes boxes2: a numpy array with shape [M, 4] holding M boxes Returns: a numpy array with shape [N*M] representing pairwise intersection area \"\"\" [ y_min1 , x_min1 , y_max1 , x_max1 ] = np . split ( boxes1 , 4 , axis = 1 ) [ y_min2 , x_min2 , y_max2 , x_max2 ] = np . split ( boxes2 , 4 , axis = 1 ) all_pairs_min_ymax = np . minimum ( y_max1 , np . transpose ( y_max2 )) all_pairs_max_ymin = np . maximum ( y_min1 , np . transpose ( y_min2 )) intersect_heights = np . maximum ( np . zeros ( all_pairs_max_ymin . shape , dtype = 'f4' ), all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = np . minimum ( x_max1 , np . transpose ( x_max2 )) all_pairs_max_xmin = np . maximum ( x_min1 , np . transpose ( x_min2 )) intersect_widths = np . maximum ( np . zeros ( all_pairs_max_xmin . shape , dtype = 'f4' ), all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths","title":"intersection"},{"location":"reference/mot/object_detection/utils/np_box_ops/#ioa","text":"def ioa ( boxes1 , boxes2 ) Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. View Source def ioa ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-area between box collections. Intersection-over-area (ioa) between two boxes box1 and box2 is defined as their intersection area over box2's area. Note that ioa is not symmetric, that is, IOA(box1, box2) != IOA(box2, box1). Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding N boxes. Returns: a numpy array with shape [N, M] representing pairwise ioa scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) inv_areas = np . expand_dims ( 1 . 0 / area ( boxes2 ), axis = 0 ) return intersect * inv_areas","title":"ioa"},{"location":"reference/mot/object_detection/utils/np_box_ops/#iou","text":"def iou ( boxes1 , boxes2 ) Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. View Source def iou ( boxes1 , boxes2 ): \"\"\"Computes pairwise intersection-over-union between box collections. Args: boxes1: a numpy array with shape [N, 4] holding N boxes. boxes2: a numpy array with shape [M, 4] holding M boxes. Returns: a numpy array with shape [N, M] representing pairwise iou scores. \"\"\" intersect = intersection ( boxes1 , boxes2 ) area1 = area ( boxes1 ) area2 = area ( boxes2 ) union = np . expand_dims ( area1 , axis = 1 ) + np . expand_dims ( area2 , axis = 0 ) - intersect return intersect / union","title":"iou"},{"location":"reference/mot/serving/","text":"Module mot.serving Sub-modules mot.serving.app mot.serving.inference","title":"Index"},{"location":"reference/mot/serving/#module-motserving","text":"","title":"Module mot.serving"},{"location":"reference/mot/serving/#sub-modules","text":"mot.serving.app mot.serving.inference","title":"Sub-modules"},{"location":"reference/mot/serving/app/","text":"Module mot.serving.app View Source from flask import Flask , render_template , request from mot.serving.inference import handle_post_request app = Flask ( __name__ ) @app.route ( '/' , methods = [ 'GET' , 'POST' ]) def index (): if request . method == 'POST' : return handle_post_request () return render_template ( \"upload.html\" ) if __name__ == \"__main__\" : app . run ( threaded = True , port = 5000 , debug = False , host = \"0.0.0.0\" ) Variables app Functions index def index ( ) View Source @ app . route ( '/' , methods = [ 'GET' , 'POST' ]) def index (): if request . method == 'POST' : return handle_post_request () return render_template ( \"upload.html\" )","title":"App"},{"location":"reference/mot/serving/app/#module-motservingapp","text":"View Source from flask import Flask , render_template , request from mot.serving.inference import handle_post_request app = Flask ( __name__ ) @app.route ( '/' , methods = [ 'GET' , 'POST' ]) def index (): if request . method == 'POST' : return handle_post_request () return render_template ( \"upload.html\" ) if __name__ == \"__main__\" : app . run ( threaded = True , port = 5000 , debug = False , host = \"0.0.0.0\" )","title":"Module mot.serving.app"},{"location":"reference/mot/serving/app/#variables","text":"app","title":"Variables"},{"location":"reference/mot/serving/app/#functions","text":"","title":"Functions"},{"location":"reference/mot/serving/app/#index","text":"def index ( ) View Source @ app . route ( '/' , methods = [ 'GET' , 'POST' ]) def index (): if request . method == 'POST' : return handle_post_request () return render_template ( \"upload.html\" )","title":"index"},{"location":"reference/mot/serving/inference/","text":"Module mot.serving.inference View Source import json import multiprocessing import os import shutil from typing import Dict , List , Tuple import cv2 import numpy as np from flask import request from tensorpack.utils import logger from tqdm import tqdm from werkzeug import FileStorage from werkzeug.utils import secure_filename from zipfile import ZipFile from mot.object_detection.query_server import \\ localizer_tensorflow_serving_inference from mot.tracker.object_tracking import ObjectTracking from mot.tracker.video_utils import read_folder , split_video SERVING_URL = \"http://localhost:8501\" # the url where the tf-serving container exposes the model UPLOAD_FOLDER = 'tmp' # folder used to store images or videos when sending files FPS = 4 RESOLUTION = ( 1024 , 768 ) CLASS_NAMES = [ \"bottles\" , \"others\" , \"fragments\" ] SUM_THRESHOLD = 0.6 # the sum of scores for all classes must be greater than this value # for the prediction to be kept CLASS_TO_THRESHOLD = { \"bottles\" : 0.4 , \"others\" : 0.3 , \"fragments\" : 0.3 } CPU_COUNT = min ( int ( multiprocessing . cpu_count () / 2 ), 32 ) def handle_post_request ( upload_folder : str = UPLOAD_FOLDER ) -> Dict [ str , np . array ]: \"\"\"This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: ```json {\"image\":[[[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]]]} ``` or ```json {\"video\": TODO} ``` Arguments: - *upload_folder*: Where the files are temporarly stored Returns: - *Dict[str, np.array]*: The predictions of the TF serving module Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if \"file\" in request . files : return handle_file ( request . files [ 'file' ], upload_folder , ** request . form ) data = json . loads ( request . data . decode ( \"utf-8\" )) if \"image\" in data : image = np . array ( data [ \"image\" ]) return { \"detected_trash\" : predict_and_format_image ( image )} if \"video\" in data : raise NotImplementedError ( \"video\" ) raise ValueError ( \"Error during the reading of JSON. Keys {} aren't valid ones.\" . format ( data . keys ()) + \"For an image, send a JSON such as {'image': [0, 0, 0]}.\" + \"Sending videos over JSON isn't implemented yet.\" ) def handle_file ( file : FileStorage , upload_folder : str = UPLOAD_FOLDER , fps : int = FPS , resolution : Tuple [ int , int ] = RESOLUTION , ** kwargs ) -> Dict [ str , np . array ]: \"\"\"Make the prediction if the data is coming from an uploaded file. Arguments: - *file*: The file, can be either an image or a video, or a zipped folder - *upload_folder*: Where the files are temporarly stored Returns: - for an image: a json of format ```json { \"image\": filename, \"detected_trash\": [ { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 }, { \"box\": [10, 10, 25, 20], \"label\": \"bottles\", \"score\": 0.75 } ] } ``` - for a video or a zipped file: a json of format ```json { \"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ { \"label\": \"bottles\", \"id\": 0, \"frame_to_box\": { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\": \"fragments\", \"id\": 1, \"frame_to_box\": { 12: [10, 8, 9, 15] } } ] } ``` Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if kwargs : logger . warning ( \"Unused kwargs: {} \" . format ( kwargs )) filename = secure_filename ( file . filename ) full_filepath = os . path . join ( upload_folder , filename ) if not os . path . isdir ( upload_folder ): os . mkdir ( upload_folder ) if os . path . isfile ( full_filepath ): os . remove ( full_filepath ) file . save ( full_filepath ) file_type = file . mimetype . split ( \"/\" )[ 0 ] # mimetype is for example 'image/png' and we only want the image if file_type == \"image\" : image = cv2 . imread ( full_filepath ) # cv2 opens in BGR os . remove ( full_filepath ) # remove it as we don't need it anymore try : detected_trash = predict_and_format_image ( image ) except ValueError as e : return { \"error\" : str ( e )} return { \"image\" : filename , \"detected_trash\" : detected_trash } elif file_type in [ \"video\" , \"application\" ]: folder = None if file . mimetype == \"application/zip\" : # zip case ZipFile ( full_filepath ) . extractall ( upload_folder ) dirname = None with ZipFile ( full_filepath , 'r' ) as zipObj : listOfFileNames = zipObj . namelist () for fileName in listOfFileNames : dirname = os . path . dirname ( fileName ) zipObj . extract ( fileName , upload_folder ) folder = os . path . join ( upload_folder , dirname ) else : # video case: splitting video and saving frames folder = os . path . join ( upload_folder , \" {} _split\" . format ( filename )) if os . path . isdir ( folder ): shutil . rmtree ( folder ) os . mkdir ( folder ) logger . info ( \"Splitting video {} to {} .\" . format ( full_filepath , folder )) split_video ( full_filepath , folder , fps = fps , resolution = resolution ) print ( \"folder:\" , folder , \"uplaod_folder:\" , upload_folder , \"file.filename:\" , file . filename ) image_paths = read_folder ( folder ) if len ( image_paths ) == 0 : raise ValueError ( \"No output image\" ) # making inference on frames logger . info ( \" {} images to analyze on {} CPUs.\" . format ( len ( image_paths ), CPU_COUNT )) try : with multiprocessing . Pool ( CPU_COUNT ) as p : inference_outputs = list ( tqdm ( p . imap ( process_image , image_paths ), total = len ( image_paths ), ) ) except ValueError as e : return { \"error\" : str ( e )} logger . info ( \"Finish analyzing video {} .\" . format ( full_filepath )) # tracking objects logger . info ( \"Starting tracking.\" ) object_tracker = ObjectTracking ( filename , image_paths , inference_outputs , fps = fps ) tracks = object_tracker . compute_tracks () logger . info ( \"Tracking finished.\" ) return object_tracker . json_result ( tracks ) else : raise NotImplementedError ( file_type ) def process_image ( image_path : str ) -> Dict [ str , object ]: \"\"\"Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: - *image_path* Returns: - *Dict[str, object]*: Predictions for this image path ```python predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" image = cv2 . imread ( image_path ) # cv2 opens in BGR return localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = True ) def predict_and_format_image ( image : np . ndarray , class_names : List [ str ] = CLASS_NAMES , class_to_threshold : Dict [ str , float ] = CLASS_TO_THRESHOLD ) -> List [ Dict [ str , object ]]: \"\"\"Make prediction on an image and return them in a human readable format. Arguments: - *image*: An numpy array in BGR - *class_names*: The list of class names without background - *class_to_threshold*: A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: - *List[Dict[str, object]]*: List of dicts such as: ```python3 { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 } ``` \"\"\" class_names = [ \"BG\" ] + class_names outputs = localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = False ) detected_trash = [] for box , label , score in zip ( outputs [ \"output/boxes:0\" ], outputs [ \"output/labels:0\" ], outputs [ \"output/scores:0\" ] ): if keep_prediction ( class_names , label , class_to_threshold , score ): trash_json = { \"box\" : [ round ( coord , 2 ) for coord in box ], \"label\" : class_names [ label ], \"score\" : score , } detected_trash . append ( trash_json ) return detected_trash def keep_prediction ( class_names , label , class_to_threshold , score ): if isinstance ( score , list ): # we have scores for all classes if np . array ( score ) . sum () < SUM_THRESHOLD : return False return True return class_names [ label ] not in class_to_threshold or score >= class_to_threshold [ class_names [ label ]] Variables CLASS_NAMES CLASS_TO_THRESHOLD CPU_COUNT FPS RESOLUTION SERVING_URL SUM_THRESHOLD UPLOAD_FOLDER Functions handle_file def handle_file ( file : werkzeug . datastructures . FileStorage , upload_folder : str = 'tmp' , fps : int = 4 , resolution : Tuple [ int , int ] = ( 1024 , 768 ), ** kwargs ) -> Dict [ str , < built - in function array > ] Make the prediction if the data is coming from an uploaded file. Arguments: file : The file, can be either an image or a video, or a zipped folder upload_folder : Where the files are temporarly stored Returns: for an image: a json of format { \"image\" : filename , \"detected_trash\" : [ { \"box\" : [ 1 , 1 , 2 , 20 ], \"label\" : \"fragments\" , \"score\" : 0.92 }, { \"box\" : [ 10 , 10 , 25 , 20 ], \"label\" : \"bottles\" , \"score\" : 0.75 } ] } for a video or a zipped file: a json of format { \"video_length\" : 132 , \"fps\" : 2 , \"video_id\" : \"GOPRO1234.mp4\" , \"detected_trash\" : [ { \"label\" : \"bottles\" , \"id\" : 0 , \"frame_to_box\" : { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\" : \"fragments\" , \"id\" : 1 , \"frame_to_box\" : { 12: [10, 8, 9, 15] } } ] } Raises: NotImplementedError : If the format of data isn't handled yet View Source def handle_file ( file : FileStorage , upload_folder : str = UPLOAD_FOLDER , fps : int = FPS , resolution : Tuple [ int , int ] = RESOLUTION , ** kwargs ) -> Dict [ str , np . array ]: \"\"\"Make the prediction if the data is coming from an uploaded file. Arguments: - *file*: The file, can be either an image or a video, or a zipped folder - *upload_folder*: Where the files are temporarly stored Returns: - for an image: a json of format ```json { \"image\": filename, \"detected_trash\": [ { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 }, { \"box\": [10, 10, 25, 20], \"label\": \"bottles\", \"score\": 0.75 } ] } ``` - for a video or a zipped file: a json of format ```json { \"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ { \"label\": \"bottles\", \"id\": 0, \"frame_to_box\": { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\": \"fragments\", \"id\": 1, \"frame_to_box\": { 12: [10, 8, 9, 15] } } ] } ``` Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if kwargs : logger . warning ( \"Unused kwargs: {}\" . format ( kwargs )) filename = secure_filename ( file . filename ) full_filepath = os . path . join ( upload_folder , filename ) if not os . path . isdir ( upload_folder ): os . mkdir ( upload_folder ) if os . path . isfile ( full_filepath ): os . remove ( full_filepath ) file . save ( full_filepath ) file_type = file . mimetype . split ( \"/\" )[ 0 ] # mimetype is for example 'image/png' and we only want the image if file_type == \"image\" : image = cv2 . imread ( full_filepath ) # cv2 opens in BGR os . remove ( full_filepath ) # remove it as we don't need it anymore try : detected_trash = predict_and_format_image ( image ) except ValueError as e : return { \"error\" : str ( e )} return { \"image\" : filename , \"detected_trash\" : detected_trash } elif file_type in [ \"video\" , \"application\" ]: folder = None if file . mimetype == \"application/zip\" : # zip case ZipFile ( full_filepath ) . extractall ( upload_folder ) dirname = None with ZipFile ( full_filepath , 'r' ) as zipObj : listOfFileNames = zipObj . namelist () for fileName in listOfFileNames : dirname = os . path . dirname ( fileName ) zipObj . extract ( fileName , upload_folder ) folder = os . path . join ( upload_folder , dirname ) else : # video case: splitting video and saving frames folder = os . path . join ( upload_folder , \"{}_split\" . format ( filename )) if os . path . isdir ( folder ): shutil . rmtree ( folder ) os . mkdir ( folder ) logger . info ( \"Splitting video {} to {}.\" . format ( full_filepath , folder )) split_video ( full_filepath , folder , fps = fps , resolution = resolution ) print ( \"folder:\" , folder , \"uplaod_folder:\" , upload_folder , \"file.filename:\" , file . filename ) image_paths = read_folder ( folder ) if len ( image_paths ) == 0 : raise ValueError ( \"No output image\" ) # making inference on frames logger . info ( \"{} images to analyze on {} CPUs.\" . format ( len ( image_paths ), CPU_COUNT )) try : with multiprocessing . Pool ( CPU_COUNT ) as p : inference_outputs = list ( tqdm ( p . imap ( process_image , image_paths ), total = len ( image_paths ), ) ) except ValueError as e : return { \"error\" : str ( e )} logger . info ( \"Finish analyzing video {}.\" . format ( full_filepath )) # tracking objects logger . info ( \"Starting tracking.\" ) object_tracker = ObjectTracking ( filename , image_paths , inference_outputs , fps = fps ) tracks = object_tracker . compute_tracks () logger . info ( \"Tracking finished.\" ) return object_tracker . json_result ( tracks ) else : raise NotImplementedError ( file_type ) handle_post_request def handle_post_request ( upload_folder : str = 'tmp' ) -> Dict [ str , < built - in function array > ] This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: { \"image\" :[[[ 0 , 0 , 0 ],[ 0 , 0 , 0 ]],[[ 0 , 0 , 0 ],[ 0 , 0 , 0 ]]]} or { \"video\" : TODO } Arguments: upload_folder : Where the files are temporarly stored Returns: Dict[str, np.array] : The predictions of the TF serving module Raises: NotImplementedError : If the format of data isn't handled yet View Source def handle_post_request ( upload_folder : str = UPLOAD_FOLDER ) -> Dict [ str , np . array ]: \"\"\"This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: ```json {\"image\":[[[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]]]} ``` or ```json {\"video\": TODO} ``` Arguments: - *upload_folder*: Where the files are temporarly stored Returns: - *Dict[str, np.array]*: The predictions of the TF serving module Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if \"file\" in request . files : return handle_file ( request . files [ 'file' ], upload_folder , ** request . form ) data = json . loads ( request . data . decode ( \"utf-8\" )) if \"image\" in data : image = np . array ( data [ \"image\" ]) return { \"detected_trash\" : predict_and_format_image ( image )} if \"video\" in data : raise NotImplementedError ( \"video\" ) raise ValueError ( \"Error during the reading of JSON. Keys {} aren't valid ones.\" . format ( data . keys ()) + \"For an image, send a JSON such as {'image': [0, 0, 0]}.\" + \"Sending videos over JSON isn't implemented yet.\" ) keep_prediction def keep_prediction ( class_names , label , class_to_threshold , score ) View Source def keep_prediction ( class_names , label , class_to_threshold , score ) : if isinstance ( score , list ) : # we have scores for all classes if np . array ( score ). sum () < SUM_THRESHOLD : return False return True return class_names [ label ] not in class_to_threshold or score >= class_to_threshold [ class_names[label ] ] predict_and_format_image def predict_and_format_image ( image : numpy . ndarray , class_names : List [ str ] = [ 'bottles' , 'others' , 'fragments' ], class_to_threshold : Dict [ str , float ] = { 'bottles' : 0.4 , 'others' : 0.3 , 'fragments' : 0.3 } ) -> List [ Dict [ str , object ]] Make prediction on an image and return them in a human readable format. Arguments: image : An numpy array in BGR class_names : The list of class names without background class_to_threshold : A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: List[Dict[str, object]] : List of dicts such as: { \"box\" : [ 1 , 1 , 2 , 20 ], \"label\" : \"fragments\" , \"score\" : 0.92 } View Source def predict_and_format_image ( image : np . ndarray , class_names : List [ str ] = CLASS_NAMES , class_to_threshold : Dict [ str, float ] = CLASS_TO_THRESHOLD ) -> List [ Dict[str, object ] ]: \"\"\"Make prediction on an image and return them in a human readable format. Arguments: - *image*: An numpy array in BGR - *class_names*: The list of class names without background - *class_to_threshold*: A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: - *List[Dict[str, object]]*: List of dicts such as: ```python3 { \" box \": [1, 1, 2, 20], \" label \": \" fragments \", \" score \": 0.92 } ``` \"\"\" class_names = [ \"BG\" ] + class_names outputs = localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = False ) detected_trash = [] for box , label , score in zip ( outputs [ \"output/boxes:0\" ] , outputs [ \"output/labels:0\" ] , outputs [ \"output/scores:0\" ] ) : if keep_prediction ( class_names , label , class_to_threshold , score ) : trash_json = { \"box\" : [ round(coord, 2) for coord in box ] , \"label\" : class_names [ label ] , \"score\" : score , } detected_trash . append ( trash_json ) return detected_trash process_image def process_image ( image_path : str ) -> Dict [ str , object ] Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: image_path Returns: Dict[str, object] : Predictions for this image path predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ 0.98 , 0.87 , 0.76 ] # sorted in descending order } View Source def process_image ( image_path : str ) -> Dict [ str , object ]: \"\"\"Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: - *image_path* Returns: - *Dict[str, object]*: Predictions for this image path ```python predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" image = cv2 . imread ( image_path ) # cv2 opens in BGR return localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = True )","title":"Inference"},{"location":"reference/mot/serving/inference/#module-motservinginference","text":"View Source import json import multiprocessing import os import shutil from typing import Dict , List , Tuple import cv2 import numpy as np from flask import request from tensorpack.utils import logger from tqdm import tqdm from werkzeug import FileStorage from werkzeug.utils import secure_filename from zipfile import ZipFile from mot.object_detection.query_server import \\ localizer_tensorflow_serving_inference from mot.tracker.object_tracking import ObjectTracking from mot.tracker.video_utils import read_folder , split_video SERVING_URL = \"http://localhost:8501\" # the url where the tf-serving container exposes the model UPLOAD_FOLDER = 'tmp' # folder used to store images or videos when sending files FPS = 4 RESOLUTION = ( 1024 , 768 ) CLASS_NAMES = [ \"bottles\" , \"others\" , \"fragments\" ] SUM_THRESHOLD = 0.6 # the sum of scores for all classes must be greater than this value # for the prediction to be kept CLASS_TO_THRESHOLD = { \"bottles\" : 0.4 , \"others\" : 0.3 , \"fragments\" : 0.3 } CPU_COUNT = min ( int ( multiprocessing . cpu_count () / 2 ), 32 ) def handle_post_request ( upload_folder : str = UPLOAD_FOLDER ) -> Dict [ str , np . array ]: \"\"\"This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: ```json {\"image\":[[[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]]]} ``` or ```json {\"video\": TODO} ``` Arguments: - *upload_folder*: Where the files are temporarly stored Returns: - *Dict[str, np.array]*: The predictions of the TF serving module Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if \"file\" in request . files : return handle_file ( request . files [ 'file' ], upload_folder , ** request . form ) data = json . loads ( request . data . decode ( \"utf-8\" )) if \"image\" in data : image = np . array ( data [ \"image\" ]) return { \"detected_trash\" : predict_and_format_image ( image )} if \"video\" in data : raise NotImplementedError ( \"video\" ) raise ValueError ( \"Error during the reading of JSON. Keys {} aren't valid ones.\" . format ( data . keys ()) + \"For an image, send a JSON such as {'image': [0, 0, 0]}.\" + \"Sending videos over JSON isn't implemented yet.\" ) def handle_file ( file : FileStorage , upload_folder : str = UPLOAD_FOLDER , fps : int = FPS , resolution : Tuple [ int , int ] = RESOLUTION , ** kwargs ) -> Dict [ str , np . array ]: \"\"\"Make the prediction if the data is coming from an uploaded file. Arguments: - *file*: The file, can be either an image or a video, or a zipped folder - *upload_folder*: Where the files are temporarly stored Returns: - for an image: a json of format ```json { \"image\": filename, \"detected_trash\": [ { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 }, { \"box\": [10, 10, 25, 20], \"label\": \"bottles\", \"score\": 0.75 } ] } ``` - for a video or a zipped file: a json of format ```json { \"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ { \"label\": \"bottles\", \"id\": 0, \"frame_to_box\": { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\": \"fragments\", \"id\": 1, \"frame_to_box\": { 12: [10, 8, 9, 15] } } ] } ``` Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if kwargs : logger . warning ( \"Unused kwargs: {} \" . format ( kwargs )) filename = secure_filename ( file . filename ) full_filepath = os . path . join ( upload_folder , filename ) if not os . path . isdir ( upload_folder ): os . mkdir ( upload_folder ) if os . path . isfile ( full_filepath ): os . remove ( full_filepath ) file . save ( full_filepath ) file_type = file . mimetype . split ( \"/\" )[ 0 ] # mimetype is for example 'image/png' and we only want the image if file_type == \"image\" : image = cv2 . imread ( full_filepath ) # cv2 opens in BGR os . remove ( full_filepath ) # remove it as we don't need it anymore try : detected_trash = predict_and_format_image ( image ) except ValueError as e : return { \"error\" : str ( e )} return { \"image\" : filename , \"detected_trash\" : detected_trash } elif file_type in [ \"video\" , \"application\" ]: folder = None if file . mimetype == \"application/zip\" : # zip case ZipFile ( full_filepath ) . extractall ( upload_folder ) dirname = None with ZipFile ( full_filepath , 'r' ) as zipObj : listOfFileNames = zipObj . namelist () for fileName in listOfFileNames : dirname = os . path . dirname ( fileName ) zipObj . extract ( fileName , upload_folder ) folder = os . path . join ( upload_folder , dirname ) else : # video case: splitting video and saving frames folder = os . path . join ( upload_folder , \" {} _split\" . format ( filename )) if os . path . isdir ( folder ): shutil . rmtree ( folder ) os . mkdir ( folder ) logger . info ( \"Splitting video {} to {} .\" . format ( full_filepath , folder )) split_video ( full_filepath , folder , fps = fps , resolution = resolution ) print ( \"folder:\" , folder , \"uplaod_folder:\" , upload_folder , \"file.filename:\" , file . filename ) image_paths = read_folder ( folder ) if len ( image_paths ) == 0 : raise ValueError ( \"No output image\" ) # making inference on frames logger . info ( \" {} images to analyze on {} CPUs.\" . format ( len ( image_paths ), CPU_COUNT )) try : with multiprocessing . Pool ( CPU_COUNT ) as p : inference_outputs = list ( tqdm ( p . imap ( process_image , image_paths ), total = len ( image_paths ), ) ) except ValueError as e : return { \"error\" : str ( e )} logger . info ( \"Finish analyzing video {} .\" . format ( full_filepath )) # tracking objects logger . info ( \"Starting tracking.\" ) object_tracker = ObjectTracking ( filename , image_paths , inference_outputs , fps = fps ) tracks = object_tracker . compute_tracks () logger . info ( \"Tracking finished.\" ) return object_tracker . json_result ( tracks ) else : raise NotImplementedError ( file_type ) def process_image ( image_path : str ) -> Dict [ str , object ]: \"\"\"Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: - *image_path* Returns: - *Dict[str, object]*: Predictions for this image path ```python predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" image = cv2 . imread ( image_path ) # cv2 opens in BGR return localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = True ) def predict_and_format_image ( image : np . ndarray , class_names : List [ str ] = CLASS_NAMES , class_to_threshold : Dict [ str , float ] = CLASS_TO_THRESHOLD ) -> List [ Dict [ str , object ]]: \"\"\"Make prediction on an image and return them in a human readable format. Arguments: - *image*: An numpy array in BGR - *class_names*: The list of class names without background - *class_to_threshold*: A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: - *List[Dict[str, object]]*: List of dicts such as: ```python3 { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 } ``` \"\"\" class_names = [ \"BG\" ] + class_names outputs = localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = False ) detected_trash = [] for box , label , score in zip ( outputs [ \"output/boxes:0\" ], outputs [ \"output/labels:0\" ], outputs [ \"output/scores:0\" ] ): if keep_prediction ( class_names , label , class_to_threshold , score ): trash_json = { \"box\" : [ round ( coord , 2 ) for coord in box ], \"label\" : class_names [ label ], \"score\" : score , } detected_trash . append ( trash_json ) return detected_trash def keep_prediction ( class_names , label , class_to_threshold , score ): if isinstance ( score , list ): # we have scores for all classes if np . array ( score ) . sum () < SUM_THRESHOLD : return False return True return class_names [ label ] not in class_to_threshold or score >= class_to_threshold [ class_names [ label ]]","title":"Module mot.serving.inference"},{"location":"reference/mot/serving/inference/#variables","text":"CLASS_NAMES CLASS_TO_THRESHOLD CPU_COUNT FPS RESOLUTION SERVING_URL SUM_THRESHOLD UPLOAD_FOLDER","title":"Variables"},{"location":"reference/mot/serving/inference/#functions","text":"","title":"Functions"},{"location":"reference/mot/serving/inference/#handle_file","text":"def handle_file ( file : werkzeug . datastructures . FileStorage , upload_folder : str = 'tmp' , fps : int = 4 , resolution : Tuple [ int , int ] = ( 1024 , 768 ), ** kwargs ) -> Dict [ str , < built - in function array > ] Make the prediction if the data is coming from an uploaded file. Arguments: file : The file, can be either an image or a video, or a zipped folder upload_folder : Where the files are temporarly stored Returns: for an image: a json of format { \"image\" : filename , \"detected_trash\" : [ { \"box\" : [ 1 , 1 , 2 , 20 ], \"label\" : \"fragments\" , \"score\" : 0.92 }, { \"box\" : [ 10 , 10 , 25 , 20 ], \"label\" : \"bottles\" , \"score\" : 0.75 } ] } for a video or a zipped file: a json of format { \"video_length\" : 132 , \"fps\" : 2 , \"video_id\" : \"GOPRO1234.mp4\" , \"detected_trash\" : [ { \"label\" : \"bottles\" , \"id\" : 0 , \"frame_to_box\" : { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\" : \"fragments\" , \"id\" : 1 , \"frame_to_box\" : { 12: [10, 8, 9, 15] } } ] } Raises: NotImplementedError : If the format of data isn't handled yet View Source def handle_file ( file : FileStorage , upload_folder : str = UPLOAD_FOLDER , fps : int = FPS , resolution : Tuple [ int , int ] = RESOLUTION , ** kwargs ) -> Dict [ str , np . array ]: \"\"\"Make the prediction if the data is coming from an uploaded file. Arguments: - *file*: The file, can be either an image or a video, or a zipped folder - *upload_folder*: Where the files are temporarly stored Returns: - for an image: a json of format ```json { \"image\": filename, \"detected_trash\": [ { \"box\": [1, 1, 2, 20], \"label\": \"fragments\", \"score\": 0.92 }, { \"box\": [10, 10, 25, 20], \"label\": \"bottles\", \"score\": 0.75 } ] } ``` - for a video or a zipped file: a json of format ```json { \"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ { \"label\": \"bottles\", \"id\": 0, \"frame_to_box\": { 23: [0, 0, 1, 10], 24: [1, 1, 4, 13] } }, { \"label\": \"fragments\", \"id\": 1, \"frame_to_box\": { 12: [10, 8, 9, 15] } } ] } ``` Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if kwargs : logger . warning ( \"Unused kwargs: {}\" . format ( kwargs )) filename = secure_filename ( file . filename ) full_filepath = os . path . join ( upload_folder , filename ) if not os . path . isdir ( upload_folder ): os . mkdir ( upload_folder ) if os . path . isfile ( full_filepath ): os . remove ( full_filepath ) file . save ( full_filepath ) file_type = file . mimetype . split ( \"/\" )[ 0 ] # mimetype is for example 'image/png' and we only want the image if file_type == \"image\" : image = cv2 . imread ( full_filepath ) # cv2 opens in BGR os . remove ( full_filepath ) # remove it as we don't need it anymore try : detected_trash = predict_and_format_image ( image ) except ValueError as e : return { \"error\" : str ( e )} return { \"image\" : filename , \"detected_trash\" : detected_trash } elif file_type in [ \"video\" , \"application\" ]: folder = None if file . mimetype == \"application/zip\" : # zip case ZipFile ( full_filepath ) . extractall ( upload_folder ) dirname = None with ZipFile ( full_filepath , 'r' ) as zipObj : listOfFileNames = zipObj . namelist () for fileName in listOfFileNames : dirname = os . path . dirname ( fileName ) zipObj . extract ( fileName , upload_folder ) folder = os . path . join ( upload_folder , dirname ) else : # video case: splitting video and saving frames folder = os . path . join ( upload_folder , \"{}_split\" . format ( filename )) if os . path . isdir ( folder ): shutil . rmtree ( folder ) os . mkdir ( folder ) logger . info ( \"Splitting video {} to {}.\" . format ( full_filepath , folder )) split_video ( full_filepath , folder , fps = fps , resolution = resolution ) print ( \"folder:\" , folder , \"uplaod_folder:\" , upload_folder , \"file.filename:\" , file . filename ) image_paths = read_folder ( folder ) if len ( image_paths ) == 0 : raise ValueError ( \"No output image\" ) # making inference on frames logger . info ( \"{} images to analyze on {} CPUs.\" . format ( len ( image_paths ), CPU_COUNT )) try : with multiprocessing . Pool ( CPU_COUNT ) as p : inference_outputs = list ( tqdm ( p . imap ( process_image , image_paths ), total = len ( image_paths ), ) ) except ValueError as e : return { \"error\" : str ( e )} logger . info ( \"Finish analyzing video {}.\" . format ( full_filepath )) # tracking objects logger . info ( \"Starting tracking.\" ) object_tracker = ObjectTracking ( filename , image_paths , inference_outputs , fps = fps ) tracks = object_tracker . compute_tracks () logger . info ( \"Tracking finished.\" ) return object_tracker . json_result ( tracks ) else : raise NotImplementedError ( file_type )","title":"handle_file"},{"location":"reference/mot/serving/inference/#handle_post_request","text":"def handle_post_request ( upload_folder : str = 'tmp' ) -> Dict [ str , < built - in function array > ] This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: { \"image\" :[[[ 0 , 0 , 0 ],[ 0 , 0 , 0 ]],[[ 0 , 0 , 0 ],[ 0 , 0 , 0 ]]]} or { \"video\" : TODO } Arguments: upload_folder : Where the files are temporarly stored Returns: Dict[str, np.array] : The predictions of the TF serving module Raises: NotImplementedError : If the format of data isn't handled yet View Source def handle_post_request ( upload_folder : str = UPLOAD_FOLDER ) -> Dict [ str , np . array ]: \"\"\"This method is the first one to be called when a POST request is coming. It analyzes the incoming format (file or JSON) and then call the appropiate methods to do the prediction. If you want to make a prediction by sending the data as a JSON, it has to be in this format: ```json {\"image\":[[[0,0,0],[0,0,0]],[[0,0,0],[0,0,0]]]} ``` or ```json {\"video\": TODO} ``` Arguments: - *upload_folder*: Where the files are temporarly stored Returns: - *Dict[str, np.array]*: The predictions of the TF serving module Raises: - *NotImplementedError*: If the format of data isn't handled yet \"\"\" if \"file\" in request . files : return handle_file ( request . files [ 'file' ], upload_folder , ** request . form ) data = json . loads ( request . data . decode ( \"utf-8\" )) if \"image\" in data : image = np . array ( data [ \"image\" ]) return { \"detected_trash\" : predict_and_format_image ( image )} if \"video\" in data : raise NotImplementedError ( \"video\" ) raise ValueError ( \"Error during the reading of JSON. Keys {} aren't valid ones.\" . format ( data . keys ()) + \"For an image, send a JSON such as {'image': [0, 0, 0]}.\" + \"Sending videos over JSON isn't implemented yet.\" )","title":"handle_post_request"},{"location":"reference/mot/serving/inference/#keep_prediction","text":"def keep_prediction ( class_names , label , class_to_threshold , score ) View Source def keep_prediction ( class_names , label , class_to_threshold , score ) : if isinstance ( score , list ) : # we have scores for all classes if np . array ( score ). sum () < SUM_THRESHOLD : return False return True return class_names [ label ] not in class_to_threshold or score >= class_to_threshold [ class_names[label ] ]","title":"keep_prediction"},{"location":"reference/mot/serving/inference/#predict_and_format_image","text":"def predict_and_format_image ( image : numpy . ndarray , class_names : List [ str ] = [ 'bottles' , 'others' , 'fragments' ], class_to_threshold : Dict [ str , float ] = { 'bottles' : 0.4 , 'others' : 0.3 , 'fragments' : 0.3 } ) -> List [ Dict [ str , object ]] Make prediction on an image and return them in a human readable format. Arguments: image : An numpy array in BGR class_names : The list of class names without background class_to_threshold : A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: List[Dict[str, object]] : List of dicts such as: { \"box\" : [ 1 , 1 , 2 , 20 ], \"label\" : \"fragments\" , \"score\" : 0.92 } View Source def predict_and_format_image ( image : np . ndarray , class_names : List [ str ] = CLASS_NAMES , class_to_threshold : Dict [ str, float ] = CLASS_TO_THRESHOLD ) -> List [ Dict[str, object ] ]: \"\"\"Make prediction on an image and return them in a human readable format. Arguments: - *image*: An numpy array in BGR - *class_names*: The list of class names without background - *class_to_threshold*: A dict assigning class names to threshold. If a class name isn't in this dict, no threshold will be applied, which means that all predictions for this class will be kept. Returns: - *List[Dict[str, object]]*: List of dicts such as: ```python3 { \" box \": [1, 1, 2, 20], \" label \": \" fragments \", \" score \": 0.92 } ``` \"\"\" class_names = [ \"BG\" ] + class_names outputs = localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = False ) detected_trash = [] for box , label , score in zip ( outputs [ \"output/boxes:0\" ] , outputs [ \"output/labels:0\" ] , outputs [ \"output/scores:0\" ] ) : if keep_prediction ( class_names , label , class_to_threshold , score ) : trash_json = { \"box\" : [ round(coord, 2) for coord in box ] , \"label\" : class_names [ label ] , \"score\" : score , } detected_trash . append ( trash_json ) return detected_trash","title":"predict_and_format_image"},{"location":"reference/mot/serving/inference/#process_image","text":"def process_image ( image_path : str ) -> Dict [ str , object ] Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: image_path Returns: Dict[str, object] : Predictions for this image path predictions = { 'output/boxes:0' : [[ 0 , 0 , 1 , 1 ], [ 0 , 0 , 10 , 10 ], [ 10 , 10 , 15 , 100 ]], 'output/labels:0' : [ 3 , 1 , 2 ], # the labels start at 1 since 0 is for background 'output/scores:0' : [ 0.98 , 0.87 , 0.76 ] # sorted in descending order } View Source def process_image ( image_path : str ) -> Dict [ str , object ]: \"\"\"Function used to open and predict on an image. It is suposed to be used in multiprocessing. Arguments: - *image_path* Returns: - *Dict[str, object]*: Predictions for this image path ```python predictions = { 'output/boxes:0': [[0, 0, 1, 1], [0, 0, 10, 10], [10, 10, 15, 100]], 'output/labels:0': [3, 1, 2], # the labels start at 1 since 0 is for background 'output/scores:0': [0.98, 0.87, 0.76] # sorted in descending order } ``` \"\"\" image = cv2 . imread ( image_path ) # cv2 opens in BGR return localizer_tensorflow_serving_inference ( image , SERVING_URL , return_all_scores = True )","title":"process_image"},{"location":"reference/mot/tracker/","text":"Module mot.tracker Sub-modules mot.tracker.camera_flow mot.tracker.object_tracking mot.tracker.tracker_metrics mot.tracker.tracker_utils mot.tracker.video_utils","title":"Index"},{"location":"reference/mot/tracker/#module-mottracker","text":"","title":"Module mot.tracker"},{"location":"reference/mot/tracker/#sub-modules","text":"mot.tracker.camera_flow mot.tracker.object_tracking mot.tracker.tracker_metrics mot.tracker.tracker_utils mot.tracker.video_utils","title":"Sub-modules"},{"location":"reference/mot/tracker/camera_flow/","text":"Module mot.tracker.camera_flow View Source import cv2 import numpy as np from numpy.linalg import inv class CameraFlow (): '''Compute camera flow through optical flow between consecutive images currently not used in the simplest version of the tracking ''' def __init__ ( self ): self . margin_w = 200 self . margin_h = 20 self . img_shape = ( 768 , 1024 ) self . features_mask = np . zeros ( self . img_shape , dtype = \"uint8\" ) self . features_mask [ self . margin_h : - self . margin_h , self . margin_w : - self . margin_w ] = 1 def compute_transform_matrix ( self , im_prev , im_next ): ''' Computes the transformation between two images Arguments: - im_prev: np array of shape self.img_shape and no color channels - im_next: np array of shape self.img_shape and no color channels Returns: - The affine transformation matrix, np array of shape (2,3) ''' prev_pts = cv2 . goodFeaturesToTrack ( im_prev , maxCorners = 200 , qualityLevel = 0.01 , minDistance = 30 , blockSize = 3 , mask = self . features_mask ) next_pts , status , err = cv2 . calcOpticalFlowPyrLK ( im_prev , im_next , prev_pts , None ) idx = np . where ( status == 1 )[ 0 ] prev_pts = prev_pts [ idx ] next_pts = next_pts [ idx ] m = cv2 . estimateAffine2D ( prev_pts , next_pts ) return m [ 0 ] def warp_image ( self , im , matrix ): return cv2 . warpAffine ( im , matrix , self . img_shape , flags = cv2 . INTER_LINEAR + cv2 . WARP_INVERSE_MAP ) def warp_coords ( self , coords , matrix ): '''Transforms the coords of points through the affine matrix Arguments: - coords: coordinates of points, np array of shape (X, 2) - matrix: transformation matrix of shape (2, 3) ''' matrix = cv2 . invertAffineTransform ( matrix ) linear = matrix [:, 0 : 2 ] transl = matrix [:, 2 ] return np . dot ( coords , linear ) + transl def compute_transform_matrices ( self , images_stack ): im_next = images_stack [ 0 ] matrices = [] for i in range ( len ( images_stack ) - 1 ): im_prev = im_next im_next = images_stack [ i + 1 ] matrices . append ( self . compute_transform_matrix ( im_prev , im_next )) return matrices Classes CameraFlow class CameraFlow ( ) Compute camera flow through optical flow between consecutive images currently not used in the simplest version of the tracking Methods compute_transform_matrices def compute_transform_matrices ( self , images_stack ) View Source def compute_transform_matrices ( self , images_stack ): im_next = images_stack [ 0 ] matrices = [] for i in range ( len ( images_stack ) - 1 ): im_prev = im_next im_next = images_stack [ i + 1 ] matrices . append ( self . compute_transform_matrix ( im_prev , im_next )) return matrices compute_transform_matrix def compute_transform_matrix ( self , im_prev , im_next ) Computes the transformation between two images Arguments: im_prev: np array of shape self.img_shape and no color channels im_next: np array of shape self.img_shape and no color channels Returns: The affine transformation matrix, np array of shape (2,3) View Source def compute_transform_matrix ( self , im_prev , im_next ) : ''' Computes the transformation between two images Arguments: - im_prev: np array of shape self.img_shape and no color channels - im_next: np array of shape self.img_shape and no color channels Returns: - The affine transformation matrix, np array of shape (2,3) ''' prev_pts = cv2 . goodFeaturesToTrack ( im_prev , maxCorners = 200 , qualityLevel = 0.01 , minDistance = 30 , blockSize = 3 , mask = self . features_mask ) next_pts , status , err = cv2 . calcOpticalFlowPyrLK ( im_prev , im_next , prev_pts , None ) idx = np . where ( status == 1 ) [ 0 ] prev_pts = prev_pts [ idx ] next_pts = next_pts [ idx ] m = cv2 . estimateAffine2D ( prev_pts , next_pts ) return m [ 0 ] warp_coords def warp_coords ( self , coords , matrix ) Transforms the coords of points through the affine matrix Arguments: coords: coordinates of points, np array of shape (X, 2) matrix: transformation matrix of shape (2, 3) View Source def warp_coords ( self , coords , matrix ): '''Transforms the coords of points through the affine matrix Arguments: - coords: coordinates of points, np array of shape (X, 2) - matrix: transformation matrix of shape (2, 3) ''' matrix = cv2 . invertAffineTransform ( matrix ) linear = matrix [:, 0 : 2 ] transl = matrix [:, 2 ] return np . dot ( coords , linear ) + transl warp_image def warp_image ( self , im , matrix ) View Source def warp_image ( self , im , matrix ): return cv2 . warpAffine ( im , matrix , self . img_shape , flags = cv2 . INTER_LINEAR + cv2 . WARP_INVERSE_MAP )","title":"Camera Flow"},{"location":"reference/mot/tracker/camera_flow/#module-mottrackercamera_flow","text":"View Source import cv2 import numpy as np from numpy.linalg import inv class CameraFlow (): '''Compute camera flow through optical flow between consecutive images currently not used in the simplest version of the tracking ''' def __init__ ( self ): self . margin_w = 200 self . margin_h = 20 self . img_shape = ( 768 , 1024 ) self . features_mask = np . zeros ( self . img_shape , dtype = \"uint8\" ) self . features_mask [ self . margin_h : - self . margin_h , self . margin_w : - self . margin_w ] = 1 def compute_transform_matrix ( self , im_prev , im_next ): ''' Computes the transformation between two images Arguments: - im_prev: np array of shape self.img_shape and no color channels - im_next: np array of shape self.img_shape and no color channels Returns: - The affine transformation matrix, np array of shape (2,3) ''' prev_pts = cv2 . goodFeaturesToTrack ( im_prev , maxCorners = 200 , qualityLevel = 0.01 , minDistance = 30 , blockSize = 3 , mask = self . features_mask ) next_pts , status , err = cv2 . calcOpticalFlowPyrLK ( im_prev , im_next , prev_pts , None ) idx = np . where ( status == 1 )[ 0 ] prev_pts = prev_pts [ idx ] next_pts = next_pts [ idx ] m = cv2 . estimateAffine2D ( prev_pts , next_pts ) return m [ 0 ] def warp_image ( self , im , matrix ): return cv2 . warpAffine ( im , matrix , self . img_shape , flags = cv2 . INTER_LINEAR + cv2 . WARP_INVERSE_MAP ) def warp_coords ( self , coords , matrix ): '''Transforms the coords of points through the affine matrix Arguments: - coords: coordinates of points, np array of shape (X, 2) - matrix: transformation matrix of shape (2, 3) ''' matrix = cv2 . invertAffineTransform ( matrix ) linear = matrix [:, 0 : 2 ] transl = matrix [:, 2 ] return np . dot ( coords , linear ) + transl def compute_transform_matrices ( self , images_stack ): im_next = images_stack [ 0 ] matrices = [] for i in range ( len ( images_stack ) - 1 ): im_prev = im_next im_next = images_stack [ i + 1 ] matrices . append ( self . compute_transform_matrix ( im_prev , im_next )) return matrices","title":"Module mot.tracker.camera_flow"},{"location":"reference/mot/tracker/camera_flow/#classes","text":"","title":"Classes"},{"location":"reference/mot/tracker/camera_flow/#cameraflow","text":"class CameraFlow ( ) Compute camera flow through optical flow between consecutive images currently not used in the simplest version of the tracking","title":"CameraFlow"},{"location":"reference/mot/tracker/camera_flow/#methods","text":"","title":"Methods"},{"location":"reference/mot/tracker/camera_flow/#compute_transform_matrices","text":"def compute_transform_matrices ( self , images_stack ) View Source def compute_transform_matrices ( self , images_stack ): im_next = images_stack [ 0 ] matrices = [] for i in range ( len ( images_stack ) - 1 ): im_prev = im_next im_next = images_stack [ i + 1 ] matrices . append ( self . compute_transform_matrix ( im_prev , im_next )) return matrices","title":"compute_transform_matrices"},{"location":"reference/mot/tracker/camera_flow/#compute_transform_matrix","text":"def compute_transform_matrix ( self , im_prev , im_next ) Computes the transformation between two images Arguments: im_prev: np array of shape self.img_shape and no color channels im_next: np array of shape self.img_shape and no color channels Returns: The affine transformation matrix, np array of shape (2,3) View Source def compute_transform_matrix ( self , im_prev , im_next ) : ''' Computes the transformation between two images Arguments: - im_prev: np array of shape self.img_shape and no color channels - im_next: np array of shape self.img_shape and no color channels Returns: - The affine transformation matrix, np array of shape (2,3) ''' prev_pts = cv2 . goodFeaturesToTrack ( im_prev , maxCorners = 200 , qualityLevel = 0.01 , minDistance = 30 , blockSize = 3 , mask = self . features_mask ) next_pts , status , err = cv2 . calcOpticalFlowPyrLK ( im_prev , im_next , prev_pts , None ) idx = np . where ( status == 1 ) [ 0 ] prev_pts = prev_pts [ idx ] next_pts = next_pts [ idx ] m = cv2 . estimateAffine2D ( prev_pts , next_pts ) return m [ 0 ]","title":"compute_transform_matrix"},{"location":"reference/mot/tracker/camera_flow/#warp_coords","text":"def warp_coords ( self , coords , matrix ) Transforms the coords of points through the affine matrix Arguments: coords: coordinates of points, np array of shape (X, 2) matrix: transformation matrix of shape (2, 3) View Source def warp_coords ( self , coords , matrix ): '''Transforms the coords of points through the affine matrix Arguments: - coords: coordinates of points, np array of shape (X, 2) - matrix: transformation matrix of shape (2, 3) ''' matrix = cv2 . invertAffineTransform ( matrix ) linear = matrix [:, 0 : 2 ] transl = matrix [:, 2 ] return np . dot ( coords , linear ) + transl","title":"warp_coords"},{"location":"reference/mot/tracker/camera_flow/#warp_image","text":"def warp_image ( self , im , matrix ) View Source def warp_image ( self , im , matrix ): return cv2 . warpAffine ( im , matrix , self . img_shape , flags = cv2 . INTER_LINEAR + cv2 . WARP_INVERSE_MAP )","title":"warp_image"},{"location":"reference/mot/tracker/object_tracking/","text":"Module mot.tracker.object_tracking View Source import copy from typing import List import math import numpy as np from cached_property import cached_property from mot.object_detection.utils import np_box_ops from mot.tracker.tracker_utils import ratio , area , center , center_dist def similarity ( new_detection , old_detection ): new_scores , new_box , new_idx = new_detection old_scores , old_box , old_idx = old_detection scorediff = np . mean ( np . abs ( new_scores - old_scores )) ratiodiff = min ( 1.0 , abs ( ratio ( new_box ) - ratio ( old_box ))) sizediff = min ( 1.0 , abs ( area ( new_box ) - area ( old_box )) / max ( area ( new_box ), area ( old_box ))) centerdiffx = min ( 1.0 , abs ( center ( new_box )[ 0 ] - center ( old_box )[ 0 ])) centerdiffy = min ( 1.0 , abs ( center ( new_box )[ 1 ] - center ( old_box )[ 1 ])) framediff = min ( 1.0 , ( new_idx - 1 - old_idx ) / 3 ) return 1.0 - 0.5 * scorediff - 0.2 * ratiodiff - 0.2 * sizediff - 0.5 * centerdiffx - 0.2 * centerdiffy - 0.2 * framediff class Track (): '''Track (or trajectory) class mainly defined by a sequence of frames and the corresponding detections Multi-object tracking notations: - The output of the detector are called \"detections\" - The matching of successive boxes are called \"tracklets\" which are small tracks - The final trajectories are called \"tracks\" ''' def __init__ ( self , id : int , class_scores : List [ float ], box : List [ float ], frame : int ): self . id = id self . scores = [ class_scores ] self . boxes = [ box ] self . frames = [ frame ] self . speed = None self . track_score = max ( class_scores ) self . THR_SPEED = 0.0 self . THR_BOX_CENTER = 0.05 self . THR_BOX_RATIO = 0.1 def add_matching_detection ( self , scores , box , frame ): self . scores . append ( scores ) self . boxes . append ( box ) self . frames . append ( frame ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) def get_latest_np_box ( self ): return np . array ( self . boxes [ - 1 ]) def get_latest_detection ( self , apply_speed , new_frame_id ): last_box = np . array ( self . boxes [ - 1 ]) frame_offset = new_frame_id - self . frames [ - 1 ] if apply_speed and self . speed is not None : last_box = np . clip ( last_box + self . speed * frame_offset , 0.0 , 1.0 ) return [ np . array ( self . scores [ - 1 ]), last_box , self . frames [ - 1 ]] def is_in_range ( self , frame_idx , time_window ): return ( frame_idx - self . frames [ - 1 ]) <= time_window def is_valid ( self , min_length ): return len ( self . frames ) >= min_length def has_valid_speed ( self , speed ): speed_dot = np . dot ( speed , self . speed [ 0 : 2 ]) return np . sum ( speed_dot ) > self . THR_SPEED def get_center ( self ): x1 , y1 , x2 , y2 = self . boxes [ - 1 ] return ( x2 + x1 ) / 2 , ( y2 + y1 ) / 2 def get_average_scores ( self ): return np . mean ( np . array ( self . scores ), axis = 0 ) def get_label ( self ): return np . argmax ( np . array ( self . scores ) . sum ( axis = 0 )) + 1 def compute_speed ( self ): boxes_array = np . array ( self . boxes ) time_differences = np . array ([ float ( y - x ) for x , y in zip ( self . frames [ 1 :], self . frames [ 0 : - 1 ])]) speeds = ( boxes_array [ 1 :, :] - boxes_array [ 0 : - 1 , :]) . T / time_differences speed = np . mean ( speeds , axis = - 1 ) vx = ( speed [ 0 ] + speed [ 2 ]) / 2 vy = ( speed [ 1 ] + speed [ 3 ]) / 2 return np . array ([ vx , vy , vx , vy ]) def compatibility ( self , track ): \"\"\"Computes a soft compatibility score between this track and a later one. Arguments: - track: the newer track to compare to this one. Returns: - A floating point value corresponding to the compatibility. -1 is not compatible \"\"\" if self . frames [ - 1 ] >= track . frames [ 0 ]: return - 1.0 projected_box = self . get_latest_detection ( apply_speed = True , new_frame_id = track . frames [ 0 ]) old_detection = [ self . get_average_scores (), projected_box , self . frames [ - 1 ]] new_detection = [ track . get_average_scores (), track . boxes [ 0 ], track . frames [ 0 ]] return similarity ( old_detection , new_detection ) def contains_subtrack ( self , track ): \"\"\"Compare two tracks and verify if the track is a subpart of this one Arguments: - track: the newer track to compare to this one. Returns: - True if it matches, false otherwise \"\"\" if self . get_label () != track . get_label (): return False common_frames = set ( self . frames ) . intersection ( set ( track . frames )) match_box = 0. for frame in common_frames : box1 = self . boxes [ self . frames . index ( frame )] box2 = track . boxes [ track . frames . index ( frame )] if center_dist ( box1 , box2 ) < self . THR_BOX_CENTER and \\ abs ( ratio ( box1 ) - ratio ( box2 )) < self . THR_BOX_RATIO : match_box += 1. match_ratio = match_box / len ( track . frames ) if match_ratio > 0.2 : return True return False def append_track ( self , track ): self . scores . extend ( track . scores ) self . boxes . extend ( track . boxes ) self . frames . extend ( track . frames ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) def __repr__ ( self ): return \"(id: {} , label: {} , center:( {:.1f} , {:.1f} ), frames: {} )\" . format ( self . id , self . get_label (), self . get_center ()[ 0 ], self . get_center ()[ 1 ], self . frames ) def json_result ( self , class_names = [ \"bottles\" , \"others\" , \"fragments\" ]): class_names = [ \"BG\" ] + class_names rounded_boxes = [[ round ( coord , 2 ) for coord in box ] for box in self . boxes ] return { \"label\" : class_names [ self . get_label ()], \"score\" : self . track_score , \"frame_to_box\" : { frame : box for frame , box in zip ( self . frames , rounded_boxes )}, \"id\" : self . id } class ObjectTracking (): '''Wrapper class to tracking trash objects in video output frames ''' def __init__ ( self , video_id , list_path_images , list_inference_output = None , fps = 2 , list_geoloc = None ): self . video_id = video_id self . list_path_images = list_path_images self . list_inference_output = list_inference_output self . num_images = len ( list_path_images ) self . list_geoloc = list_geoloc self . fps = fps self . tracking_done = False self . iou_threshold = 0.3 self . rewind_window_match = 2 def build_tracklet_similarity_matrix ( self , tracklets : List ): \"\"\"Builds a compatibility matrix between tracklets Arguments: - new_scores: list of [classes scores] of length N - new_boxes: list of [4 coordinates] of length N - idx: integer frame idx - potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) \"\"\" nb_tracklets = len ( tracklets ) if nb_tracklets == 0 : return None m = np . zeros (( nb_tracklets , nb_tracklets )) for i in range ( nb_tracklets ): for j in range ( i , nb_tracklets ): m [ i , j ] = tracklets [ i ] . compatibility ( tracklets [ j ]) return m def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ): \"\"\"Builds a similarity matrix between new scores and boxes and existing tracklets Arguments: - new_scores: list of [classes scores] of length N - new_boxes: list of [4 coordinates] of length N - idx: integer frame idx - potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) \"\"\" nb_new = len ( new_boxes ) nb_old = len ( potential_matching_tracklets ) if nb_old == 0 or nb_new == 0 : return None m = np . zeros (( nb_new , nb_old )) for i in range ( nb_new ): new_detection = [ np . array ( new_scores [ i ]), np . array ( new_boxes [ i ]), idx ] for j in range ( nb_old ): m [ i , j ] = similarity ( new_detection , potential_matching_tracklets [ j ] . get_latest_detection ( apply_speed = True , new_frame_id = idx )) return m def average_move_speed ( self , tracklets ): \"\"\"Computes the average displacement of tracklets \"\"\" speeds = np . array ([[ tracklet . speed [ 0 ], tracklet . speed [ 1 ]] for tracklet in tracklets if tracklet . speed is not None ]) return np . mean ( speeds , axis = 0 ) def compute_tracks ( self ): \"\"\"Main function which computes tracks from detection on successive frames \"\"\" tracklets = self . build_tracklets ( self . list_inference_output , 2 , 0.5 ) print ( \"build tracks length tracklets:\" , len ( tracklets )) average_speed = self . average_move_speed ( tracklets ) filtered_tracklets = tracklets # Filter tracklets with wrong speed if np . linalg . norm ( average_speed ) > 0.05 : filtered_tracklets = list ( filter ( lambda t : t . has_valid_speed ( average_speed ), filtered_tracklets )) # Match tracklets matched_tracks = self . match_tracklets ( filtered_tracklets , average_speed ) # Filter tracklets that are too small matched_tracks = list ( filter ( lambda t : t . is_valid ( 2 ), matched_tracks )) return matched_tracks def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ): \"\"\"Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: - input_detections: list of successive frames and their corresponding boxes and classes - time_window: integer corresponding to the number of previous frames considered - matching_threshold: float threshold for accepting a match Returns: - a list of Track objects, mainly defined by [frame_ids, boxes and classes] \"\"\" tracklets = [] for frame_idx , json_object in enumerate ( input_detections ): new_scores = json_object . get ( \"output/scores:0\" , []) new_boxes = json_object . get ( \"output/boxes:0\" , []) # Expects boxes with Non maximum suppression ? # Build the list of previous trash that could be matched and similarity with the new potential_matching_tracklets = list ( filter ( lambda t : t . is_in_range ( frame_idx , time_window ), tracklets )) sim_matrix = self . build_similarity_matrix ( new_scores , new_boxes , frame_idx , potential_matching_tracklets ) # greedily match the new boxes: new_tracklets_idxs = list ( range ( len ( new_boxes ))) if sim_matrix is not None : for i in range ( len ( new_boxes )): max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # append to the corresponding tracklet potential_matching_tracklets [ matching_idx ] . add_matching_detection ( new_scores [ new_idx ], new_boxes [ new_idx ], frame_idx ) # remove from similarity matrix sim_matrix [ new_idx ,:] = - 1.0 sim_matrix [:, matching_idx ] = - 1.0 new_tracklets_idxs . remove ( new_idx ) #remaining boxes become new tracklets for i in new_tracklets_idxs : tracklets . append ( Track ( len ( tracklets ), new_scores [ i ], new_boxes [ i ], frame_idx )) return tracklets def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ): \"\"\"Match tracklets \"\"\" # greedily match the tracklets # may be improved using : # scipy.optimize.linear_sum_assignment(matrix) sim_matrix = self . build_tracklet_similarity_matrix ( tracklets ) matches = [] nb_tracklets = len ( tracklets ) list_outbound = list ( range ( nb_tracklets )) list_inbound = list ( range ( nb_tracklets )) if nb_tracklets == 0 : return [] if sim_matrix is not None : for i in range ( nb_tracklets ): max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # remove from similarity matrix sim_matrix [ new_idx ,:] = - 1.0 sim_matrix [:, matching_idx ] = - 1.0 matches . append ([ new_idx , matching_idx ]) for match in matches [:: - 1 ]: tracklets [ match [ 1 ]] . append_tracklet ( tracklets [ match [ 0 ]]) tracklet_idxes_to_remove = [ m [ 0 ] for m in matches ] return [ tracklet for i , tracklet in enumerate ( tracklets ) if i not in tracklet_idxes_to_remove ] def json_result ( self , tracks , include_geo = False ): '''Outputs a json result centered on tracked objects. Score not yet included Arguments: - include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: - a json file of the following format: ```python {\"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ {\"label\": \"bottle\", \"id\": 0, \"frames\": [23,24,25]}, {\"label\": \"fragment\", \"id\": 1, \"frames\": [32]}, ]} ``` ''' json_output = {} json_output [ \"video_length\" ] = self . num_images json_output [ \"fps\" ] = self . fps json_output [ \"video_id\" ] = self . video_id json_output [ \"detected_trash\" ] = [ track . json_result () for track in tracks ] return json_output Functions similarity def similarity ( new_detection , old_detection ) View Source def similarity ( new_detection , old_detection ): new_scores , new_box , new_idx = new_detection old_scores , old_box , old_idx = old_detection scorediff = np . mean ( np . abs ( new_scores - old_scores )) ratiodiff = min ( 1 . 0 , abs ( ratio ( new_box ) - ratio ( old_box ))) sizediff = min ( 1 . 0 , abs ( area ( new_box ) - area ( old_box )) / max ( area ( new_box ), area ( old_box ))) centerdiffx = min ( 1 . 0 , abs ( center ( new_box )[ 0 ] - center ( old_box )[ 0 ])) centerdiffy = min ( 1 . 0 , abs ( center ( new_box )[ 1 ] - center ( old_box )[ 1 ])) framediff = min ( 1 . 0 , ( new_idx - 1 - old_idx ) / 3 ) return 1 . 0 - 0 . 5 * scorediff - 0 . 2 * ratiodiff - 0 . 2 * sizediff - 0 . 5 * centerdiffx - 0 . 2 * centerdiffy - 0 . 2 * framediff Classes ObjectTracking class ObjectTracking ( video_id , list_path_images , list_inference_output = None , fps = 2 , list_geoloc = None ) Wrapper class to tracking trash objects in video output frames Methods average_move_speed def average_move_speed ( self , tracklets ) Computes the average displacement of tracklets View Source def average_move_speed ( self , tracklets ): \"\"\"Computes the average displacement of tracklets \"\"\" speeds = np . array ([[ tracklet . speed [ 0 ], tracklet . speed [ 1 ]] for tracklet in tracklets if tracklet . speed is not None ]) return np . mean ( speeds , axis = 0 ) build_similarity_matrix def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ) Builds a similarity matrix between new scores and boxes and existing tracklets Arguments: new_scores: list of [classes scores] of length N new_boxes: list of [4 coordinates] of length N idx: integer frame idx potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) View Source def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ) : \"\"\"Builds a similarity matrix between new scores and boxes and existing tracklets Arguments : - new_scores : list of [ classes scores ] of length N - new_boxes : list of [ 4 coordinates ] of length N - idx : integer frame idx - potential_matching_tracklets : list of tracklets of length M Returns : - a similarity matrix ( numpy array ) of shape ( N , M ) \"\"\" nb_new = len ( new_boxes ) nb_old = len ( potential_matching_tracklets ) if nb_old == 0 or nb_new == 0 : return None m = np . zeros (( nb_new , nb_old )) for i in range ( nb_new ) : new_detection = [ np . array ( new_scores [ i ]), np . array ( new_boxes [ i ]), idx ] for j in range ( nb_old ) : m [ i , j ] = similarity ( new_detection , potential_matching_tracklets [ j ]. get_latest_detection ( apply_speed = True , new_frame_id = idx )) return m build_tracklet_similarity_matrix def build_tracklet_similarity_matrix ( self , tracklets : List ) Builds a compatibility matrix between tracklets Arguments: new_scores: list of [classes scores] of length N new_boxes: list of [4 coordinates] of length N idx: integer frame idx potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) View Source def build_tracklet_similarity_matrix ( self , tracklets : List ) : \"\"\"Builds a compatibility matrix between tracklets Arguments : - new_scores : list of [ classes scores ] of length N - new_boxes : list of [ 4 coordinates ] of length N - idx : integer frame idx - potential_matching_tracklets : list of tracklets of length M Returns : - a similarity matrix ( numpy array ) of shape ( N , M ) \"\"\" nb_tracklets = len ( tracklets ) if nb_tracklets == 0 : return None m = np . zeros (( nb_tracklets , nb_tracklets )) for i in range ( nb_tracklets ) : for j in range ( i , nb_tracklets ) : m [ i , j ] = tracklets [ i ]. compatibility ( tracklets [ j ]) return m build_tracklets def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ) Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: input_detections: list of successive frames and their corresponding boxes and classes time_window: integer corresponding to the number of previous frames considered matching_threshold: float threshold for accepting a match Returns: a list of Track objects, mainly defined by [frame_ids, boxes and classes] View Source def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ) : \"\"\"Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: - input_detections: list of successive frames and their corresponding boxes and classes - time_window: integer corresponding to the number of previous frames considered - matching_threshold: float threshold for accepting a match Returns: - a list of Track objects, mainly defined by [frame_ids, boxes and classes] \"\"\" tracklets = [] for frame_idx , json_object in enumerate ( input_detections ) : new_scores = json_object . get ( \"output/scores:0\" , [] ) new_boxes = json_object . get ( \"output/boxes:0\" , [] ) # Expects boxes with Non maximum suppression ? # Build the list of previous trash that could be matched and similarity with the new potential_matching_tracklets = list ( filter ( lambda t : t . is_in_range ( frame_idx , time_window ), tracklets )) sim_matrix = self . build_similarity_matrix ( new_scores , new_boxes , frame_idx , potential_matching_tracklets ) # greedily match the new boxes : new_tracklets_idxs = list ( range ( len ( new_boxes ))) if sim_matrix is not None : for i in range ( len ( new_boxes )) : max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ] ) # append to the corresponding tracklet potential_matching_tracklets [ matching_idx ] . add_matching_detection ( new_scores [ new_idx ] , new_boxes [ new_idx ] , frame_idx ) # remove from similarity matrix sim_matrix [ new_idx,: ] = - 1.0 sim_matrix [ :,matching_idx ] = - 1.0 new_tracklets_idxs . remove ( new_idx ) #remaining boxes become new tracklets for i in new_tracklets_idxs : tracklets . append ( Track ( len ( tracklets ), new_scores [ i ] , new_boxes [ i ] , frame_idx )) return tracklets compute_tracks def compute_tracks ( self ) Main function which computes tracks from detection on successive frames View Source def compute_tracks ( self ): \"\"\"Main function which computes tracks from detection on successive frames \"\"\" tracklets = self . build_tracklets ( self . list_inference_output , 2 , 0 . 5 ) print ( \"build tracks length tracklets:\" , len ( tracklets )) average_speed = self . average_move_speed ( tracklets ) filtered_tracklets = tracklets # Filter tracklets with wrong speed if np . linalg . norm ( average_speed ) > 0 . 05 : filtered_tracklets = list ( filter ( lambda t : t . has_valid_speed ( average_speed ), filtered_tracklets )) # Match tracklets matched_tracks = self . match_tracklets ( filtered_tracklets , average_speed ) # Filter tracklets that are too small matched_tracks = list ( filter ( lambda t : t . is_valid ( 2 ), matched_tracks )) return matched_tracks json_result def json_result ( self , tracks , include_geo = False ) Outputs a json result centered on tracked objects. Score not yet included Arguments: include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: a json file of the following format: { \"video_length\" : 132 , \"fps\" : 2 , \"video_id\" : \"GOPRO1234.mp4\" , \"detected_trash\" : [ { \"label\" : \"bottle\" , \"id\" : 0 , \"frames\" : [ 23 , 24 , 25 ]}, { \"label\" : \"fragment\" , \"id\" : 1 , \"frames\" : [ 32 ]}, ]} View Source def json_result ( self , tracks , include_geo = False ): '''Outputs a json result centered on tracked objects. Score not yet included Arguments: - include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: - a json file of the following format: ```python {\"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ {\"label\": \"bottle\", \"id\": 0, \"frames\": [23,24,25]}, {\"label\": \"fragment\", \"id\": 1, \"frames\": [32]}, ]} ``` ''' json_output = {} json_output [ \"video_length\" ] = self . num_images json_output [ \"fps\" ] = self . fps json_output [ \"video_id\" ] = self . video_id json_output [ \"detected_trash\" ] = [ track . json_result () for track in tracks ] return json_output match_tracklets def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ) Match tracklets View Source def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ) : \"\"\"Match tracklets \"\"\" # greedily match the tracklets # may be improved using : # scipy . optimize . linear_sum_assignment ( matrix ) sim_matrix = self . build_tracklet_similarity_matrix ( tracklets ) matches = [] nb_tracklets = len ( tracklets ) list_outbound = list ( range ( nb_tracklets )) list_inbound = list ( range ( nb_tracklets )) if nb_tracklets == 0 : return [] if sim_matrix is not None : for i in range ( nb_tracklets ) : max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold: break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # remove from similarity matrix sim_matrix [ new_idx , : ] = - 1.0 sim_matrix [ : , matching_idx ] = - 1.0 matches . append ([ new_idx , matching_idx ]) for match in matches [ ::- 1 ] : tracklets [ match [ 1 ]]. append_tracklet ( tracklets [ match [ 0 ]]) tracklet_idxes_to_remove = [ m [ 0 ] for m in matches ] return [ tracklet for i , tracklet in enumerate ( tracklets ) if i not in tracklet_idxes_to_remove ] Track class Track ( id : int , class_scores : List [ float ], box : List [ float ], frame : int ) Track (or trajectory) class mainly defined by a sequence of frames and the corresponding detections Multi-object tracking notations: - The output of the detector are called \"detections\" - The matching of successive boxes are called \"tracklets\" which are small tracks - The final trajectories are called \"tracks\" Methods add_matching_detection def add_matching_detection ( self , scores , box , frame ) View Source def add_matching_detection ( self , scores , box , frame ): self . scores . append ( scores ) self . boxes . append ( box ) self . frames . append ( frame ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) append_track def append_track ( self , track ) View Source def append_track ( self , track ): self . scores . extend ( track . scores ) self . boxes . extend ( track . boxes ) self . frames . extend ( track . frames ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) compatibility def compatibility ( self , track ) Computes a soft compatibility score between this track and a later one. Arguments: track: the newer track to compare to this one. Returns: A floating point value corresponding to the compatibility. -1 is not compatible View Source def compatibility ( self , track ): \"\"\"Computes a soft compatibility score between this track and a later one. Arguments: - track: the newer track to compare to this one. Returns: - A floating point value corresponding to the compatibility. -1 is not compatible \"\"\" if self . frames [ - 1 ] >= track . frames [ 0 ]: return - 1 . 0 projected_box = self . get_latest_detection ( apply_speed = True , new_frame_id = track . frames [ 0 ]) old_detection = [ self . get_average_scores (), projected_box , self . frames [ - 1 ]] new_detection = [ track . get_average_scores (), track . boxes [ 0 ], track . frames [ 0 ]] return similarity ( old_detection , new_detection ) compute_speed def compute_speed ( self ) View Source def compute_speed ( self ) : boxes_array = np . array ( self . boxes ) time_differences = np . array ([ float ( y - x ) for x , y in zip ( self . frames [ 1 : ], self . frames [ 0 :- 1 ])]) speeds = ( boxes_array [ 1 : , : ] - boxes_array [ 0 :- 1 , : ]). T / time_differences speed = np . mean ( speeds , axis = - 1 ) vx = ( speed [ 0 ] + speed [ 2 ]) / 2 vy = ( speed [ 1 ] + speed [ 3 ]) / 2 return np . array ([ vx , vy , vx , vy ]) contains_subtrack def contains_subtrack ( self , track ) Compare two tracks and verify if the track is a subpart of this one Arguments: track: the newer track to compare to this one. Returns: True if it matches, false otherwise View Source def contains_subtrack ( self , track ): \"\"\"Compare two tracks and verify if the track is a subpart of this one Arguments: - track: the newer track to compare to this one. Returns: - True if it matches, false otherwise \"\"\" if self . get_label () != track . get_label (): return False common_frames = set ( self . frames ). intersection ( set ( track . frames )) match_box = 0 . for frame in common_frames : box1 = self . boxes [ self . frames . index ( frame )] box2 = track . boxes [ track . frames . index ( frame )] if center_dist ( box1 , box2 ) < self . THR_BOX_CENTER and \\ abs ( ratio ( box1 ) - ratio ( box2 )) < self . THR_BOX_RATIO : match_box += 1 . match_ratio = match_box / len ( track . frames ) if match_ratio > 0 . 2 : return True return False get_average_scores def get_average_scores ( self ) View Source def get_average_scores ( self ): return np . mean ( np . array ( self . scores ), axis = 0 ) get_center def get_center ( self ) View Source def get_center ( self ): x1 , y1 , x2 , y2 = self . boxes [ - 1 ] return ( x2 + x1 ) / 2 , ( y2 + y1 ) / 2 get_label def get_label ( self ) View Source def get_label ( self ): return np . argmax ( np . array ( self . scores ). sum ( axis = 0 )) + 1 get_latest_detection def get_latest_detection ( self , apply_speed , new_frame_id ) View Source def get_latest_detection ( self , apply_speed , new_frame_id ): last_box = np . array ( self . boxes [ - 1 ]) frame_offset = new_frame_id - self . frames [ - 1 ] if apply_speed and self . speed is not None : last_box = np . clip ( last_box + self . speed * frame_offset , 0 . 0 , 1 . 0 ) return [ np . array ( self . scores [ - 1 ]), last_box , self . frames [ - 1 ]] get_latest_np_box def get_latest_np_box ( self ) View Source def get_latest_np_box ( self ): return np . array ( self . boxes [ - 1 ]) has_valid_speed def has_valid_speed ( self , speed ) View Source def has_valid_speed ( self , speed ): speed_dot = np . dot ( speed , self . speed [ 0 : 2 ]) return np . sum ( speed_dot ) > self . THR_SPEED is_in_range def is_in_range ( self , frame_idx , time_window ) View Source def is_in_range ( self , frame_idx , time_window ): return ( frame_idx - self . frames [ - 1 ]) <= time_window is_valid def is_valid ( self , min_length ) View Source def is_valid ( self , min_length ): return len ( self . frames ) >= min_length json_result def json_result ( self , class_names = [ 'bottles' , 'others' , 'fragments' ] ) View Source def json_result ( self , class_names = [ \"bottles\" , \"others\" , \"fragments\" ]): class_names = [ \"BG\" ] + class_names rounded_boxes = [[ round ( coord , 2 ) for coord in box ] for box in self . boxes ] return { \"label\" : class_names [ self . get_label ()], \"score\" : self . track_score , \"frame_to_box\" : { frame : box for frame , box in zip ( self . frames , rounded_boxes ) } , \"id\" : self . id }","title":"Object Tracking"},{"location":"reference/mot/tracker/object_tracking/#module-mottrackerobject_tracking","text":"View Source import copy from typing import List import math import numpy as np from cached_property import cached_property from mot.object_detection.utils import np_box_ops from mot.tracker.tracker_utils import ratio , area , center , center_dist def similarity ( new_detection , old_detection ): new_scores , new_box , new_idx = new_detection old_scores , old_box , old_idx = old_detection scorediff = np . mean ( np . abs ( new_scores - old_scores )) ratiodiff = min ( 1.0 , abs ( ratio ( new_box ) - ratio ( old_box ))) sizediff = min ( 1.0 , abs ( area ( new_box ) - area ( old_box )) / max ( area ( new_box ), area ( old_box ))) centerdiffx = min ( 1.0 , abs ( center ( new_box )[ 0 ] - center ( old_box )[ 0 ])) centerdiffy = min ( 1.0 , abs ( center ( new_box )[ 1 ] - center ( old_box )[ 1 ])) framediff = min ( 1.0 , ( new_idx - 1 - old_idx ) / 3 ) return 1.0 - 0.5 * scorediff - 0.2 * ratiodiff - 0.2 * sizediff - 0.5 * centerdiffx - 0.2 * centerdiffy - 0.2 * framediff class Track (): '''Track (or trajectory) class mainly defined by a sequence of frames and the corresponding detections Multi-object tracking notations: - The output of the detector are called \"detections\" - The matching of successive boxes are called \"tracklets\" which are small tracks - The final trajectories are called \"tracks\" ''' def __init__ ( self , id : int , class_scores : List [ float ], box : List [ float ], frame : int ): self . id = id self . scores = [ class_scores ] self . boxes = [ box ] self . frames = [ frame ] self . speed = None self . track_score = max ( class_scores ) self . THR_SPEED = 0.0 self . THR_BOX_CENTER = 0.05 self . THR_BOX_RATIO = 0.1 def add_matching_detection ( self , scores , box , frame ): self . scores . append ( scores ) self . boxes . append ( box ) self . frames . append ( frame ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) def get_latest_np_box ( self ): return np . array ( self . boxes [ - 1 ]) def get_latest_detection ( self , apply_speed , new_frame_id ): last_box = np . array ( self . boxes [ - 1 ]) frame_offset = new_frame_id - self . frames [ - 1 ] if apply_speed and self . speed is not None : last_box = np . clip ( last_box + self . speed * frame_offset , 0.0 , 1.0 ) return [ np . array ( self . scores [ - 1 ]), last_box , self . frames [ - 1 ]] def is_in_range ( self , frame_idx , time_window ): return ( frame_idx - self . frames [ - 1 ]) <= time_window def is_valid ( self , min_length ): return len ( self . frames ) >= min_length def has_valid_speed ( self , speed ): speed_dot = np . dot ( speed , self . speed [ 0 : 2 ]) return np . sum ( speed_dot ) > self . THR_SPEED def get_center ( self ): x1 , y1 , x2 , y2 = self . boxes [ - 1 ] return ( x2 + x1 ) / 2 , ( y2 + y1 ) / 2 def get_average_scores ( self ): return np . mean ( np . array ( self . scores ), axis = 0 ) def get_label ( self ): return np . argmax ( np . array ( self . scores ) . sum ( axis = 0 )) + 1 def compute_speed ( self ): boxes_array = np . array ( self . boxes ) time_differences = np . array ([ float ( y - x ) for x , y in zip ( self . frames [ 1 :], self . frames [ 0 : - 1 ])]) speeds = ( boxes_array [ 1 :, :] - boxes_array [ 0 : - 1 , :]) . T / time_differences speed = np . mean ( speeds , axis = - 1 ) vx = ( speed [ 0 ] + speed [ 2 ]) / 2 vy = ( speed [ 1 ] + speed [ 3 ]) / 2 return np . array ([ vx , vy , vx , vy ]) def compatibility ( self , track ): \"\"\"Computes a soft compatibility score between this track and a later one. Arguments: - track: the newer track to compare to this one. Returns: - A floating point value corresponding to the compatibility. -1 is not compatible \"\"\" if self . frames [ - 1 ] >= track . frames [ 0 ]: return - 1.0 projected_box = self . get_latest_detection ( apply_speed = True , new_frame_id = track . frames [ 0 ]) old_detection = [ self . get_average_scores (), projected_box , self . frames [ - 1 ]] new_detection = [ track . get_average_scores (), track . boxes [ 0 ], track . frames [ 0 ]] return similarity ( old_detection , new_detection ) def contains_subtrack ( self , track ): \"\"\"Compare two tracks and verify if the track is a subpart of this one Arguments: - track: the newer track to compare to this one. Returns: - True if it matches, false otherwise \"\"\" if self . get_label () != track . get_label (): return False common_frames = set ( self . frames ) . intersection ( set ( track . frames )) match_box = 0. for frame in common_frames : box1 = self . boxes [ self . frames . index ( frame )] box2 = track . boxes [ track . frames . index ( frame )] if center_dist ( box1 , box2 ) < self . THR_BOX_CENTER and \\ abs ( ratio ( box1 ) - ratio ( box2 )) < self . THR_BOX_RATIO : match_box += 1. match_ratio = match_box / len ( track . frames ) if match_ratio > 0.2 : return True return False def append_track ( self , track ): self . scores . extend ( track . scores ) self . boxes . extend ( track . boxes ) self . frames . extend ( track . frames ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 )) def __repr__ ( self ): return \"(id: {} , label: {} , center:( {:.1f} , {:.1f} ), frames: {} )\" . format ( self . id , self . get_label (), self . get_center ()[ 0 ], self . get_center ()[ 1 ], self . frames ) def json_result ( self , class_names = [ \"bottles\" , \"others\" , \"fragments\" ]): class_names = [ \"BG\" ] + class_names rounded_boxes = [[ round ( coord , 2 ) for coord in box ] for box in self . boxes ] return { \"label\" : class_names [ self . get_label ()], \"score\" : self . track_score , \"frame_to_box\" : { frame : box for frame , box in zip ( self . frames , rounded_boxes )}, \"id\" : self . id } class ObjectTracking (): '''Wrapper class to tracking trash objects in video output frames ''' def __init__ ( self , video_id , list_path_images , list_inference_output = None , fps = 2 , list_geoloc = None ): self . video_id = video_id self . list_path_images = list_path_images self . list_inference_output = list_inference_output self . num_images = len ( list_path_images ) self . list_geoloc = list_geoloc self . fps = fps self . tracking_done = False self . iou_threshold = 0.3 self . rewind_window_match = 2 def build_tracklet_similarity_matrix ( self , tracklets : List ): \"\"\"Builds a compatibility matrix between tracklets Arguments: - new_scores: list of [classes scores] of length N - new_boxes: list of [4 coordinates] of length N - idx: integer frame idx - potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) \"\"\" nb_tracklets = len ( tracklets ) if nb_tracklets == 0 : return None m = np . zeros (( nb_tracklets , nb_tracklets )) for i in range ( nb_tracklets ): for j in range ( i , nb_tracklets ): m [ i , j ] = tracklets [ i ] . compatibility ( tracklets [ j ]) return m def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ): \"\"\"Builds a similarity matrix between new scores and boxes and existing tracklets Arguments: - new_scores: list of [classes scores] of length N - new_boxes: list of [4 coordinates] of length N - idx: integer frame idx - potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) \"\"\" nb_new = len ( new_boxes ) nb_old = len ( potential_matching_tracklets ) if nb_old == 0 or nb_new == 0 : return None m = np . zeros (( nb_new , nb_old )) for i in range ( nb_new ): new_detection = [ np . array ( new_scores [ i ]), np . array ( new_boxes [ i ]), idx ] for j in range ( nb_old ): m [ i , j ] = similarity ( new_detection , potential_matching_tracklets [ j ] . get_latest_detection ( apply_speed = True , new_frame_id = idx )) return m def average_move_speed ( self , tracklets ): \"\"\"Computes the average displacement of tracklets \"\"\" speeds = np . array ([[ tracklet . speed [ 0 ], tracklet . speed [ 1 ]] for tracklet in tracklets if tracklet . speed is not None ]) return np . mean ( speeds , axis = 0 ) def compute_tracks ( self ): \"\"\"Main function which computes tracks from detection on successive frames \"\"\" tracklets = self . build_tracklets ( self . list_inference_output , 2 , 0.5 ) print ( \"build tracks length tracklets:\" , len ( tracklets )) average_speed = self . average_move_speed ( tracklets ) filtered_tracklets = tracklets # Filter tracklets with wrong speed if np . linalg . norm ( average_speed ) > 0.05 : filtered_tracklets = list ( filter ( lambda t : t . has_valid_speed ( average_speed ), filtered_tracklets )) # Match tracklets matched_tracks = self . match_tracklets ( filtered_tracklets , average_speed ) # Filter tracklets that are too small matched_tracks = list ( filter ( lambda t : t . is_valid ( 2 ), matched_tracks )) return matched_tracks def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ): \"\"\"Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: - input_detections: list of successive frames and their corresponding boxes and classes - time_window: integer corresponding to the number of previous frames considered - matching_threshold: float threshold for accepting a match Returns: - a list of Track objects, mainly defined by [frame_ids, boxes and classes] \"\"\" tracklets = [] for frame_idx , json_object in enumerate ( input_detections ): new_scores = json_object . get ( \"output/scores:0\" , []) new_boxes = json_object . get ( \"output/boxes:0\" , []) # Expects boxes with Non maximum suppression ? # Build the list of previous trash that could be matched and similarity with the new potential_matching_tracklets = list ( filter ( lambda t : t . is_in_range ( frame_idx , time_window ), tracklets )) sim_matrix = self . build_similarity_matrix ( new_scores , new_boxes , frame_idx , potential_matching_tracklets ) # greedily match the new boxes: new_tracklets_idxs = list ( range ( len ( new_boxes ))) if sim_matrix is not None : for i in range ( len ( new_boxes )): max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # append to the corresponding tracklet potential_matching_tracklets [ matching_idx ] . add_matching_detection ( new_scores [ new_idx ], new_boxes [ new_idx ], frame_idx ) # remove from similarity matrix sim_matrix [ new_idx ,:] = - 1.0 sim_matrix [:, matching_idx ] = - 1.0 new_tracklets_idxs . remove ( new_idx ) #remaining boxes become new tracklets for i in new_tracklets_idxs : tracklets . append ( Track ( len ( tracklets ), new_scores [ i ], new_boxes [ i ], frame_idx )) return tracklets def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ): \"\"\"Match tracklets \"\"\" # greedily match the tracklets # may be improved using : # scipy.optimize.linear_sum_assignment(matrix) sim_matrix = self . build_tracklet_similarity_matrix ( tracklets ) matches = [] nb_tracklets = len ( tracklets ) list_outbound = list ( range ( nb_tracklets )) list_inbound = list ( range ( nb_tracklets )) if nb_tracklets == 0 : return [] if sim_matrix is not None : for i in range ( nb_tracklets ): max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # remove from similarity matrix sim_matrix [ new_idx ,:] = - 1.0 sim_matrix [:, matching_idx ] = - 1.0 matches . append ([ new_idx , matching_idx ]) for match in matches [:: - 1 ]: tracklets [ match [ 1 ]] . append_tracklet ( tracklets [ match [ 0 ]]) tracklet_idxes_to_remove = [ m [ 0 ] for m in matches ] return [ tracklet for i , tracklet in enumerate ( tracklets ) if i not in tracklet_idxes_to_remove ] def json_result ( self , tracks , include_geo = False ): '''Outputs a json result centered on tracked objects. Score not yet included Arguments: - include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: - a json file of the following format: ```python {\"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ {\"label\": \"bottle\", \"id\": 0, \"frames\": [23,24,25]}, {\"label\": \"fragment\", \"id\": 1, \"frames\": [32]}, ]} ``` ''' json_output = {} json_output [ \"video_length\" ] = self . num_images json_output [ \"fps\" ] = self . fps json_output [ \"video_id\" ] = self . video_id json_output [ \"detected_trash\" ] = [ track . json_result () for track in tracks ] return json_output","title":"Module mot.tracker.object_tracking"},{"location":"reference/mot/tracker/object_tracking/#functions","text":"","title":"Functions"},{"location":"reference/mot/tracker/object_tracking/#similarity","text":"def similarity ( new_detection , old_detection ) View Source def similarity ( new_detection , old_detection ): new_scores , new_box , new_idx = new_detection old_scores , old_box , old_idx = old_detection scorediff = np . mean ( np . abs ( new_scores - old_scores )) ratiodiff = min ( 1 . 0 , abs ( ratio ( new_box ) - ratio ( old_box ))) sizediff = min ( 1 . 0 , abs ( area ( new_box ) - area ( old_box )) / max ( area ( new_box ), area ( old_box ))) centerdiffx = min ( 1 . 0 , abs ( center ( new_box )[ 0 ] - center ( old_box )[ 0 ])) centerdiffy = min ( 1 . 0 , abs ( center ( new_box )[ 1 ] - center ( old_box )[ 1 ])) framediff = min ( 1 . 0 , ( new_idx - 1 - old_idx ) / 3 ) return 1 . 0 - 0 . 5 * scorediff - 0 . 2 * ratiodiff - 0 . 2 * sizediff - 0 . 5 * centerdiffx - 0 . 2 * centerdiffy - 0 . 2 * framediff","title":"similarity"},{"location":"reference/mot/tracker/object_tracking/#classes","text":"","title":"Classes"},{"location":"reference/mot/tracker/object_tracking/#objecttracking","text":"class ObjectTracking ( video_id , list_path_images , list_inference_output = None , fps = 2 , list_geoloc = None ) Wrapper class to tracking trash objects in video output frames","title":"ObjectTracking"},{"location":"reference/mot/tracker/object_tracking/#methods","text":"","title":"Methods"},{"location":"reference/mot/tracker/object_tracking/#average_move_speed","text":"def average_move_speed ( self , tracklets ) Computes the average displacement of tracklets View Source def average_move_speed ( self , tracklets ): \"\"\"Computes the average displacement of tracklets \"\"\" speeds = np . array ([[ tracklet . speed [ 0 ], tracklet . speed [ 1 ]] for tracklet in tracklets if tracklet . speed is not None ]) return np . mean ( speeds , axis = 0 )","title":"average_move_speed"},{"location":"reference/mot/tracker/object_tracking/#build_similarity_matrix","text":"def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ) Builds a similarity matrix between new scores and boxes and existing tracklets Arguments: new_scores: list of [classes scores] of length N new_boxes: list of [4 coordinates] of length N idx: integer frame idx potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) View Source def build_similarity_matrix ( self , new_scores : List , new_boxes : List , idx : int , potential_matching_tracklets : List ) : \"\"\"Builds a similarity matrix between new scores and boxes and existing tracklets Arguments : - new_scores : list of [ classes scores ] of length N - new_boxes : list of [ 4 coordinates ] of length N - idx : integer frame idx - potential_matching_tracklets : list of tracklets of length M Returns : - a similarity matrix ( numpy array ) of shape ( N , M ) \"\"\" nb_new = len ( new_boxes ) nb_old = len ( potential_matching_tracklets ) if nb_old == 0 or nb_new == 0 : return None m = np . zeros (( nb_new , nb_old )) for i in range ( nb_new ) : new_detection = [ np . array ( new_scores [ i ]), np . array ( new_boxes [ i ]), idx ] for j in range ( nb_old ) : m [ i , j ] = similarity ( new_detection , potential_matching_tracklets [ j ]. get_latest_detection ( apply_speed = True , new_frame_id = idx )) return m","title":"build_similarity_matrix"},{"location":"reference/mot/tracker/object_tracking/#build_tracklet_similarity_matrix","text":"def build_tracklet_similarity_matrix ( self , tracklets : List ) Builds a compatibility matrix between tracklets Arguments: new_scores: list of [classes scores] of length N new_boxes: list of [4 coordinates] of length N idx: integer frame idx potential_matching_tracklets: list of tracklets of length M Returns: - a similarity matrix (numpy array) of shape (N, M) View Source def build_tracklet_similarity_matrix ( self , tracklets : List ) : \"\"\"Builds a compatibility matrix between tracklets Arguments : - new_scores : list of [ classes scores ] of length N - new_boxes : list of [ 4 coordinates ] of length N - idx : integer frame idx - potential_matching_tracklets : list of tracklets of length M Returns : - a similarity matrix ( numpy array ) of shape ( N , M ) \"\"\" nb_tracklets = len ( tracklets ) if nb_tracklets == 0 : return None m = np . zeros (( nb_tracklets , nb_tracklets )) for i in range ( nb_tracklets ) : for j in range ( i , nb_tracklets ) : m [ i , j ] = tracklets [ i ]. compatibility ( tracklets [ j ]) return m","title":"build_tracklet_similarity_matrix"},{"location":"reference/mot/tracker/object_tracking/#build_tracklets","text":"def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ) Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: input_detections: list of successive frames and their corresponding boxes and classes time_window: integer corresponding to the number of previous frames considered matching_threshold: float threshold for accepting a match Returns: a list of Track objects, mainly defined by [frame_ids, boxes and classes] View Source def build_tracklets ( self , input_detections , time_window = 2 , matching_threshold = 0.5 ) : \"\"\"Builds tracklets, i.e. confident matching between successive frames using a greedy algorithm (considers the best matching previous box) Arguments: - input_detections: list of successive frames and their corresponding boxes and classes - time_window: integer corresponding to the number of previous frames considered - matching_threshold: float threshold for accepting a match Returns: - a list of Track objects, mainly defined by [frame_ids, boxes and classes] \"\"\" tracklets = [] for frame_idx , json_object in enumerate ( input_detections ) : new_scores = json_object . get ( \"output/scores:0\" , [] ) new_boxes = json_object . get ( \"output/boxes:0\" , [] ) # Expects boxes with Non maximum suppression ? # Build the list of previous trash that could be matched and similarity with the new potential_matching_tracklets = list ( filter ( lambda t : t . is_in_range ( frame_idx , time_window ), tracklets )) sim_matrix = self . build_similarity_matrix ( new_scores , new_boxes , frame_idx , potential_matching_tracklets ) # greedily match the new boxes : new_tracklets_idxs = list ( range ( len ( new_boxes ))) if sim_matrix is not None : for i in range ( len ( new_boxes )) : max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold : break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ] ) # append to the corresponding tracklet potential_matching_tracklets [ matching_idx ] . add_matching_detection ( new_scores [ new_idx ] , new_boxes [ new_idx ] , frame_idx ) # remove from similarity matrix sim_matrix [ new_idx,: ] = - 1.0 sim_matrix [ :,matching_idx ] = - 1.0 new_tracklets_idxs . remove ( new_idx ) #remaining boxes become new tracklets for i in new_tracklets_idxs : tracklets . append ( Track ( len ( tracklets ), new_scores [ i ] , new_boxes [ i ] , frame_idx )) return tracklets","title":"build_tracklets"},{"location":"reference/mot/tracker/object_tracking/#compute_tracks","text":"def compute_tracks ( self ) Main function which computes tracks from detection on successive frames View Source def compute_tracks ( self ): \"\"\"Main function which computes tracks from detection on successive frames \"\"\" tracklets = self . build_tracklets ( self . list_inference_output , 2 , 0 . 5 ) print ( \"build tracks length tracklets:\" , len ( tracklets )) average_speed = self . average_move_speed ( tracklets ) filtered_tracklets = tracklets # Filter tracklets with wrong speed if np . linalg . norm ( average_speed ) > 0 . 05 : filtered_tracklets = list ( filter ( lambda t : t . has_valid_speed ( average_speed ), filtered_tracklets )) # Match tracklets matched_tracks = self . match_tracklets ( filtered_tracklets , average_speed ) # Filter tracklets that are too small matched_tracks = list ( filter ( lambda t : t . is_valid ( 2 ), matched_tracks )) return matched_tracks","title":"compute_tracks"},{"location":"reference/mot/tracker/object_tracking/#json_result","text":"def json_result ( self , tracks , include_geo = False ) Outputs a json result centered on tracked objects. Score not yet included Arguments: include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: a json file of the following format: { \"video_length\" : 132 , \"fps\" : 2 , \"video_id\" : \"GOPRO1234.mp4\" , \"detected_trash\" : [ { \"label\" : \"bottle\" , \"id\" : 0 , \"frames\" : [ 23 , 24 , 25 ]}, { \"label\" : \"fragment\" , \"id\" : 1 , \"frames\" : [ 32 ]}, ]} View Source def json_result ( self , tracks , include_geo = False ): '''Outputs a json result centered on tracked objects. Score not yet included Arguments: - include_geo: Boolean which specifies whether the return format includes the geolocalization data, or just the simple timestamp data Returns: - a json file of the following format: ```python {\"video_length\": 132, \"fps\": 2, \"video_id\": \"GOPRO1234.mp4\", \"detected_trash\": [ {\"label\": \"bottle\", \"id\": 0, \"frames\": [23,24,25]}, {\"label\": \"fragment\", \"id\": 1, \"frames\": [32]}, ]} ``` ''' json_output = {} json_output [ \"video_length\" ] = self . num_images json_output [ \"fps\" ] = self . fps json_output [ \"video_id\" ] = self . video_id json_output [ \"detected_trash\" ] = [ track . json_result () for track in tracks ] return json_output","title":"json_result"},{"location":"reference/mot/tracker/object_tracking/#match_tracklets","text":"def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ) Match tracklets View Source def match_tracklets ( self , tracklets , average_speed , matching_threshold = 0.5 ) : \"\"\"Match tracklets \"\"\" # greedily match the tracklets # may be improved using : # scipy . optimize . linear_sum_assignment ( matrix ) sim_matrix = self . build_tracklet_similarity_matrix ( tracklets ) matches = [] nb_tracklets = len ( tracklets ) list_outbound = list ( range ( nb_tracklets )) list_inbound = list ( range ( nb_tracklets )) if nb_tracklets == 0 : return [] if sim_matrix is not None : for i in range ( nb_tracklets ) : max_values = np . max ( sim_matrix , axis = 1 ) if np . max ( max_values ) < matching_threshold: break new_idx = np . argmax ( max_values ) matching_idx = np . argmax ( sim_matrix [ new_idx ]) # remove from similarity matrix sim_matrix [ new_idx , : ] = - 1.0 sim_matrix [ : , matching_idx ] = - 1.0 matches . append ([ new_idx , matching_idx ]) for match in matches [ ::- 1 ] : tracklets [ match [ 1 ]]. append_tracklet ( tracklets [ match [ 0 ]]) tracklet_idxes_to_remove = [ m [ 0 ] for m in matches ] return [ tracklet for i , tracklet in enumerate ( tracklets ) if i not in tracklet_idxes_to_remove ]","title":"match_tracklets"},{"location":"reference/mot/tracker/object_tracking/#track","text":"class Track ( id : int , class_scores : List [ float ], box : List [ float ], frame : int ) Track (or trajectory) class mainly defined by a sequence of frames and the corresponding detections Multi-object tracking notations: - The output of the detector are called \"detections\" - The matching of successive boxes are called \"tracklets\" which are small tracks - The final trajectories are called \"tracks\"","title":"Track"},{"location":"reference/mot/tracker/object_tracking/#methods_1","text":"","title":"Methods"},{"location":"reference/mot/tracker/object_tracking/#add_matching_detection","text":"def add_matching_detection ( self , scores , box , frame ) View Source def add_matching_detection ( self , scores , box , frame ): self . scores . append ( scores ) self . boxes . append ( box ) self . frames . append ( frame ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 ))","title":"add_matching_detection"},{"location":"reference/mot/tracker/object_tracking/#append_track","text":"def append_track ( self , track ) View Source def append_track ( self , track ): self . scores . extend ( track . scores ) self . boxes . extend ( track . boxes ) self . frames . extend ( track . frames ) if len ( self . boxes ) > 2 : self . speed = self . compute_speed () self . track_score = np . max ( np . mean ( np . array ( self . scores ), axis = 0 ))","title":"append_track"},{"location":"reference/mot/tracker/object_tracking/#compatibility","text":"def compatibility ( self , track ) Computes a soft compatibility score between this track and a later one. Arguments: track: the newer track to compare to this one. Returns: A floating point value corresponding to the compatibility. -1 is not compatible View Source def compatibility ( self , track ): \"\"\"Computes a soft compatibility score between this track and a later one. Arguments: - track: the newer track to compare to this one. Returns: - A floating point value corresponding to the compatibility. -1 is not compatible \"\"\" if self . frames [ - 1 ] >= track . frames [ 0 ]: return - 1 . 0 projected_box = self . get_latest_detection ( apply_speed = True , new_frame_id = track . frames [ 0 ]) old_detection = [ self . get_average_scores (), projected_box , self . frames [ - 1 ]] new_detection = [ track . get_average_scores (), track . boxes [ 0 ], track . frames [ 0 ]] return similarity ( old_detection , new_detection )","title":"compatibility"},{"location":"reference/mot/tracker/object_tracking/#compute_speed","text":"def compute_speed ( self ) View Source def compute_speed ( self ) : boxes_array = np . array ( self . boxes ) time_differences = np . array ([ float ( y - x ) for x , y in zip ( self . frames [ 1 : ], self . frames [ 0 :- 1 ])]) speeds = ( boxes_array [ 1 : , : ] - boxes_array [ 0 :- 1 , : ]). T / time_differences speed = np . mean ( speeds , axis = - 1 ) vx = ( speed [ 0 ] + speed [ 2 ]) / 2 vy = ( speed [ 1 ] + speed [ 3 ]) / 2 return np . array ([ vx , vy , vx , vy ])","title":"compute_speed"},{"location":"reference/mot/tracker/object_tracking/#contains_subtrack","text":"def contains_subtrack ( self , track ) Compare two tracks and verify if the track is a subpart of this one Arguments: track: the newer track to compare to this one. Returns: True if it matches, false otherwise View Source def contains_subtrack ( self , track ): \"\"\"Compare two tracks and verify if the track is a subpart of this one Arguments: - track: the newer track to compare to this one. Returns: - True if it matches, false otherwise \"\"\" if self . get_label () != track . get_label (): return False common_frames = set ( self . frames ). intersection ( set ( track . frames )) match_box = 0 . for frame in common_frames : box1 = self . boxes [ self . frames . index ( frame )] box2 = track . boxes [ track . frames . index ( frame )] if center_dist ( box1 , box2 ) < self . THR_BOX_CENTER and \\ abs ( ratio ( box1 ) - ratio ( box2 )) < self . THR_BOX_RATIO : match_box += 1 . match_ratio = match_box / len ( track . frames ) if match_ratio > 0 . 2 : return True return False","title":"contains_subtrack"},{"location":"reference/mot/tracker/object_tracking/#get_average_scores","text":"def get_average_scores ( self ) View Source def get_average_scores ( self ): return np . mean ( np . array ( self . scores ), axis = 0 )","title":"get_average_scores"},{"location":"reference/mot/tracker/object_tracking/#get_center","text":"def get_center ( self ) View Source def get_center ( self ): x1 , y1 , x2 , y2 = self . boxes [ - 1 ] return ( x2 + x1 ) / 2 , ( y2 + y1 ) / 2","title":"get_center"},{"location":"reference/mot/tracker/object_tracking/#get_label","text":"def get_label ( self ) View Source def get_label ( self ): return np . argmax ( np . array ( self . scores ). sum ( axis = 0 )) + 1","title":"get_label"},{"location":"reference/mot/tracker/object_tracking/#get_latest_detection","text":"def get_latest_detection ( self , apply_speed , new_frame_id ) View Source def get_latest_detection ( self , apply_speed , new_frame_id ): last_box = np . array ( self . boxes [ - 1 ]) frame_offset = new_frame_id - self . frames [ - 1 ] if apply_speed and self . speed is not None : last_box = np . clip ( last_box + self . speed * frame_offset , 0 . 0 , 1 . 0 ) return [ np . array ( self . scores [ - 1 ]), last_box , self . frames [ - 1 ]]","title":"get_latest_detection"},{"location":"reference/mot/tracker/object_tracking/#get_latest_np_box","text":"def get_latest_np_box ( self ) View Source def get_latest_np_box ( self ): return np . array ( self . boxes [ - 1 ])","title":"get_latest_np_box"},{"location":"reference/mot/tracker/object_tracking/#has_valid_speed","text":"def has_valid_speed ( self , speed ) View Source def has_valid_speed ( self , speed ): speed_dot = np . dot ( speed , self . speed [ 0 : 2 ]) return np . sum ( speed_dot ) > self . THR_SPEED","title":"has_valid_speed"},{"location":"reference/mot/tracker/object_tracking/#is_in_range","text":"def is_in_range ( self , frame_idx , time_window ) View Source def is_in_range ( self , frame_idx , time_window ): return ( frame_idx - self . frames [ - 1 ]) <= time_window","title":"is_in_range"},{"location":"reference/mot/tracker/object_tracking/#is_valid","text":"def is_valid ( self , min_length ) View Source def is_valid ( self , min_length ): return len ( self . frames ) >= min_length","title":"is_valid"},{"location":"reference/mot/tracker/object_tracking/#json_result_1","text":"def json_result ( self , class_names = [ 'bottles' , 'others' , 'fragments' ] ) View Source def json_result ( self , class_names = [ \"bottles\" , \"others\" , \"fragments\" ]): class_names = [ \"BG\" ] + class_names rounded_boxes = [[ round ( coord , 2 ) for coord in box ] for box in self . boxes ] return { \"label\" : class_names [ self . get_label ()], \"score\" : self . track_score , \"frame_to_box\" : { frame : box for frame , box in zip ( self . frames , rounded_boxes ) } , \"id\" : self . id }","title":"json_result"},{"location":"reference/mot/tracker/tracker_metrics/","text":"Module mot.tracker.tracker_metrics View Source import copy from typing import List import math import numpy as np from collections import Counter from mot.object_detection.utils import np_box_ops from mot.tracker import object_tracking def compute_scores ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths ** Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 ** - Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. - Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks - Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory - False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). ''' score_MT = 0. score_OT = 0. score_UT = 0. remaining_tracks = copy . deepcopy ( tracks ) for track_gt in tracks_gt : label_gt = track_gt . get_label () matched_tracks = [] for track_idx , track in enumerate ( remaining_tracks ): if track_gt . contains_subtrack ( track ): matched_tracks . append ( track_idx ) for index in sorted ( matched_tracks , reverse = True ): del remaining_tracks [ index ] if matched_tracks == []: score_UT += 1. elif len ( matched_tracks ) == 1 : score_MT += 1. else : score_OT += 1. score_UT /= len ( tracks_gt ) score_OT /= len ( tracks_gt ) score_MT /= len ( tracks_gt ) return score_MT , score_OT , score_UT def count_match ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section ''' num_gt = Counter ([ track . get_label () for track in tracks_gt ]) num = Counter ([ track . get_label () for track in tracks ]) num_gt . subtract ( num ) return dict ( num_gt ) Functions compute_scores def compute_scores ( tracks : List [ mot . tracker . object_tracking . Track ], tracks_gt : List [ mot . tracker . object_tracking . Track ] ) Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). View Source def compute_scores ( tracks : List [ object_tracking.Track ] , tracks_gt : List [ object_tracking.Track ] ) : ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths ** Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 ** - Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. - Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks - Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory - False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). ''' score_MT = 0. score_OT = 0. score_UT = 0. remaining_tracks = copy . deepcopy ( tracks ) for track_gt in tracks_gt : label_gt = track_gt . get_label () matched_tracks = [] for track_idx , track in enumerate ( remaining_tracks ) : if track_gt . contains_subtrack ( track ) : matched_tracks . append ( track_idx ) for index in sorted ( matched_tracks , reverse = True ) : del remaining_tracks [ index ] if matched_tracks == []: score_UT += 1. elif len ( matched_tracks ) == 1 : score_MT += 1. else : score_OT += 1. score_UT /= len ( tracks_gt ) score_OT /= len ( tracks_gt ) score_MT /= len ( tracks_gt ) return score_MT , score_OT , score_UT count_match def count_match ( tracks : List [ mot . tracker . object_tracking . Track ], tracks_gt : List [ mot . tracker . object_tracking . Track ] ) Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section View Source def count_match ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section ''' num_gt = Counter ([ track . get_label () for track in tracks_gt ]) num = Counter ([ track . get_label () for track in tracks ]) num_gt . subtract ( num ) return dict ( num_gt )","title":"Tracker Metrics"},{"location":"reference/mot/tracker/tracker_metrics/#module-mottrackertracker_metrics","text":"View Source import copy from typing import List import math import numpy as np from collections import Counter from mot.object_detection.utils import np_box_ops from mot.tracker import object_tracking def compute_scores ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths ** Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 ** - Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. - Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks - Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory - False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). ''' score_MT = 0. score_OT = 0. score_UT = 0. remaining_tracks = copy . deepcopy ( tracks ) for track_gt in tracks_gt : label_gt = track_gt . get_label () matched_tracks = [] for track_idx , track in enumerate ( remaining_tracks ): if track_gt . contains_subtrack ( track ): matched_tracks . append ( track_idx ) for index in sorted ( matched_tracks , reverse = True ): del remaining_tracks [ index ] if matched_tracks == []: score_UT += 1. elif len ( matched_tracks ) == 1 : score_MT += 1. else : score_OT += 1. score_UT /= len ( tracks_gt ) score_OT /= len ( tracks_gt ) score_MT /= len ( tracks_gt ) return score_MT , score_OT , score_UT def count_match ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section ''' num_gt = Counter ([ track . get_label () for track in tracks_gt ]) num = Counter ([ track . get_label () for track in tracks ]) num_gt . subtract ( num ) return dict ( num_gt )","title":"Module mot.tracker.tracker_metrics"},{"location":"reference/mot/tracker/tracker_metrics/#functions","text":"","title":"Functions"},{"location":"reference/mot/tracker/tracker_metrics/#compute_scores","text":"def compute_scores ( tracks : List [ mot . tracker . object_tracking . Track ], tracks_gt : List [ mot . tracker . object_tracking . Track ] ) Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). View Source def compute_scores ( tracks : List [ object_tracking.Track ] , tracks_gt : List [ object_tracking.Track ] ) : ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths ** Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection, CVPR 2006 ** - Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 50% of the frames. - Over Tracked (OT) trajectories: number of ground-truth trajectories that are tracked by several (>1) different tracks - Un-tracked (UT) trajectories: number of ground-truth trajectories that are not matched with a single trajectory - False trajectories (FT): predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). ''' score_MT = 0. score_OT = 0. score_UT = 0. remaining_tracks = copy . deepcopy ( tracks ) for track_gt in tracks_gt : label_gt = track_gt . get_label () matched_tracks = [] for track_idx , track in enumerate ( remaining_tracks ) : if track_gt . contains_subtrack ( track ) : matched_tracks . append ( track_idx ) for index in sorted ( matched_tracks , reverse = True ) : del remaining_tracks [ index ] if matched_tracks == []: score_UT += 1. elif len ( matched_tracks ) == 1 : score_MT += 1. else : score_OT += 1. score_UT /= len ( tracks_gt ) score_OT /= len ( tracks_gt ) score_MT /= len ( tracks_gt ) return score_MT , score_OT , score_UT","title":"compute_scores"},{"location":"reference/mot/tracker/tracker_metrics/#count_match","text":"def count_match ( tracks : List [ mot . tracker . object_tracking . Track ], tracks_gt : List [ mot . tracker . object_tracking . Track ] ) Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section View Source def count_match ( tracks : List [ object_tracking . Track ], tracks_gt : List [ object_tracking . Track ]): ''' Computes the tracking matches between a list of tracklets and a list of tracklets ground truths Only based on the number of detected object in the section ''' num_gt = Counter ([ track . get_label () for track in tracks_gt ]) num = Counter ([ track . get_label () for track in tracks ]) num_gt . subtract ( num ) return dict ( num_gt )","title":"count_match"},{"location":"reference/mot/tracker/tracker_utils/","text":"Module mot.tracker.tracker_utils View Source import math def area ( box ): return math . sqrt (( box [ 2 ] - box [ 0 ]) * ( box [ 3 ] - box [ 1 ])) def ratio ( box ): return ( box [ 2 ] - box [ 0 ]) / ( box [ 3 ] - box [ 1 ]) def center ( box ): return (( box [ 2 ] + box [ 0 ]) / 2 , ( box [ 3 ] + box [ 1 ]) / 2 ) def center_dist ( box1 , box2 ): c1 = center ( box1 ) c2 = center ( box2 ) return math . sqrt (( c1 [ 0 ] - c2 [ 0 ]) ** 2 + ( c1 [ 1 ] - c2 [ 1 ]) ** 2 ) Functions area def area ( box ) View Source def area ( box ): return math . sqrt (( box [ 2 ] - box [ 0 ]) * ( box [ 3 ] - box [ 1 ])) center def center ( box ) View Source def center ( box ): return (( box [ 2 ] + box [ 0 ]) / 2 , ( box [ 3 ] + box [ 1 ]) / 2 ) center_dist def center_dist ( box1 , box2 ) View Source def center_dist ( box1 , box2 ): c1 = center ( box1 ) c2 = center ( box2 ) return math . sqrt (( c1 [ 0 ] - c2 [ 0 ]) ** 2 + ( c1 [ 1 ] - c2 [ 1 ]) ** 2 ) ratio def ratio ( box ) View Source def ratio ( box ): return ( box [ 2 ] - box [ 0 ]) / ( box [ 3 ] - box [ 1 ])","title":"Tracker Utils"},{"location":"reference/mot/tracker/tracker_utils/#module-mottrackertracker_utils","text":"View Source import math def area ( box ): return math . sqrt (( box [ 2 ] - box [ 0 ]) * ( box [ 3 ] - box [ 1 ])) def ratio ( box ): return ( box [ 2 ] - box [ 0 ]) / ( box [ 3 ] - box [ 1 ]) def center ( box ): return (( box [ 2 ] + box [ 0 ]) / 2 , ( box [ 3 ] + box [ 1 ]) / 2 ) def center_dist ( box1 , box2 ): c1 = center ( box1 ) c2 = center ( box2 ) return math . sqrt (( c1 [ 0 ] - c2 [ 0 ]) ** 2 + ( c1 [ 1 ] - c2 [ 1 ]) ** 2 )","title":"Module mot.tracker.tracker_utils"},{"location":"reference/mot/tracker/tracker_utils/#functions","text":"","title":"Functions"},{"location":"reference/mot/tracker/tracker_utils/#area","text":"def area ( box ) View Source def area ( box ): return math . sqrt (( box [ 2 ] - box [ 0 ]) * ( box [ 3 ] - box [ 1 ]))","title":"area"},{"location":"reference/mot/tracker/tracker_utils/#center","text":"def center ( box ) View Source def center ( box ): return (( box [ 2 ] + box [ 0 ]) / 2 , ( box [ 3 ] + box [ 1 ]) / 2 )","title":"center"},{"location":"reference/mot/tracker/tracker_utils/#center_dist","text":"def center_dist ( box1 , box2 ) View Source def center_dist ( box1 , box2 ): c1 = center ( box1 ) c2 = center ( box2 ) return math . sqrt (( c1 [ 0 ] - c2 [ 0 ]) ** 2 + ( c1 [ 1 ] - c2 [ 1 ]) ** 2 )","title":"center_dist"},{"location":"reference/mot/tracker/tracker_utils/#ratio","text":"def ratio ( box ) View Source def ratio ( box ): return ( box [ 2 ] - box [ 0 ]) / ( box [ 3 ] - box [ 1 ])","title":"ratio"},{"location":"reference/mot/tracker/video_utils/","text":"Module mot.tracker.video_utils View Source import os import ffmpeg def split_video ( input_path , output_folder , fps = 1.5 , resolution = ( 1024 , 768 )): \"\"\"Splits a video into frames Arguments: - *input_path*: string of video full path - *output_folder*: folder to store images - *fps*: float for number of frames per second - *resolution*: integer tuple for resolution \"\"\" if not os . path . isdir ( output_folder ): os . mkdir ( output_folder ) ( ffmpeg . input ( input_path ) . filter ( \"scale\" , width = \"{}\" . format ( resolution [ 0 ]), height = \"{}\" . format ( resolution [ 1 ]) ) . filter ( \"fps\" , fps = fps , round = \"up\" ) . trim ( start_frame = 0 ) . output ( os . path . join ( output_folder , \"frame_ %4d .jpeg\" ), format = \"image2\" , vcodec = \"mjpeg\" ) . run () ) def read_folder ( input_path ): # for now, read directly from images in folder ; later from json outputs return [ os . path . join ( input_path , file ) for file in sorted ( os . listdir ( input_path ))] Functions read_folder def read_folder ( input_path ) View Source def read_folder ( input_path ): # for now , read directly from images in folder ; later from json outputs return [ os . path . join ( input_path , file ) for file in sorted ( os . listdir ( input_path ))] split_video def split_video ( input_path , output_folder , fps = 1.5 , resolution = ( 1024 , 768 ) ) Splits a video into frames Arguments: input_path : string of video full path output_folder : folder to store images fps : float for number of frames per second resolution : integer tuple for resolution View Source def split_video ( input_path , output_folder , fps = 1 . 5 , resolution = ( 1024 , 768 )): \"\"\"Splits a video into frames Arguments: - *input_path*: string of video full path - *output_folder*: folder to store images - *fps*: float for number of frames per second - *resolution*: integer tuple for resolution \"\"\" if not os . path . isdir ( output_folder ): os . mkdir ( output_folder ) ( ffmpeg . input ( input_path ). filter ( \"scale\" , width = \"{}\" . format ( resolution [ 0 ]), height = \"{}\" . format ( resolution [ 1 ]) ). filter ( \"fps\" , fps = fps , round = \"up\" ). trim ( start_frame = 0 ). output ( os . path . join ( output_folder , \"frame_%4d.jpeg\" ), format = \"image2\" , vcodec = \"mjpeg\" ). run () )","title":"Video Utils"},{"location":"reference/mot/tracker/video_utils/#module-mottrackervideo_utils","text":"View Source import os import ffmpeg def split_video ( input_path , output_folder , fps = 1.5 , resolution = ( 1024 , 768 )): \"\"\"Splits a video into frames Arguments: - *input_path*: string of video full path - *output_folder*: folder to store images - *fps*: float for number of frames per second - *resolution*: integer tuple for resolution \"\"\" if not os . path . isdir ( output_folder ): os . mkdir ( output_folder ) ( ffmpeg . input ( input_path ) . filter ( \"scale\" , width = \"{}\" . format ( resolution [ 0 ]), height = \"{}\" . format ( resolution [ 1 ]) ) . filter ( \"fps\" , fps = fps , round = \"up\" ) . trim ( start_frame = 0 ) . output ( os . path . join ( output_folder , \"frame_ %4d .jpeg\" ), format = \"image2\" , vcodec = \"mjpeg\" ) . run () ) def read_folder ( input_path ): # for now, read directly from images in folder ; later from json outputs return [ os . path . join ( input_path , file ) for file in sorted ( os . listdir ( input_path ))]","title":"Module mot.tracker.video_utils"},{"location":"reference/mot/tracker/video_utils/#functions","text":"","title":"Functions"},{"location":"reference/mot/tracker/video_utils/#read_folder","text":"def read_folder ( input_path ) View Source def read_folder ( input_path ): # for now , read directly from images in folder ; later from json outputs return [ os . path . join ( input_path , file ) for file in sorted ( os . listdir ( input_path ))]","title":"read_folder"},{"location":"reference/mot/tracker/video_utils/#split_video","text":"def split_video ( input_path , output_folder , fps = 1.5 , resolution = ( 1024 , 768 ) ) Splits a video into frames Arguments: input_path : string of video full path output_folder : folder to store images fps : float for number of frames per second resolution : integer tuple for resolution View Source def split_video ( input_path , output_folder , fps = 1 . 5 , resolution = ( 1024 , 768 )): \"\"\"Splits a video into frames Arguments: - *input_path*: string of video full path - *output_folder*: folder to store images - *fps*: float for number of frames per second - *resolution*: integer tuple for resolution \"\"\" if not os . path . isdir ( output_folder ): os . mkdir ( output_folder ) ( ffmpeg . input ( input_path ). filter ( \"scale\" , width = \"{}\" . format ( resolution [ 0 ]), height = \"{}\" . format ( resolution [ 1 ]) ). filter ( \"fps\" , fps = fps , round = \"up\" ). trim ( start_frame = 0 ). output ( os . path . join ( output_folder , \"frame_%4d.jpeg\" ), format = \"image2\" , vcodec = \"mjpeg\" ). run () )","title":"split_video"}]}